{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using LinearAlgebra, RowEchelon, LaTeXStrings, Plots, SymPy, LAcode\n",
    "#title( \"Orthogonality\", sz=30, color=\"darkred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:center;width:100%;text-align:center;\">\n",
    "<strong style=\"height:100px;color:darkred;font-size:40px;\">Orthogonal Vectors</strong><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html\"<iframe width=\\\"400\\\" height=\\\"200\\\" src=\\\"https://www.youtube.com/embed/JrqM8bBw8ug\\\"  frameborder=\\\"0\\\" allow=\\\"accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Adding Vector Length to Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Basic Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $\\mathbb{R}^2$, we know a concept of Euclidean length:\n",
    "\n",
    "$\\qquad\n",
    "u = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} \\Rightarrow \\lVert u \\rVert = \\sqrt{ u_1^2 + u_2^2} = \\sqrt{ u \\cdot u }.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **dot product can be used to define the length of a vector!**\n",
    "\n",
    "$\\quad$ However, the dot product is **not defined for Vector spaces:**<br>\n",
    "$\\quad\\quad$ e.g., what would be the dot product of two functions $f(x) \\cdot g(x) ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitions below are carefully written to **generalize to arbitrary vector spaces.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Allow Both Real and Complex Numbers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the notion of the **length of a vector** to a vector space proceeds in two steps.<br>\n",
    "$\\quad$ In the following, we allow the scalars to be $\\mathbb{F} = \\mathbb{R}, \\mathbb{Q}$ or $\\mathbb{C}$.\n",
    "\n",
    "To allow **complex numbers,** we will need the complex conjugate:<br>\n",
    "$\\quad$ a bar over an expression signifies complex conjugation.\n",
    "\n",
    "**Example:**  $\\overline{3 + 2 i} = 3 - 2 i.$<br>\n",
    "$\\quad\\quad\\quad\\;$ Note that for real numbers $\\overline{x} = x.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inner Products (Dot Product)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;\">\n",
    "\n",
    "**Definition:** An **inner product space** is a vector space $V$ over the scalars $\\mathbb{F}$\n",
    " with a function $\\; \\cdot : V \\times V \\rightarrow \\mathbb{F}$<br> $\\quad\\quad$ with the following properties\n",
    "$\\forall x,y,z \\in V, \\; \\forall \\alpha\\in \\mathbb{F}:$\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    " &x \\cdot y          &=& \\;\\overline{ y \\cdot x } & \\text{(conjugate symmetry)             } \\\\\n",
    " &x \\cdot (\\alpha y)  &=& \\;\\alpha x \\cdot y       & \\text{(linearity in the second argument)} \\\\\n",
    " &x \\cdot ( y+z )     &=&\\; x \\cdot y + x \\cdot z        & \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "and\n",
    "\n",
    "$\\qquad\n",
    "x \\cdot x\\quad\\quad\\quad  = \\left\\{ \\begin{align}& c > 0 & \\quad x \\ne 0\\\\ & 0 \\quad & \\text{ otherwise} \\end{align} \\right. \\quad\\quad \\text{ (positive definite)}\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The complex conjugate is required for complex numbers, since the dot product is not positive definite,<br>\n",
    "$\\quad$ i.e., we have\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align} (\\alpha + \\beta i )\\ (\\alpha + \\beta i ) & = \\alpha^2 - \\beta^2 + 2 \\alpha \\beta\\; i  \\\\\n",
    " \\overline{(\\alpha + \\beta i )}\\ (\\alpha + \\beta i ) & = \\alpha^2 + \\beta^2  \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "For example, given $u=\\begin{pmatrix}1+i \\\\ 0\\end{pmatrix},\\;$ then $\\overline{u} \\cdot u = (1+i)(1-i) = 2,\\;$ a positive number<br>\n",
    "$\\qquad$ whereas $u \\cdot u = (1+i)^2 = 2 i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that for modulo 2 arithmetic, i.e., $\\mathbb{F} = \\mathbb{Z}_2$ the dot product is not positive definite.\n",
    " \n",
    "$\\quad$ <span style=\"color:red;\"><strong>ALL THEOREMS FROM HERE ON REQUIRE THE POSITIVE DEFINITE PROPERTY of the dot product</strong></span>\n",
    "\n",
    "$\\quad$ **This is why the Fundamental theorem is broken up into two parts!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner products can be used to define a **distance** function, i.e.,\n",
    "<div style=\"background-color:#F2F5A9;color:black;float:left;width:60%;\">\n",
    "\n",
    "**Definition:** A **metric** for a set $M$ is a function $d : M \\times M \\rightarrow \\mathbb{R}$<br>\n",
    "$\\quad\\quad$ with the following properties\n",
    "$\\forall x,y,z \\in V, \\; \\forall \\alpha\\in \\mathbb{F}:$\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "d(x,y) & = 0 \\; \\Leftrightarrow \\; x = y & \\\\\n",
    "d(x,y) & = d(y,x) & \\\\\n",
    "d(x,y) & \\le d(x,z) + d(z,y) & \\quad\\quad \\text{ ( triangle inequality ) }\\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "**Remark**: The axioms for a metric guarantee\n",
    "\n",
    "$\\qquad d(x,y) \\ge 0$\n",
    "</div><div style=\"float:left;padding-left:1cm;\"><img src=\"Figs/NormAndDistance.svg\" style=\"float:center;\" width=240></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Length of a Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;width:18cm;\">\n",
    "\n",
    "**Definition:** The **norm** of a vector $v$ in an inner product space\n",
    "\n",
    "$\\qquad \\lVert v \\rVert = \\sqrt{ \\overline{v} \\cdot v }$\n",
    "\n",
    "**Definition:** The **distance** between two vectors $x$ and $y$ in an inner product space is\n",
    "\n",
    "$\\qquad d(x,y) = \\lVert x-y \\rVert $\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Remarks:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For the dot product in $\\mathbb{R}^2$ and $\\mathbb{R}^3$, this definition yields the **Euclidean length** of a vector. E.g.,<br>\n",
    "$\\qquad \\bigg\\lVert \\begin{pmatrix}v_1\\\\v_2 \\end{pmatrix} \\bigg\\rVert = \\sqrt{ v_1^2 + v_2^2 }$\n",
    "\n",
    "* The definition of the norm from the inner product shows that<br>\n",
    "$\\qquad\n",
    "\\lVert \\alpha v \\rVert = \\sqrt{ \\overline{\\alpha v} \\cdot \\alpha v  } = \\ \\lvert \\alpha \\rvert \\ \\lVert v \\rVert\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\qquad u = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix},\\; v = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}.$\n",
    "\n",
    "$\\qquad\n",
    "\\lVert u \\rVert = \\sqrt{ 2^2 + 5^2 } = \\sqrt{29}, \\quad \\lVert v \\rVert = \\sqrt{ 3^2 + (-1)^2 } = \\sqrt{10}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad \\lVert 2 u \\rVert = \\sqrt{ \\begin{pmatrix} 4\\\\10 \\end{pmatrix} \\cdot \\begin{pmatrix} 4\\\\10 \\end{pmatrix} } = \\sqrt{ 4 \\begin{pmatrix} 2\\\\5 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} } = 2 \\sqrt{ \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;\">\n",
    "\n",
    "**Definition:** A **unit vector** is a vector with norm equal to 1.\n",
    "\n",
    "**Remark:** $\\;\\;$ Such a vector may be constructed from any non-zero vector $u$ with\n",
    "\n",
    "$\\qquad\n",
    "\\hat{u} = \\frac{1}{\\lVert u \\rVert} u\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad\n",
    "\\begin{align}\n",
    "    u = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} & \\Rightarrow \\lVert u \\rVert = \\sqrt{29} \\\\\n",
    "                                             & \\Rightarrow \\hat{u} =\\frac{1}{\\sqrt{29}}\n",
    "                                                            \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}.\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Inequalities, Angle, Orthogonal Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Orthogonal Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;\">\n",
    "\n",
    "**Constructing Orthogonal Vectors in <span style=\"color:red;\">2D and 3D</span>:**\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "\\lVert u+v \\rVert = \\lVert u-v \\rVert\n",
    "& \\Leftrightarrow \\sqrt{ (u+v) \\cdot (u+v) } =  \\sqrt{ (u-v) \\cdot (u-v) } \\\\\n",
    "& \\Leftrightarrow      { (u+v) \\cdot (u+v) } =       { (u-v) \\cdot (u-v) } \\\\\n",
    "& \\Leftrightarrow      { \\lVert u \\rVert^2 + \\lVert v \\rVert^2 + 2 u \\cdot v } =\n",
    "                       { \\lVert u \\rVert^2 + \\lVert v \\rVert^2 - 2 u \\cdot v } \\\\\n",
    "& \\Leftrightarrow      { u \\cdot v = 0 } \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "**Orthogonal Vectors:**\n",
    "\n",
    "$\\qquad u \\perp v \\Leftrightarrow u \\cdot v = 0$\n",
    "\n",
    "</div><div style=\"float:left;padding-left:3cm;\"><img src=\"Figs/OrthogonalDirection.svg\" width=300></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** The construction shown allows the vector $v = 0$.\n",
    "\n",
    "<div style=\"background-color:#F2F5A9;color:black;\">\n",
    "\n",
    "**Definition:** The **zero vector is orthogonal** to any other vector in the metric space.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generalization: Angles Between Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;\">\n",
    "\n",
    "**Theorem: (Cauchy-Schwartz Inequality)** The inner product between two vectors $u$ and $v$ satisfies\n",
    "\n",
    "$\\qquad \\lvert \\overline{u} \\cdot v \\rvert \\le \\lVert u \\rVert \\ \\lVert v \\rVert$\n",
    "</div>\n",
    "\n",
    "**Remark:** The equality is trivially satisfied if either $u = 0$ or $v = 0$.<br>\n",
    "$\\qquad$ When neither of the vectors is zero, we can rewrite this as\n",
    "\n",
    "$\\qquad\\qquad\n",
    "-1 \\le \\frac{ \\overline{u} \\cdot v }{\\lVert u \\rVert \\ \\lVert v \\rVert} \\le 1\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $\\mathbb{R}^2$ this quotient is the cosine of the angle between the vectors $u$ and $v$.<br>\n",
    "we therefore generalize this to\n",
    "<div style=\"background-color:#F2F5A9;color:black;\">\n",
    "\n",
    "$\\qquad\n",
    "\\cos ( \\angle (u,v) ) = \\frac{ \\overline{u} \\cdot v }{\\lVert u \\rVert \\ \\lVert v \\rVert}, \\quad \\text{ where } u \\ne 0, v \\ne 0\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Remarks:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* orthogonal non-zero vectors have $\\; cos\\ 90^\\circ = 0$,<br>we recover our previous result<br>\n",
    "$\\qquad \\color{red}{u \\cdot v = 0 \\Leftrightarrow u \\perp v }$\n",
    "\n",
    "* the definition for the angle is frequently rewritten<br>\n",
    "$\\qquad \\color{red}{\\overline{u} \\cdot v = \\lVert u \\rVert\\ \\lVert v \\rVert \\ \\cos ( \\angle (u,v) )}$<br>\n",
    "which holds for all vectors $u, v$ (including zero vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let $u = \\begin{pmatrix} 1 \\\\ 5 \\\\ 3 \\end{pmatrix}, \\;\\; v = \\begin{pmatrix} 4 \\\\ 1 \\\\ 1 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **angle between $u$ and $v$**  is $\\theta = arccos \\frac{u \\cdot v}{ \\lVert u \\rVert\\ \\lVert v \\rVert } \n",
    " = arccos \\frac{12}{ \\sqrt{35}\\ \\sqrt{18} } \n",
    " \\approx $ 61.44 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **distance from $u$ to $v$** is $\\quad\\quad\\quad\\quad\\quad\\quad\\lVert v - u \\rVert =\\ \\bigg\\lVert\\ \\begin{pmatrix} 3 \\\\ -4 \\\\ -2 \\end{pmatrix}\\ \\bigg\\rVert\\; \\approx 5.39$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A **detour** via $w = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ increases the distance to $\\lVert v-w \\rVert + \\lVert w-u \\rVert  \\approx 7.42$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fundamental Theorem of Linear Algebra (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Main Definitions and Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Linear Independence of Orthogonal Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;background-color:#F2F5A9;color:black;width:50%;height:2cm;\">\n",
    "\n",
    "**Theorem:** Non-zero orthogonal vectors are **linearly independent.**\n",
    "\n",
    "</div><div style=\"float:left;padding-left:1.5cm;\">\n",
    "$\\quad\\quad$ Let $u \\perp v \\Leftrightarrow u \\cdot v = 0.$\n",
    "\n",
    "$\\quad\\quad\\begin{align}\n",
    "\\alpha u + \\beta v = 0 & \\Rightarrow \\alpha u \\cdot u + \\beta u \\cdot v = 0 \\\\\n",
    "                       & \\Rightarrow \\alpha \\lVert u \\rVert^2 = 0.\\\\\n",
    "\\end{align}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;background-color:#F2F5A9;color:black;width:50%;\">\n",
    "\n",
    "**Fundamental Theorem:** Given a matrix $A$ in $\\mathbb{R}^{M \\times N}$\n",
    "* Any two vectors $r \\in \\mathscr{R}(A), n \\in \\mathscr{N}(A)$ are orthogonal, i.e., $r \\perp n$\n",
    "* Any two vectors $c \\in \\mathscr{C}(A), \\tilde{n} \\in \\mathscr{N}(A^t)$ are orthogonal, i.e., $c \\perp \\tilde{n}$\n",
    "\n",
    "\n",
    "---\n",
    "**Remark:**\n",
    "* The sketch of the Fundamental Theorem presented previously<br>\n",
    "    depicts this situation accurately!<br><br>\n",
    "    **Vectors in the two fundamental spaces<br>in the domain and the codomain are orthogonal**.\n",
    "</div>\n",
    "<div style=\"float:left;padding-left:1.5cm;\">$\\quad$<img src=\"Figs/FundamentalTheorem_0.svg\" width=300></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Proof Outline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a vector in the nullspace $\\mathscr{N}(A),$ i.e., $A x = 0:$\n",
    " \n",
    "$\\quad\\quad A x = 0 \\Rightarrow (\\text{rows of } A) \\cdot x = 0, $ so each row of $A$ is orthogonal to $x$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since any vector in the row space can be written as a linear combination of the rows,<br>\n",
    "$\\quad\\quad (\\alpha_1 R_1 + \\alpha_2 R_2 + \\dots \\alpha_M R_M ) \\cdot x = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\therefore \\quad$ **any vector in $\\mathscr{R}(A)$ is orthogonal to any vector in $\\mathscr{N}(A)$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Remark:**  Given one or more vectors $a_i, i=1,2, n$, we now know<br>\n",
    "$\\quad$ **how to find a vector that is orthogonal**\n",
    "to the hyperplane $span \\{ a_1, a_2, \\dots a_n \\}:$\n",
    "\n",
    "* write the $a_i$ into a matrix $A$ as rows, and find a vector in the nullspace (a homogeneous solution of $A x = 0$ )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Mutually Orthogonal Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Look at the following 3 Vectors<br><br>\n",
    "> $ a_1 = \\begin{pmatrix} -4\\\\ -8\\\\  1 \\end{pmatrix}, \\;\n",
    "  a_2 = \\begin{pmatrix}  7\\\\ -4\\\\ -4 \\end{pmatrix}, \\;\n",
    "  a_3 = \\begin{pmatrix}  4\\\\ -1\\\\  8 \\end{pmatrix} \\quad \\Rightarrow \\quad\n",
    " \\left\\{ \\begin{align}  a_1 \\cdot a_1 =  81, &\\quad a_2 \\cdot a_2 =  81, \\quad a_3 \\cdot a_3 = 81, \\\\\n",
    "                        a_1 \\cdot a_2 = \\;0, &\\quad a_1 \\cdot a_3 = \\;0, \\quad\\; a_2 \\cdot a_3 = \\;0. \\end{align} \\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dot products show\n",
    "> * each of the vectors $a_i$ has length $\\lVert a_i \\rVert = \\sqrt{ a_i \\cdot a_i } = 9.$\n",
    "> * each of the vectors $a_i$ is orthogonal to the other two: $a_i \\cdot a_j = 0\\;$ for $i \\ne j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;float:left;width:45%;\">\n",
    "\n",
    "**Definition:** A set of vectors $a_1, a_2, \\dots a_n$ is **mutually orthogonal**$\\quad$<br>\n",
    "$\\quad\\quad$ iff for all $i, j$ in $1,2,\\dots N,$<br><br>\n",
    "$$\n",
    "a_i \\cdot a_j = \\left\\{ \\begin{align} \\; \\lVert a_i \\rVert^2 \\ne 0 \\quad  &\\ \\text{ when } i = j\\\\ 0 \\quad &\\ \\text{ otherwise} \\end{align} \\right.\n",
    "$$\n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:50%\">\n",
    "\n",
    "**Remarks:**\n",
    "* If the vectors $a_i$ are the columns of a matrix $A$, then<br>\n",
    "$\\quad\\quad$  **the matrix $A^t A$ is diagonal**<br>\n",
    "$\\quad\\quad$  with entry $i,i$ equal to $a_i \\cdot a_i = \\lVert a_i \\rVert^2 .$\n",
    "* the matrix $A^t A$ is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example Revisited**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;width:50%;\">\n",
    "> Let $A = \\begin{pmatrix} -4 & 7 & 4 \\\\ -8 & -4 & -1 \\\\  1 & -4 & 8 \\end{pmatrix} \\Rightarrow\n",
    "A^t A = \\begin{pmatrix} 81 & 0 & 0 \\\\ 0 & 81 & 0 \\\\ 0 & 0 & 81 \\end{pmatrix}\n",
    "$\n",
    "</div><div style=\"float:left;width:40%;padding-left:0cm;\">\n",
    "\n",
    "**Remark:** the example created a square matrix $A$.<br>$\\quad\\quad$ We can have more entries in a vector<br>\n",
    "$\\quad\\quad$ than vectors, e.g.,<br><br>\n",
    "$\\quad\\quad A = ( a_1 \\; a_2 )$.\n",
    "    \n",
    "The resulting matrix $A^t A$ is always **square.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Important Remarks:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A set of mutually orthogonal vectors is often called an **orthogonal set** of vectors\n",
    "* The vectors in an **orthogonal set** are **linearly independent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The vectors in an **orthogonal set form a basis** for their span.\n",
    "* If the vectors in an **orthogonal set** are **unit vectors**, then $A^t A = I$.<br>\n",
    "  $\\quad\\quad$ Think $i,j$ coordinate vectors, possibly in $\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Orthogonal Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;float:left;width:15cm;padding-top:0.5cm;\">\n",
    "\n",
    "**Definition:** A vector space $U$ is orthogonal to a vector space $V$<br>\n",
    "    $\\quad\\quad$ iff $\\forall u \\in U, \\forall v \\in V, \\; u \\perp v$\n",
    "\n",
    "**Definition:** Let $U$ be a subspace of $V$.<br>\n",
    "    $\\quad\\quad$ The **orthogonal complement** $U^\\perp = \\left\\{ v \\in V \\mid \\forall u \\in U, \\ v \\perp u \\right\\}$.\n",
    "\n",
    "**Theorem:** Given a vector space $U$, then $(U^\\perp)^\\perp = U$.<br>\n",
    "**Theorem:** Given two vector spaces $U$ and $V$ such that $U^\\perp = V$, then $V^\\perp = U$.\n",
    "\n",
    "</div>\n",
    "<div style=\"float:left;padding-left:1.5cm;\"><img src=\"Figs/FundamentalTheorem_1.svg\" width=250></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;float:left;width:15cm;\">\n",
    "\n",
    "**Fundamental Theorem:** Given a matrix $A \\in \\mathbb{R}^{M \\times N}$<br>\n",
    "    $\\quad\\quad$ then $\\mathscr{R}(A)^\\perp = \\mathscr{N}(A)$ in $\\mathbb{R}^N$.<br>\n",
    "**Fundamental Theorem:** Given a matrix $A \\in \\mathbb{R}^{M \\times N}$<br>\n",
    "    $\\quad\\quad$  then $\\mathscr{C}(A)^\\perp = \\mathscr{N}(A^t)$ in $\\mathbb{R}^M$.\n",
    "\n",
    "**Fundamental Theorem:** Let $A$ be a matrix of size $M \\times N.$<br>\n",
    "    $\\quad\\quad$  The union of the bases for $\\mathscr{C}(A)$ and $\\mathscr{N}(A^t)$ is a basis for $\\mathbb{R}^M$.<br>\n",
    "**Fundamental Theorem:** Let $A$ be a matrix of size $M \\times N.$<br>\n",
    "    $\\quad\\quad$  The union of the bases for $\\mathscr{R}(A)$ and $\\mathscr{N}(A)$   is a basis for $\\mathbb{R}^N$.\n",
    "</div>\n",
    "<div style=\"float:left;padding-left:1.5cm;\"><img src=\"Figs/FundamentalTheorem_2.svg\" width=250></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Use the Fundamental Theorem to Decompose a Vector (Naive Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A$ be a matrix of size $M \\times N$ with rank $r$.<br><br>\n",
    "Let $\\left\\{ c_1, c_2, \\dots c_r \\right\\}$ be a basis for $\\mathscr{C}(A)$,<br>\n",
    "and $\\left\\{ ñ_1, ñ_2, \\dots ñ_{M-r} \\right\\}$ be a basis for $\\mathscr{N}(A')$.<br>\n",
    "\n",
    "The combined basis $\\quad\\left\\{\\  c_1, c_2, \\dots c_r,\\;  ñ_1, ñ_2, \\dots ñ_{M-r} \\ \\right\\}$ $\\quad\\quad$\n",
    "is a basis for $\\mathbb{R}^M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let $A = \\begin{pmatrix}  1  & 2 \\\\ 2 & 2 \\\\ -1 & 1 \\end{pmatrix}$.\n",
    ">\n",
    "> A quick computation shows<br>\n",
    "> $\\qquad\n",
    "\\text{basis } \\mathscr{C}(A) = \\left\\{\\;\n",
    "c_1= \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\; c_2 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} \\; \\right\\},\\quad\\quad\n",
    "\\text{basis } \\mathscr{N}(A^t) = \\left\\{\\ \\tilde{n}\\ = \\ \\begin{pmatrix} -4 \\\\ 3 \\\\ 2 \\end{pmatrix} \\; \\right\\}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Combining the two bases yields a basis for $\\mathbb{R}^3$:<br>\n",
    "> $\\qquad\n",
    "\\text{basis } \\mathbb{R}^3 = \\left\\{\\;\n",
    "\\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\;  \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} \\;\n",
    "\\begin{pmatrix} -4 \\\\ 3 \\\\ 2 \\end{pmatrix} \\; \\right\\}.\n",
    "$\n",
    ">\n",
    "> $\\therefore\\quad$ **Any vector $b$ in $\\mathbb{R}^3$ can be written as a unique linear combination**\n",
    "$\\qquad\n",
    "b = \\alpha_1 c_1 + \\alpha_2 c_2 + \\alpha_3 \\tilde{n}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Sketch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;float:left;padding-top:0.5cm;\">\n",
    "\n",
    "A consequence of the Fundamental Theorem Part II for a matrix $A \\in \\mathbb{R}^{M \\times N}$:\n",
    "\n",
    "Given  $\\left\\{ c_1, c_2, \\dots c_r \\right\\},$ a basis for $\\mathscr{C}(A)$,<br>\n",
    "and $\\left\\{ ñ_1, ñ_2, \\dots ñ_{M-r} \\right\\},$ a basis for $\\mathscr{N}(A^t)$.<br>\n",
    "\n",
    "> Any vector $b \\in \\mathbb{R}^M$ can be written as a linear combination of these vectors:\n",
    "$$\n",
    "\\begin{align}\n",
    "&b           \\; = \\color{blue}{b_{\\parallel}} + \\color{red}{b_{\\perp}},  &\\\\\n",
    " \\text{ where }\\quad &\\color{blue}{b_{\\parallel}  = \\alpha_1 c_1 + \\alpha_2 c_2 \\dots + \\alpha_r c_r} &\\\\\n",
    " \\text{ and }\\quad &\\color{red}{b_\\perp        = \\beta_1 ñ_1 + \\beta_2 ñ_2 \\dots \\beta_{M-r} ñ_{M-r}}.&\n",
    "\\end{align}\n",
    "$$\n",
    "</div>\n",
    "<div style=\"float:left;padding-left:3cm;\">\n",
    "The result is depicted in the following Figure:<br>\n",
    "    (note this is the codomain of $y=A x$ turned on its side)<br><br>\n",
    "<img src=\"Figs/NormalEquations.svg\"  width=\"350\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:center;>\n",
    "* the red vector $b_\\parallel$ is the part of $b$<br>\n",
    "    that lies in the $\\mathscr{C}(A)$ hyperplane (the linear combination formed with $\\alpha_i c_i$)<br>\n",
    "  it is the orthogonal projection $Proj_{\\mathscr{C}(A)}^\\perp b$ onto the column space $\\mathscr{C}(A)$\n",
    "* the blue vector $b_\\perp$ is the part of $b$<br>\n",
    "    that lies in the $\\mathscr{N}(A')$ hyperplane (the linear combination formed with $\\beta_j ñ_j$)$\\quad\\quad$\n",
    "* these two vector components are orthogonal.\n",
    "            </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;\">\n",
    "\n",
    "* the red vector $b_\\parallel$ is the part of $b$<br>\n",
    "that lies in the $\\mathscr{C}(A)$ hyperplane (the linear combination formed with $\\alpha_i c_i$)<br>\n",
    "it is the orthogonal projection $Proj_{\\mathscr{C}(A)}^\\perp b$ onto the column space $\\mathscr{C}(A)$\n",
    "* the blue vector $b_\\perp$ is the part of $b$<br>\n",
    "that lies in the $\\mathscr{N}(A^t)$ hyperplane (the linear combination formed with $\\beta_j ñ_j$)\n",
    "* these two vector components are orthogonal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example Continued**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Decompose a Vector into Two Orthogonal Components ( <span style=\"color:red;\"> Naive Method</span> )** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let $A = \\left(\\begin{array}{rr}  1  & 2 \\\\ 2 & 2 \\\\ -1 & 1 \\end{array}\\right)$, the matrix from the previous example.\n",
    ">\n",
    "> **Step 1:** We had found the bases\n",
    ">\n",
    ">\n",
    "> $\\qquad\n",
    "\\text{basis } \\mathscr{C}(A) = \\left\\{\\;\n",
    "c_1= \\left(\\begin{array}{r} 1 \\\\ 2 \\\\ -1 \\end{array}\\right), \\; c_2 = \\left(\\begin{array}{r} 2 \\\\ 2 \\\\ 1 \\end{array}\\right) \\; \\right\\}, \\quad\\quad\n",
    "\\text{basis } \\mathscr{N}(A^t) = \\left\\{\\ \\tilde{n}\\ = \\ \\left(\\begin{array}{r} -4 \\\\ 3 \\\\ 2 \\end{array}\\right) \\; \\right\\}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Step 2:** Split the vector $b = \\left(\\begin{array}{r} -3 \\\\ 7 \\\\ -2 \\end{array}\\right)\\;$\n",
    "into two orthogonal components: $b = \\color{red}{b_{//}} + \\color{blue}{b_\\perp}\\;$,\n",
    "where $\\color{red}{b_{//}}$ is in $\\mathscr{C}(A)$ and $\\color{blue}{b_\\perp}$ is in $\\mathscr{N}(A^t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\quad\\quad$ We need to solve $\\quad b = \\color{red}{ \\alpha_1 c_1 + \\alpha_2 c_2 } + \\color{blue}{ \\alpha_3 \\tilde{n}} \\; \\Leftrightarrow \\;\n",
    "\\left(\\begin{array}{rrr} \\color{red}1 & \\color{red}2 & \\color{blue}{-4} \\\\ \\color{red}2 & \\color{red}2 & \\color{blue}3 \\\\  \\color{red}{-1} &  \\color{red}1 &  \\color{blue}2 \\end{array}\\right)\n",
    "\\left(\\begin{array}{r} \\color{red}{\\alpha_1} \\\\ \\color{red}{\\alpha_2} \\\\ \\color{blue}{\\alpha_3} \\end{array}\\right) = \\left(\\begin{array}{r} -3 \\\\ 7 \\\\ -2 \\end{array}\\right).\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\quad\\quad$ This yields<br>\n",
    "> $\\qquad\n",
    "\\begin{align}\n",
    "\\color{red}{b_{//}}   &= \\color{red}{3 a_1 - a_2} &=& \\; \\color{red}{\\begin{pmatrix} 1 \\\\ 4 \\\\ -4 \\end{pmatrix}} \\\\\n",
    "\\color{blue}{b_\\perp} &= \\color{blue}{\\tilde{n}} &=& \\; \\color{blue}{\\begin{pmatrix} -4 \\\\ 3 \\\\ 2 \\end{pmatrix}} \\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Check:** Let's check orthogonality: $\\qquad\\color{red}{b_{//}} \\cdot \\color{blue}{b_\\perp} = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---\n",
    "> This computation was involved: we needed to\n",
    "> * **Step 1:** find the bases of both $\\mathscr{C}(A)$ and $\\mathscr{N}(A^t)$\n",
    "> * **Step 2:** Solve the resulting $A x = b$ type problem\n",
    "> * **Step 3:** Split the resulting column view decomposition of $b = \\color{red}{b_\\parallel} + \\color{blue}{b_\\perp}.$\n",
    ">\n",
    "> **It turns out we can do better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Use the Fundamental Theorem to Decompose a Vector (Refinement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Key Observation 1: Decomposing a Vector into Orthogonal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observation:** decomposition of a vector $b$\n",
    "<div style=\"float:left;\">\n",
    "\n",
    "Any vector $b \\in \\mathbb{R}^M$ can be written as a linear combination:\n",
    "$$\n",
    "\\begin{align}\n",
    "                         &b                         \\;&=\\;& \\color{red}{b_{\\parallel}} + \\color{blue}{b_{\\perp}},\\\\\n",
    "\\text{ where }\\quad\\quad &\\color{red}{b_{\\parallel}}  &=\\;& \\color{red}{\\alpha_1 c_1 + \\alpha_2 c_2 \\dots + \\alpha_r c_r} &\\\\\n",
    "\\text{ and }  \\quad\\quad &\\color{blue}{b_\\perp}       &=\\;& \\color{blue}{\\beta_1 ñ_1 + \\beta_2 ñ_2 \\dots \\beta_{M-r} ñ_{M-r}},&\n",
    "\\end{align}\n",
    "$$\n",
    "the $c_i$ vectors form a basis for a hyperplane containing $b_\\parallel$,<br>\n",
    "the $ñ_j$ vectors form a basis for the orthogonal complement of this hyperplane.\n",
    "</div><div style=\"float:left;padding-left:3cm;\">\n",
    "<img src=\"Figs/NormalEquations.svg\"  width=\"340\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The basic idea is to replace the system of equations for the coefficients $\\alpha_i, \\beta_j$ by taking dot products with each of the $c_j$ in the column space $\\mathscr{C}(A)$:\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "(\\xi) & \\Leftrightarrow b &\\;=\\;& \\color{red}{\\alpha_1 c_1 + \\alpha_2 c_2 \\dots + \\alpha_r c_r} + \\color{blue}{b_\\perp} \\\\\n",
    "      & \\Rightarrow \\color{red}{c_j} \\cdot b &\\;=\\;& \\color{red}{\\alpha_1 c_j \\cdot c_1 + \\alpha_2  c_j \\cdot c_2 \\dots + \\alpha_r  c_j \\cdot c_r} + \\color{red}{c_j} \\cdot \\color{blue}{b_\\perp}, \\quad\\quad j=1,2, \\dots r\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Since $\\color{red}{c_j} \\perp \\color{blue}{b_\\perp}$, the $\\color{red}{c_j} \\cdot \\color{blue}{b_\\perp} = 0$: **we are left with a set of equations that only involve the unknown coefficients $\\color{red}{\\alpha_i}$!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's return to $A = \\begin{pmatrix}  1  & 2 \\\\ 2 & 2 \\\\ -1 & 1 \\end{pmatrix}$, which has\n",
    "$\n",
    "\\text{basis } \\mathscr{C}(A) = \\left\\{\\;\n",
    "c_1= \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\; c_2 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} \\; \\right\\} \\quad\n",
    "$ and the vector $b = \\begin{pmatrix} -3 \\\\ 7 \\\\ -2 \\end{pmatrix}\\;$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Set $ \\alpha_1  \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix} + \\alpha_2 \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} + b_\\perp = \\begin{pmatrix} -3 \\\\ 7 \\\\ -2 \\end{pmatrix}.$\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Taking the dot product with $c_1$ and $c_2$ yields<br><br>\n",
    "> $\\qquad \\left.\n",
    "\\begin{align}\n",
    "c_1 : \\quad & 6 \\alpha_1 + 5 \\alpha_2 =& 13 \\\\\n",
    "c_2 : \\quad & 5 \\alpha_1 + 9 \\alpha_2 =& 6 \\\\\n",
    "\\end{align}\n",
    "\\right\\} \\Leftrightarrow \\left\\{ \\begin{aligned}\n",
    "\\alpha_1 =&\\; 3 \\\\ \\alpha_2 =& -1 \\end{aligned} \\right.\n",
    "$<br><br>\n",
    "the same solution we found before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Key Observation 2: No Need to Identify the Column Space of $A$ and the Null Space of $A^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observation:** we do **not need to identify the fundamental spaces!**\n",
    "\n",
    "$\\quad$ Keeping all the columns of $A$ instead of just the pivot columns $c_j, j=1,2,\\dots r$<br>\n",
    "$\\qquad\\quad$ potentially yields an infinite number of ways of expressing the same solution $\\color{red}{b_\\parallel}:$<br>\n",
    "$\\qquad\\quad$ We would get the same solution as before by setting the additional free variables equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key observation:** remember that $b = \\color{red}{b_\\parallel} + \\color{blue}{b_\\perp}$.\n",
    "\n",
    "$\\qquad$ We are given vectors $\\color{red}{a_1, a_2, \\dots a_n}$ and $b$.\n",
    "\n",
    "$\\qquad$ Once we compute $\\color{red}{b_\\parallel}$, we obtain $\\color{blue}{b_\\perp} = b - \\color{red}{b_\\parallel}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 The Final Touch: Rewrite the Equations in Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, observe that we can rewrite our equations in matrix form:\n",
    "\n",
    "$\\qquad\n",
    " b \\;=\\; \\color{red}{\\alpha_1 a_1 + \\alpha_2 a_2 \\dots + \\alpha_n a_n} + \\color{blue}{b_\\perp}\n",
    " \\Leftrightarrow b =  \\color{red}{A x} + \\color{blue}{b_\\perp}.\n",
    "$\n",
    "\n",
    "Taking the dot products with the columns $ \\color{red}{a_j = 1, 2, \\dots n}$ of $A$ yields the **normal equation**\n",
    "\n",
    "$\\qquad\n",
    " \\color{red}{A^t A x = A^t b}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Basic Properties of the Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Solution of $A\\ x\\ =\\ b$ versus Solution of $A^t\\ A\\ x\\ =\\ A^t\\ b$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $A x = b_\\parallel + b_\\perp$ has solutions iff $b_\\perp = 0$ since $b_\\parallel \\in \\mathscr{C}(A).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Any solution of $A x = b_\\parallel$ is a solution of the normal equation<br>\n",
    "    $\\begin{align}\n",
    "    A x = b_\\parallel \\Rightarrow A^t A x & = A^t b_\\parallel \\\\\n",
    "                                          & = A^t b_\\parallel + A^t b_\\perp \\\\\n",
    "                                          & = A^t b\n",
    "    \\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;background-color:#F2F5A9;color:black;width:45%;\">\n",
    "    \n",
    "**Theorem:** $\\quad\\quad\\quad\\quad\\quad\\mathscr{N}(A) = \\mathscr{N}(A^t A)$\n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;width:50%;\">\n",
    "\n",
    "$\\therefore$ **$\\; A^t A x = A^t b\\;$ has the same solutions as $A x = b_\\parallel$**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"float:left;\">\n",
    "The proof is of interest:\n",
    "\n",
    "* Let $x$ be a solution of $A x = 0.$ Multiplying by $A^t$ from the left yields $A^t A x = 0,$<br>\n",
    "    and therefore $\\mathscr{N}(A) \\subseteq \\mathscr{N}(A^t A).$\n",
    "* Let $x$ be a solution of $A^t A x = 0.$<br><br>\n",
    "$\\qquad\\begin{align}\n",
    "    A^t A x = 0 \\Rightarrow & x^t A^t A x = 0\\\\\n",
    "            \\Rightarrow & (A x )^t (A x ) = 0 \\\\\n",
    "            \\Rightarrow & \\lVert A x \\rVert = 0 \\\\\n",
    "            \\Rightarrow & A x = 0,\n",
    "    \\end{align}\n",
    "$<br><br>\n",
    "and therefore $\\mathscr{N}(A^t A) \\subseteq \\mathscr{N}(A).$\n",
    "</div><div style=\"float:left;padding-left:3cm;padding-top:1cm;\"><img src=\"Figs/VennDiagram_AtA.svg\" width=300>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;padding:10pt;\">\n",
    "\n",
    "**Normal Equation Theorem:** Given a set of vectors $\\{ \\; a_1, a_2 \\dots a_k \\; \\}$ in $\\mathbb{R}^n$.<br>\n",
    "$\\quad\\quad$ Any vector $b \\in \\mathbb{R}^n$ can be decomposed uniquely into two orthogonal components<br><br>\n",
    "$\\qquad\\qquad b = \\color{red}{b_\\parallel} + \\color{blue}{b_\\perp}$<br>\n",
    "$\\quad\\quad$ such that ${\\color{red}{b_\\parallel}} \\in S = span\\{  a_1, a_2 \\dots a_k \\}$ and\n",
    "${\\color{blue}{b_\\perp}} \\in S^\\perp$\n",
    "\n",
    "$\\qquad\\qquad\n",
    "   \\begin{align}\n",
    "   &A^t A x     &=&\\ A^t b  \\\\\n",
    "   &\\color{red}{b_\\parallel} &=&\\ A x    \\\\\n",
    "   &\\color{blue}{b_\\perp}     &=&\\ b - \\color{red}{b_\\parallel}\n",
    "   \\end{align}.\n",
    "$\n",
    "\n",
    "$\\quad\\quad$ where $A = ( a_1 \\ a_2 \\ \\dots a_k ).$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **key observation** was that the multiplication of $A x = b = b_\\parallel + b_\\perp$ by $A^t$ zeros out the $b_\\perp$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 The Equivalent Minimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;\">\n",
    "The triangle inequality shows that the solutions to the normal equations also solve\n",
    "the problem\n",
    "<div style=\"background-color:#F2F5A9;color:black;padding-top:0.3cm;padding-bottom:0.3cm;\">\n",
    "\n",
    "$\\qquad\n",
    "x^* = \\arg\\min_x { \\lVert b - A x \\rVert }\n",
    "$\n",
    "</div>\n",
    "since $b_\\parallel = b - A x^*$ is the shortest vector from a point in $\\mathscr{C}(A)$ to $b$.<br><br>\n",
    "Note that the solution need not be unique: as before, we are interested in\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "   &b_\\parallel &=&\\ A x^*    \\\\\n",
    "   &b_\\perp     &=&\\ b - b_\\parallel,\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "so homogeneous solutions do not enter.\n",
    "\n",
    "<div style=\"background-color:#F2F5A9;color:black;\">\n",
    "\n",
    "**Definition:** The distance $d( b, \\mathscr{C}(A) ) = \\lVert b_\\perp \\rVert.$<br><br>\n",
    "\n",
    "</div></div><div style=\"float:left;padding-left:3cm;padding-top:1cm;\"><img src=\"Figs/NormalEqMinDist.svg\" width=330></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Example: Projection Onto a Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A = \\left(\\begin{array}{rrr}\n",
    " 1 & 3 &4 \\\\\n",
    " 5 & 3 &8 \\\\\n",
    " 1 &-1 &0 \\\\\n",
    " 2 & 2 &4\n",
    "\\end{array}\\right), \\quad$ and\n",
    "$\\; b = \\left(\\begin{array}{r} 15\\\\ 23\\\\ -2\\\\ 9 \\end{array}\\right).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Normal Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad A^t A x = A^t b \\;\\Leftrightarrow\\; \\left( \\begin{array}{rrr|r}\n",
    " 31 & 21 & 52 & 146\\\\\n",
    " 21 & 23 & 44 & 134\\\\\n",
    " 52 & 44 & 96 & 280 \\end{array} \\right) \\quad \\Leftrightarrow x =\\begin{pmatrix} 2 \\\\ 4 \\\\ 0 \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Split the $b$ Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$b = b_\\parallel + b_\\perp,$ where\n",
    "\n",
    "$\\qquad\n",
    "b_\\parallel = A x = \\left(\\begin{array}{r} 14 \\\\ 22 \\\\ -2 \\\\ 12 \\end{array}\\right),\\quad \\text{ and } \\;\n",
    "b_\\perp     = b - b_\\parallel = \\left(\\begin{array}{r} 1 \\\\ 1 \\\\ 0 \\\\ -3 \\end{array}\\right); \\quad\\quad \\textbf{Check orthogonality: }\n",
    "b_\\parallel \\cdot b_\\perp = 0.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Distance from $b$ to the hyperplane $\\mathscr{C}(A)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad\n",
    "d(\\ b,\\ \\mathscr{C}(A)\\ )\\ =\\ \\lVert b_\\perp \\rVert\\ =\\ \\sqrt{11}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Special Case: Projection onto a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;width:40%;padding-top:0.5cm;\">\n",
    "Decompose a vector $b = b_\\parallel + b_\\perp$,  where\n",
    "\n",
    "* $\\quad\\quad b_\\parallel$ is a vector in the span$\\{ a \\}$, and\n",
    "* $\\quad\\quad b_\\perp$ is a vector orthogonal to $a$.\n",
    "\n",
    "**Remark:** Here $A = ( a ),$ $x = ( \\alpha ) $\n",
    "</div>\n",
    "<img src=\"Figs/NormalProjOntoLine.svg\" width=250 style=\"float:left;padding-right:2cm;\">\n",
    "<div style=\"float:left;width:6cm;height:3.5cm;border:1px solid black;padding-top:0.5cm;\">\n",
    "$\\quad$ Consider<br>\n",
    "$\\qquad\\quad a = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix}, \\quad b =  \\begin{pmatrix} 0 \\\\ 5 \\\\ 2 \\end{pmatrix}.$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1: Solve the Normal Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;width:40%;\">\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "\\alpha\\ a\\ + \\ b_\\perp =\\ b \\;& \\Rightarrow\\;&\n",
    "\\alpha\\ a \\cdot a      = a \\cdot b \\; \\\\\n",
    "& \\Leftrightarrow\\;& \\alpha = \\frac{ a \\cdot b }{a \\cdot a}.\\\\\n",
    "\\end{align}\n",
    "$\n",
    "</div><div style=\"float:left;background-color:#F2F5A9;color:black; padding-right:0.3cm;margin-right:1cm;\">\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "A x + \\ b_\\perp =\\ b &\\; \\Rightarrow    \\;& A^t A x  = A^t A b \\\\\n",
    "                     &\\; \\Leftrightarrow\\;& x = \\frac{ a \\cdot b }{a \\cdot a}.\\\\\n",
    "\\end{align}\n",
    "$\n",
    "</div>\n",
    "<div style=\"float:left;width:6cm;height:1.8cm;border:1px solid black;padding-left:1cm;\">\n",
    "<br>\n",
    "$\\qquad \\alpha = x = \\frac{4}{5}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2: Decompose the Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left;width:40%;padding-top:0.5cm;\">\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "& b_\\parallel        = \\ \\alpha\\ a \\quad  =  \\frac{ a \\cdot b }{a \\cdot a}\\ a\\\\\n",
    "& b_\\perp            = \\ b - b_\\parallel  \\\\\n",
    "\\end{align}\n",
    "$\n",
    "</div><div style=\"float:left;background-color:#F2F5A9;color:black;height:2cm;padding-right:3cm;margin-right:1cm;\">\n",
    "\\begin{align}\n",
    "\\quad b_\\parallel        =&\\ A x  =&  \\frac{ a \\cdot b }{a \\cdot a}\\ a & \\\\\n",
    "\\quad b_\\perp            =&\\ b - b_\\parallel &\n",
    "\\end{align}\n",
    "</div><div style=\"float:left;width:6cm;height:2cm;border:1px solid black;padding-right:1cm;\">\n",
    "\n",
    "$\\qquad\n",
    "b_\\parallel = \\frac{1}{5}\\begin{pmatrix} 12\\\\16\\\\0 \\end{pmatrix},\\; b_\\perp = \\frac{1}{5}\\left(\\begin{array}{r} -12 \\\\ 9\\\\ 2 \\end{array}\\right)\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Special Case: the Columns of $A$ are Mutually Orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations simplify considerably when the columns $\\{ a_1, a_2, \\dots a_N \\}$ of $A$<br> are **mutually orthogonal vectors**, i.e, when\n",
    "\n",
    "$\\qquad\n",
    "a_i \\cdot a_j = \\left\\{ \\begin{align} \\; \\lVert a_i \\rVert^2 \\ne 0 \\quad  &\\ \\text{ when } i = j\\\\ 0 \\quad &\\ \\text{ otherwise} \\end{align} \\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal equation takes the form\n",
    "\n",
    "$\\qquad\n",
    "A^t A x = A^t b \\Leftrightarrow D x = A^t b \\Leftrightarrow x = (A^t A)^{-1} A^t b,\n",
    "$\n",
    "\n",
    "where  $D$ is a diagonal matrix\n",
    "\n",
    "$\\qquad\n",
    "D = A^t A = \\begin{pmatrix} \\lVert a_1 \\rVert^2 & 0                   & \\dots & 0 \\\\\n",
    "                    0                   & \\lVert a_1 \\rVert^2 & \\dots & 0 \\\\\n",
    "                    \\                   &     \\               &  \\    & 0 \\\\\n",
    "                    0                   & 0                   & \\dots & \\lVert a_N \\rVert^2 \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"float:left;background-color:#F2F5A9;color:black;width:48%;padding-left:0.3cm;\">\n",
    "\n",
    "For **mutually orthogonal** $a_i, i=1,\\dots N$, the normal equations reduce to\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "x_i          =&\\ \\frac{ b \\cdot a_i }{ a_i \\cdot a_i } \\\\\n",
    "b_\\parallel  =&\\ \\sum_{i=1}^{N}{ \\frac{ b \\cdot a_i }{ a_i \\cdot a_i} a_i } \\\\\n",
    "b_\\perp      =&\\ b - \\sum_{i=1}^{N}{ \\frac{ b \\cdot a_i }{ a_i \\cdot a_i} a_i } \\\\\n",
    "\\end{align}\n",
    "$\n",
    "</div><div style=\"float:right;background-color:#F2F5A9;color:black;width:48%;height:4.5cm;padding-left:0.3cm;\">\n",
    "\n",
    "For **mutually orthonormal** $q_i, i=1,\\dots N$, the normal equations reduce to\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "x_i          =&\\ b \\cdot q_i  \\\\\n",
    "b_\\parallel  =&\\ \\sum_{i=1}^{N}{ b \\cdot q_i \\ q_i } \\\\\n",
    "b_\\perp      =&\\ b - \\sum_{i=1}^{N}{b \\cdot q_i \\ q_i } \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "**Remark**:\n",
    "Orthonormal Coordinate vectors are NICE TO HAVE!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $A = \\left(\\begin{array}{rrr}\n",
    "1 & 1 &  1 \\\\\n",
    "1 & 1 & -1 \\\\\n",
    "1 &-1 &  0 \\\\\n",
    "1 &-1 &  0 \\\\\n",
    "\\end{array}\\right), \\quad b = \\left(\\begin{array}{r} 4\\\\-4\\\\8\\\\12 \\end{array}\\right)\\;\\Rightarrow\\;\\;\n",
    "A^t A = \\left(\\begin{array}{rrr} \\color{red}4&0&0\\\\ 0&\\color{red}4&0\\\\ 0&0&\\color{red}2\\end{array}\\right)\n",
    ",$ so the columns of $A$ are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 1: Solve the Normal Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad A^t A x = A^t b\\quad $ yields\n",
    "$\\quad\n",
    "\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} =\n",
    "\\begin{pmatrix} \\color{red}4&0&0\\\\ 0&\\color{red}4&0\\\\ 0&0&\\color{red}2\\end{pmatrix}^{-1} \\;\n",
    "\\left(\\begin{array}{r} 20 \\\\ -20 \\\\ 8 \\end{array}\\right)\\;\n",
    "= \\left(\\begin{array}{r} 5 \\\\ -5 \\\\ 4 \\end{array}\\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Step 2: Decompose the Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad\n",
    "b_\\parallel = A x = \\left(\\begin{array}{r} 4 \\\\ -4 \\\\ 10 \\\\ 10 \\end{array}\\right),\\quad\n",
    "b_\\perp  = b - b_\\parallel = \\left(\\begin{array}{r} 0 \\\\ 0 \\\\ -2 \\\\ 2 \\end{array}\\right),\\quad\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Take Away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 The Fundamental Theorem of Linear Algebra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;float:left;height:5cm; width:12cm;padding-top:1cm;padding-left:0.3cm;\">\n",
    "\n",
    "**Fundamental Theorem (Part II)**\n",
    "* $\\mathscr{R}(A)^\\perp = \\mathscr{N}(A)$<br>\n",
    "* $\\mathscr{C}(A)^\\perp = \\mathscr{N}(A^t)$\n",
    "\n",
    "**Nota Bene:** Part II requires a **positive definite** inner product.\n",
    "</div>\n",
    "<img src=\"Figs/FundamentalTheorem.svg\" width=300 style=\"float:left;padding-left:3cm;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style type=\"text/css\">\n",
    ".tftable {font-size:12px;color:#333333;width:100%;border-width: 1px;border-color: #729ea5;border-collapse: collapse;}\n",
    ".tftable th {font-size:12px;background-color:#acc8cc;border-width: 1px;padding: 8px;border-style: solid;border-color: #729ea5;text-align:left;}\n",
    ".tftable tr {background-color:#ffffff;}\n",
    ".tftable td {font-size:12px;border-width: 1px;padding: 8px;border-style: solid;border-color: #729ea5;}\n",
    "</style>\n",
    "\n",
    "<table class=\"tftable\" border=\"1\">\n",
    "<tr><th style=\"width:6cm;\">Equation</th><th style=\"width:6cm;\">Orthogonal Projection</th><th>Comment</th></tr>\n",
    "<tr><td>$x = \\arg\\min_x { \\lVert b - A x \\rVert }$</td><td>$b_\\parallel=A x$</td><td>Minimize Distance to $\\mathscr{C}(A)$</td></tr>\n",
    "<tr><td>$A^t A x = A^t b$</td><td>$b_\\parallel=A x$</td><td>Remove $\\mathscr{N}(A^t)$ component from $b$</td></tr>\n",
    "<tr><td>$x = \\frac{a \\cdot b}{a \\cdot a}$</td><td>$b_\\parallel=\\frac{a \\cdot b}{a \\cdot a}\\;a$</td><td>Column Vector Case: $A = a$</td></tr>\n",
    "<tr><td>$x_i = \\frac{a_i \\cdot b}{a_i \\cdot a_i}$</td><td>$b_\\parallel=\\sum_i\\frac{a_i \\cdot b}{a_i \\cdot a_i}\\;a_i$</td><td>Orthogonal Vectors $a_i$ Case: $A = (a_1 \\ a_2 \\ \\dots)$</td></tr>\n",
    "<tr><td>$x_i = q_i \\cdot b$</td><td>$b_\\parallel=\\sum_i{q_i \\cdot b\\;q_i}$</td><td>Orthonormal Vectors $q_i$ Case: $A = (q_1\\ q_2 \\dots )$</td></tr>\n",
    "</table>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.0",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
