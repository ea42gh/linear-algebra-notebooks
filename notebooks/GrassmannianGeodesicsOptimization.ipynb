{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Libraries and Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    from julia.api import Julia\n",
    "    jl = Julia(compiled_modules=False)\n",
    "\n",
    "#import julia; julia.install(quiet=True)\n",
    "from julia import Main\n",
    "\n",
    "import numpy     as np\n",
    "from scipy.linalg import svd, qr\n",
    "\n",
    "import panel     as pn; pn.extension()\n",
    "import holoviews as hv; hv.extension( \"bokeh\", logo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext julia.magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%julia\n",
    "using Pkg; Pkg.activate(\"../GenLinAlgProblems\")\n",
    "using GenLinAlgProblems, LinearAlgebra, RowEchelon, Printf, Latexify, LaTeXStrings, Random, SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%julia\n",
    "function ex()\n",
    "    function f(U)\n",
    "        P_U = U * U'\n",
    "        P_T = T * T'\n",
    "        return norm(P_U - P_T)^2\n",
    "    end\n",
    "\n",
    "    function grad_f(U)\n",
    "        return 2 * (U * U' - T * T') * U\n",
    "    end\n",
    "\n",
    "    # Starting subspace U₀ ∈ Gr(2,3)\n",
    "    U₀ = [1.0 0.0;\n",
    "          0.0 1.0;\n",
    "          0.0 0.0]\n",
    "\n",
    "    # Target subspace T: rotated 2-plane\n",
    "    θ = π / 6\n",
    "    T = [1.0 0.0;\n",
    "         0.0 cos(θ);\n",
    "         0.0 sin(θ)]\n",
    "    T = Matrix(qr(T).Q)  # Ensure T is orthonormal\n",
    "\n",
    "    # Run optimization\n",
    "    U_final, losses = grassmann_optimize(f, grad_f, U₀, t=0.1, max_iter=100)\n",
    "\n",
    "    # Print first and last few losses\n",
    "    py_show(\"Initial loss: \", round(losses[1], digits=4))\n",
    "    py_show(\"Final loss: \", round(losses[end], digits=4))\n",
    "    return losses\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### \n",
    "<div style=\"height:2cm;\">\n",
    "<div style=\"float:center;width:100%;text-align:center;\"><strong style=\"height:100px;color:darkred;font-size:40px;\">Geodesics and Optimization of the Grassmannian</strong>\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation: Why Optimize on the Grassmannian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now shift from exploring the geometry of subspaces to using it:<br>\n",
    "applying the Grassmannian structure to design **optimization algorithms that stay on the manifold.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many applications, the object we’re trying to learn is not a vector, but a subspace.\n",
    "- In **PCA**, we learn a low-dimensional subspace that captures most of the variance.\n",
    "- In **subspace tracking**, we follow how that subspace evolves over time.\n",
    "- In **compressed sensing** and **dimension reduction**, we work with structured low-rank approximations.\n",
    "\n",
    "These subspaces are elements of the **Grassmannian manifold**, $ \\mathrm{Gr}(k, n),$ the space of all $k$-dimensional subspaces of $ \\mathbb{R}^n $.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Updates Don't Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we represent a subspace $ \\mathcal{U} \\subset \\mathbb{R}^n $ by an orthonormal matrix $ U \\in \\mathbb{R}^{n \\times k} $, with $ U^T U = I $.<br>\n",
    "If we try to update it using a standard gradient step:\n",
    "\n",
    "$\\qquad\n",
    "U_{\\text{new}} = U - \\eta\\ \\nabla f(U),\n",
    "$\n",
    "\n",
    "the new matrix $ U_{\\text{new}} $ will **no longer be orthonormal**, and may not even span a valid subspace.\n",
    "\n",
    "**Remark:** Matrix derivatives are treated in [**this notebook**](MatrixDerivatives.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grassmannian Optimization\n",
    "\n",
    "To fix this, we need to:\n",
    "1. Understand the **tangent space** at $U$, where we can take meaningful steps.\n",
    "2. Use the **Riemannian gradient** — a projection of the Euclidean gradient.\n",
    "3. Step along **geodesics** to stay on the manifold.\n",
    "\n",
    "This is the core idea of **optimization on the Grassmannian**:<br>\n",
    "Move along directions that respect the geometry, keeping subspaces valid at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Riemannian Optimization on $\\mathrm{Gr}(k, n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Tangents and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Tangent Space Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2F5A9;color:black;padding-bottom:0cm;\">\n",
    "\n",
    "**Def:** Let $U \\in \\mathbb{R}^{n \\times k}$ be a matrix with orthonormal columns, representing a point on the Grassmannian $\\mathrm{Gr}(k, n)$.<br>\n",
    "$\\qquad$ The **tangent space** to the Grassmannian at $U$, denoted $T_U \\mathrm{Gr}(k, n)$, is the set of all matrices $Z \\in \\mathbb{R}^{n \\times k}$ such that\n",
    "\n",
    "$\\qquad\\qquad\n",
    "U^T Z = 0.\n",
    "$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The definition ensures that each column of $Z$ is orthogonal to the columns of $U$.<br>\n",
    "$\\qquad$ That is, tangent vectors lie in the **orthogonal complement of the column space of $U$**<br>\n",
    "$\\qquad$ (This follows directly from the **Fundamental Theorem of Linear Algebra**)\n",
    "\n",
    "**Remarks:**\n",
    "* **$\\mathbf{T_U \\mathrm{Gr}(k, n)}$ is standard notation** and reads\n",
    "\"*the tangent space to the Grassmannian $\\mathrm{Gr}(k, n)$ at the point $U$.*\"\n",
    "* Although the tangent space has dimension $k(n - k)$, we represent its elements as $n \\times k$ matrices satisfying $U^T Z = 0$.<br>\n",
    "This keeps the representation consistent with the shape of $U$ and simplifies computations, especially in optimization contexts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Tangent Space in $ \\mathrm{Gr}(2,3) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $\\;\n",
    "U = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix},\\;\n",
    "$\n",
    "which represents the standard 2-plane spanned by the $x$- and $y$-axes in $\\mathbb{R}^3$.\n",
    "\n",
    "A matrix $Z \\in T_U \\mathrm{Gr}(2,3)$ must satisfy $U^T Z = 0$. i.e., its columns must be in $\\mathscr{N}(U^T) = \\mathrm{span}\\left\\{ \\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}  \\right\\}$.\n",
    "\n",
    "So any **tangent matrix** $Z$ must have the form $\\;\n",
    "Z = \\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "z_{1} & z_{2}\n",
    "\\end{pmatrix}.\n",
    "$\n",
    "\n",
    "That is, the tangent space consists of all $3 \\times 2$ matrices with nonzero entries only in the third row, i.e.,<br>\n",
    "directions pointing \"upward\" out of the $xy$-plane into $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Projection onto the Tangent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an **Euclidean gradient** $G = \\nabla f(U)$.<br>\n",
    "The **Riemannian gradient** is its projection onto the tangent space:\n",
    "\n",
    "$\\qquad\n",
    "\\mathrm{grad}_U f = (I - U U^T) G.\n",
    "$\n",
    "\n",
    "This removes the component of $G$ that would push $U$ off the manifold.\n",
    "\n",
    "\n",
    "**Remark:** The Riemannian gradient $\\mathrm{grad}_U f$ is typically rank-deficient as a matrix,<br>\n",
    "$\\qquad$ because it lies entirely in the tangent space $T_U \\mathrm{Gr}(k, n)$, which has dimension $k (n - k) < n k$.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Property:**<br>\n",
    "$\\qquad$ This projected gradient tells us how to move **within** the Grassmannian, rather than stepping off the manifold.<br>\n",
    "$\\qquad$ It plays the same role as a regular gradient, just **respecting the constraint**<br>\n",
    "$\\qquad$ that $U$ must remain an orthonormal basis for a $k$-dimensional subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Riemannian Gradient in $\\mathrm{Gr}(2,3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are minimizing a function $f(U)$ and we compute the Euclidean gradient as\n",
    "$\\;\n",
    "G = \\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{pmatrix}.\n",
    "$\n",
    "\n",
    "To obtain the **Riemannian gradient**, we project $G$ onto the tangent space using:\n",
    "\n",
    "$\\qquad\n",
    "\\mathrm{grad}_U f = (I - U U^T) G.\n",
    "$\n",
    "\n",
    "Taking $U$ as $\\;\\;\n",
    "U = \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "0 & 0\n",
    "\\end{pmatrix},\\;\\; \\text{then}\\;\\;\n",
    "U U^T = \\begin{pmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "I - U U^t = \\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}.\n",
    "$\n",
    "\n",
    "we obtain\n",
    "$\\;\n",
    "\\mathrm{grad}_U f =\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4 \\\\\n",
    "5 & 6\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0 & 0 \\\\\n",
    "0 & 0 \\\\\n",
    "5 & 6\n",
    "\\end{pmatrix}.\n",
    "$\n",
    "\n",
    "So the Riemannian gradient only has nonzero entries in the third row — just like any valid tangent matrix at $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3  Why Use $n \\times k$ Matrices to Represent Tangents and Gradients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the Grassmannian has dimension $k(n - k)$, we represent tangent vectors and Riemannian gradients as $n \\times k$ matrices<br>\n",
    "$\\qquad$ with orthogonality constraints (i.e., $U^T Z = 0$).\n",
    "\n",
    "This representation lives in the same ambient space as $U$ itself, making it easy to perform computations<br>\n",
    "$\\qquad$ like projecting gradients or stepping along geodesics — using standard matrix operations.\n",
    "\n",
    "**Remark:** These definitions are also **coordinate-free**: we do not need to choose a specific basis for the tangent space:<br>\n",
    "replacing $U$ with $U Q$ for any $Q \\in \\mathrm{O}(k)$ does not change the subspace or the structure of the tangent space.\n",
    "\n",
    "By using $n \\times k$ matrices, we preserve both **geometric clarity** and **computational convenience**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Geodesic Updates and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Geodesic Updates from a Tangent Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [**previous notebook**](GrassmannianIntro.ipynb), we introduced geodesics as smooth curves connecting subspaces,<br>\n",
    "parameterized by principal vectors and principal angles between known points on the Grassmannian.\n",
    "\n",
    "Here, we take a **local** perspective.<br>\n",
    "Given a point $U \\in \\mathrm{Gr}(k, n)$ and a tangent vector $Z \\in T_U \\mathrm{Gr}(k,n)$,<br>\n",
    "we want to move along the geodesic $U(t)$ starting at $U$ in the direction $Z$.\n",
    "\n",
    "When $Z = A \\Theta B^T$ is the compact SVD of the tangent matrix,<br>\n",
    "the geodesic is given by:\n",
    "\n",
    "$\\qquad\n",
    "U(t) = U B \\cos(\\Theta t) B^T + A \\sin(\\Theta t) B^T.\n",
    "$\n",
    "\n",
    "This curve:\n",
    "- Starts at $U$ with initial velocity $Z$,\n",
    "- Preserves orthonormality of $U(t)$ for all $t$,\n",
    "- Stays on the Grassmannian $\\mathrm{Gr}(k,n)$.\n",
    "\n",
    "In optimization, we typically approximate the geodesic near $t = 0$ with a first-order step:\n",
    "\n",
    "$\\qquad\n",
    "U_{\\text{trial}} = U + t Z,\n",
    "$\n",
    "\n",
    "This update does not preserve orthonormality, and while $U_{\\text{trial}}$ may remain full column rank<br>\n",
    "and span a $k$-dimensional subspace, its column space does not generally lie on the true geodesic.<br>\n",
    "For small $t$, it provides a useful first-order approximation.\n",
    "\n",
    "To restore an orthonormal basis for the updated subspace, we apply a **QR retraction**:\n",
    "\n",
    "$\\qquad\n",
    "U_{\\text{new}} = \\mathrm{qf}(U + t Z),\n",
    "$\n",
    "\n",
    "where $\\mathrm{qf}(\\cdot)$ extracts the orthonormal factor from a QR decomposition.\n",
    "\n",
    "<details>\n",
    "<summary>Why apply the QR retraction?</summary>\n",
    "\n",
    "For small enough $t$, $U + tZ$ typically remains full column rank.<br>\n",
    "QR retraction\n",
    "1. Restores a well-conditioned orthonormal matrix on the Grassmannian.\n",
    "2. Keeps the result close to the true geodesic for small $t$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Example: Minimizing a Subspace Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the QR retraction in a simple optimization task on the Grassmannian $\\mathrm{Gr}(2,3)$.\n",
    "\n",
    "We define the following loss function:\n",
    "\n",
    "$\\qquad\n",
    "f(U) = \\| P_u - P_t \\|_F^2,\n",
    "$\n",
    "\n",
    "where $P_u = U U^T$ is the projection matrix onto the subspace spanned by $U$,<br>\n",
    "and $T$ is a fixed $3 \\times 2$ orthonormal matrix defining a target subspace with projection matrix $P_t$.\n",
    "\n",
    "This is a convex function of projection matrices, but not of $U$ directly — so the optimization is nontrivial.\n",
    "\n",
    "---\n",
    "\n",
    "**What We are Trying to Show**\n",
    "\n",
    "We already know the optimal subspace $T$, so this example is not about *finding* a solution.<br>\n",
    "Instead, it demonstrates that\n",
    "- The Riemannian gradient defines a descent direction within the tangent space.\n",
    "- A retraction (via QR) maps the updated point back to the Grassmannian.\n",
    "- The loss $f(U)$ decreases over successive steps.\n",
    "\n",
    "This confirms that **Riemannian gradient descent with retraction** respects the manifold structure and behaves as expected<br>\n",
    "even when applied to a curved, non-Euclidean space of subspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute the Riemann Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(\n",
    "    \"## Convergence of Riemannian Gradient Descent\",\n",
    "    hv.Curve(Main.losses, \"Iteration\", \"Loss\" ).opts( show_grid=True, tools=['hover'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Riemannian gradient descent with retraction**<br>\n",
    "Loss values over successive iterations of gradient descent on the Grassmannian $\\mathrm{Gr}(2,3)$,<br>\n",
    "using a projected Riemannian gradient and QR retraction to preserve orthonormality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Take Away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reframes the geometry of the Grassmannian in computational terms:\n",
    "\n",
    "- A **tangent vector** $Z \\in T_U \\mathrm{Gr}(k, n)$ defines a local direction of motion.\n",
    "- A **first-order step** $U + tZ$ must be corrected to stay on the manifold.\n",
    "- The **QR retraction** maps the updated point back to $\\mathrm{Gr}(k, n)$ while preserving orthonormality.\n",
    "- Gradient descent using this scheme leads to consistent reduction of geometric loss functions.\n",
    "\n",
    "These ingredients — tangent vectors, gradient projection, and retraction — form the core of Riemannian optimization.\n",
    "\n",
    "They also serve as the foundation for algorithms in the next notebook,<br>\n",
    "where we apply these techniques to interpolation, regression, and clustering of subspaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
