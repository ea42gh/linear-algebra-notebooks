{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import streamz as sz\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "from bokeh.models.widgets import HTMLTemplateFormatter\n",
    "import param\n",
    "import holoviews as hv; hv.extension(\"bokeh\", logo=False)\n",
    "import panel as pn;     pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:center;width:100%;text-align: center;\"><strong style=\"height:60px;color:darkred;font-size:40px;\">Iterative Methods</strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Code: Monitor the Evolution of an Iterative Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def _table_formatter2D(plot, element):\n",
    "    plot.handles['table'].columns[3].formatter = HTMLTemplateFormatter(\n",
    "        template = \"\"\"<div style=\"color:red;\"><%= value ? value.toExponential(4) : value %></div>\"\"\")\n",
    "def _table_formatterND(plot, element):\n",
    "    plot.handles['table'].columns[2].formatter = HTMLTemplateFormatter(\n",
    "        template = \"\"\"<div style=\"color:red;\"><%= value ? value.toExponential(4) : value %></div>\"\"\")\n",
    "\n",
    "class GraphicalMonitor2D:\n",
    "    \"\"\"monitor the error evolution of an iterative scheme\"\"\"\n",
    "\n",
    "    def __init__(self, sz=10, use_log=False):\n",
    "        self.buffer  = hv.streams.Buffer(self._to_dataframe(), length=sz, index=False)\n",
    "        self.use_log = use_log\n",
    "\n",
    "        self.plots = pn.Column( pn.Row( hv.DynamicMap(self.display_xy,  streams=[self.buffer]),\n",
    "                                        hv.DynamicMap(self.display_err, streams=[self.buffer])\n",
    "                                      ),\n",
    "                                 hv.DynamicMap(self.display_tbl, streams=[self.buffer])\n",
    "                            )\n",
    "    def _to_dataframe(self, data=None):\n",
    "        data_def = {\"step\": int, \"x\": float, \"y\": float, \"error\": float}\n",
    "\n",
    "        if data is None:\n",
    "            return pd.DataFrame(\n",
    "                {i: [] for i in data_def},\n",
    "            ).astype(dtype=data_def)\n",
    "\n",
    "        return pd.DataFrame([data], columns=data_def).astype(dtype=data_def)\n",
    "\n",
    "    def reset_plots(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def monitor(self, data):\n",
    "        self.buffer.send(self._to_dataframe(data))\n",
    "\n",
    "    def display_xy(self, data):\n",
    "        if data.empty:\n",
    "            return ( hv.Scatter([], \"x\", \"y\") * hv.Curve([], \"x\", \"y\")  * hv.Scatter([], \"x\", \"y\")\n",
    "                    ).opts( width=500,  xticks=4, yticks=4, tools=[\"hover\"], show_grid=True, title=\"Solution Estimate\" )\n",
    "        h_last_point = hv.Scatter( (data[\"x\"].iloc[-1], data[\"y\"].iloc[-1]), \"x\", \"y\" ).opts(size=10, color=\"red\", tools=[\"hover\"])\n",
    "\n",
    "        h_points = hv.Curve((data[\"x\"], data[\"y\"]), \"x\", \"y\") *\\\n",
    "                   hv.Scatter( (data[\"x\"], data[\"y\"]), \"x\", \"y\" )\\\n",
    "                     .opts(color=\"darkblue\", padding=0.05, size=8, tools=[\"hover\"], show_grid=True)\n",
    "\n",
    "        return (h_points.opts(xticks=4, yticks=4) * h_last_point).opts(width=500, tools=[\"hover\"])\n",
    "\n",
    "    def display_err( self, data):\n",
    "        edim = hv.Dimension('error', range=(1e-18, np.nan))\n",
    "        if data.empty:\n",
    "            return hv.Curve([], \"step\", edim).opts( tools=[\"hover\"], logy=self.use_log, yticks=4, show_grid=True, title=\"Error\" )\n",
    "        return hv.Curve((data[\"step\"], data[\"error\"]), \"step\", edim).opts(padding=0.05, xticks=4, logy=self.use_log)\n",
    "\n",
    "    def display_tbl( self, data ):\n",
    "        if data.empty:\n",
    "            return hv.Table(data).opts(height=450,width=500, hooks=[_table_formatter2D])\n",
    "        return hv.Table(data).opts(hooks=[_table_formatter2D])\n",
    "\n",
    "class GraphicalMonitorND:\n",
    "    \"\"\"monitor the error evolution of an iterative scheme\"\"\"\n",
    "\n",
    "    def __init__(self, dimension, sz=10, use_log=False):\n",
    "        self._data_def = {\"step\": int}\n",
    "        for i in range(dimension):\n",
    "            self._data_def['x'+ str(i+1)] = float\n",
    "        self._data_def[\"error\"] = float\n",
    "\n",
    "        self.buffer  = hv.streams.Buffer(self._to_dataframe(), length=sz, index=False)\n",
    "        self.use_log = use_log\n",
    "\n",
    "        self.plots = pn.Row( hv.DynamicMap(self.display_err, streams=[self.buffer]),\n",
    "                             hv.DynamicMap(self.display_tbl, streams=[self.buffer]) )\n",
    "\n",
    "    def _to_dataframe(self, data=None):\n",
    "        if data is None:\n",
    "            return pd.DataFrame(\n",
    "                    {i: [] for i in self._data_def},\n",
    "                ).astype(dtype= self._data_def)\n",
    "\n",
    "        flattend_data = [data[0]] + data[1].tolist() + [data[2]]\n",
    "        return pd.DataFrame([flattend_data], columns=self._data_def).astype(dtype=self._data_def)\n",
    "\n",
    "    def reset_plots(self):\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def monitor(self, data):\n",
    "        self.buffer.send(self._to_dataframe(data))\n",
    "\n",
    "    def display_err( self, data):\n",
    "        edim = hv.Dimension('error', range=(1e-18, np.nan))\n",
    "        if data.empty:\n",
    "            return hv.Curve([], \"step\", edim).opts(  logy=self.use_log, tools=[\"hover\"], yticks=4, show_grid=True, title=\"Error\" )\n",
    "        return hv.Curve((data[\"step\"], data[\"error\"]), \"step\", edim).opts(padding=0.05, xticks=4, logy=self.use_log, tools=[\"hover\"])\n",
    "\n",
    "    def display_tbl( self, data ):\n",
    "        if data.empty:\n",
    "            return hv.Table(data).opts(height=450,width=500, hooks=[_table_formatterND])\n",
    "        return hv.Table(data).opts(hooks=[_table_formatterND])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "source": [
    "# 2. Iterative Solutions of $\\mathbf{A x = b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Idea: Set up a Fixed Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wanted:** an iterative scheme to solve $A x = b$ for a square matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We split $A = S - T$ such that $S$ is invertible:\n",
    "\n",
    "$\\qquad \\begin{align}\n",
    "\\color{blue}{\\mathbf{A x = b}}   & \\Leftrightarrow ( S - T )\\ x = b \\\\\n",
    "          & \\Leftrightarrow S x = T x + b \\\\\n",
    "          & \\Leftrightarrow x = S^{-1} T x + S^{-1} b \\\\\n",
    "          & \\Leftrightarrow \\color{blue}{\\mathbf{x = \\tilde{A}x + \\tilde{b}}}\n",
    "\\end{align},$<br><br>\n",
    "$\\qquad$ where we have set $\\tilde{A} = S^{-1} T, \\;\\; \\tilde{b} = S^{-1} b.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Convert this last equation into an iteration:<br>\n",
    "$\\qquad$ Start with some vector $x_0$, and compute\n",
    "$\\qquad \\color{blue}{\\mathbf{x_{n} = \\tilde{A} x_{n-1} + \\tilde{b}, \\quad n=1,2,\\dots}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The difference between successive iterates is $\\qquad\\;\\; \\color{blue}{\\mathbf{e_{n} = x_n - x_{n-1}}}$.\n",
    "\n",
    "When $\\color{blue}{\\mathbf{\\underset{\\mathbf{n \\to \\infty}}{\\operatorname{\\lim}} e_{n} = 0}}$, the iterative converges to a **fixed point**, a solution of $A x = b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Consider $\\begin{pmatrix} x_n \\\\ y_n \\end{pmatrix} =  \\begin{pmatrix}  0.4 & -0.1 \\\\ 0.2 & 0.1 \\end{pmatrix}\n",
    "\\begin{pmatrix} x_{n-1} \\\\ y_{n-1} \\end{pmatrix} + \\begin{pmatrix} 0.8 \\\\ 1.6 \\end{pmatrix},\n",
    "\\qquad \\text{with} \\quad \\begin{pmatrix}x_0 \\\\ y_0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> A fixed point satisfies<br>\n",
    "$\\quad \\begin{pmatrix} x \\\\ y \\end{pmatrix} =  \\begin{pmatrix}  0.4 & -0.1 \\\\ 0.2 & 0.1 \\end{pmatrix}\n",
    "\\begin{pmatrix} x \\\\ y \\end{pmatrix} + \\begin{pmatrix} 0.8 \\\\ 1.6 \\end{pmatrix}\n",
    "\\quad \\Leftrightarrow \\quad\n",
    "\\begin{pmatrix} 0.6 & 0.1 \\\\ -0.2 & 0.9 \\end{pmatrix} \n",
    "\\begin{pmatrix} x \\\\ y \\end{pmatrix} =\n",
    "\\begin{pmatrix} 0.8 \\\\ 1.6 \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def iterate_scheme( A, b, x, n, plots, tol=1.e-10 ):\n",
    "    plots.reset_plots()\n",
    "\n",
    "    errs = []\n",
    "\n",
    "    twoD = A.shape[1] == 2\n",
    "    if twoD:\n",
    "        print( \" Iteration\\t\\t x \\t          y \\t\\t Error\")\n",
    "    else:\n",
    "        print( \" Iteration\\t\\t x \\t          y \\t          z \\t\\t Error\")\n",
    "\n",
    "    for i in range(n):\n",
    "        x_old = x\n",
    "        x     = A @ x + b\n",
    "        err   = np.linalg.norm( x-x_old )\n",
    "\n",
    "        errs.append( err )\n",
    "        if twoD:\n",
    "            print( f\"{i:10}\\t {x[0]: .10f}\\t   {x[1]: .10f}\\t{err: .10f}\" )\n",
    "            plots.monitor( [i, x[0], x[1], err] )\n",
    "        else:\n",
    "            with np.printoptions(formatter={'float': '{: 0.10f}'.format}):\n",
    "                print( f\"{i:10}\\t {x}\\t{err: .10f}\" )\n",
    "            plots.monitor( [i, x, err] )\n",
    "\n",
    "        if err < tol:\n",
    "            return x, err, errs\n",
    "        time.sleep(0.5)\n",
    "    return None,err,errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a scheme $x_n = A x_{n-1} + b$ with some initial guess $x_0$,<br>\n",
    "$\\qquad$ when does it converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's look at the change at each iteration step: $e_n = x_{n} - x_{n-1}$.<br>\n",
    "$\\qquad$ Starting with $n = 1,$ we see<br>\n",
    "$\\qquad\\quad\\begin{align}\n",
    "e_2 &= \\left(A x_1 + b \\right) - \\left( A x_0 + b \\right) &= A e_1 & \\\\\n",
    "e_3 &= \\left(A x_2 + b \\right) - \\left( A x_1 + b \\right) &= A e_2 & = A^2 e_1 \\\\\n",
    "\\dots & & & \\\\\n",
    "e_{n+1} &= \\left(A x_{n} + b \\right) - \\left( A x_{n-1} + b \\right) &= A e_n & = A^{n} e_1 \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "When $A = S \\Lambda S^{-1}$, we obtain<br><br>\n",
    "$\\qquad\n",
    "\\mathbf{\\underset{\\mathbf{n \\to \\infty}}{\\operatorname{\\lim}}} e_{n+1} =\n",
    "S \\ \\begin{pmatrix}\n",
    "      \\mathbf{\\underset{\\mathbf{n \\to \\infty}}{\\operatorname{\\lim}}} \\lambda^n_1 & 0 & \\dots & 0 \\\\\n",
    "  0 & \\mathbf{\\underset{\\mathbf{n \\to \\infty}}{\\operatorname{\\lim}}} \\lambda^n_2     & \\dots & 0 \\\\\n",
    "    &                                                                           &   & \\dots &   \\\\\n",
    "  0 & 0 & \\dots & \\mathbf{\\underset{\\mathbf{n \\to \\infty}}{\\operatorname{\\lim}}} \\lambda^n_N \\\\\n",
    "\\end{pmatrix}\\ S^{-1}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We see that this limit goes to zero iff $\\lvert \\lambda_i \\rvert < 1$ for each $i=1,2, \\dots N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"float:left;padding-left:5pt;width:98%;background-color:#F2F5A9;color:black;\">\n",
    "\n",
    "**Definition:** The **spectral radius** of a matrix $A$ of size $N \\times N$ is given by<br>\n",
    "$\\qquad \\rho(A) = \\underset{\\mathbf{i=1,2,\\dots N}}{\\operatorname{\\max}}\\ \\lvert \\lambda_i \\rvert$\n",
    "\n",
    "**Theorem:** the iteraitve scheme $x_n = A x_{n-1} + b$ converges iff $\\rho (A) < 1$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Jacobi Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let $A = S - T, $ where $S$ is the diagonal matrix $S_{i i} = A_{i i}, i=1, \\dots N$ (the diagonal entries of $A$)\n",
    "\n",
    "Then $A x = b \\Leftrightarrow S x = T x + b \\Leftrightarrow x = S^{-1} T x + S^{-1} b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A = np.array([[6., .1], [-.2, .9]]); b = np.array([0.8,1.6])\n",
    "S = np.diag( np.diag(A) )\n",
    "T = S - A\n",
    "\"A =\";A\n",
    "\"b =\";b\n",
    "\n",
    "\"S =\";S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Sinv = np.linalg.inv(S)\n",
    "SinvT = Sinv @ T\n",
    "Sinvb = Sinv @ b\n",
    "\n",
    "Markdown('**<font size=\"5\">Jacobi Iteration, 2D Example**</font>')\n",
    "plots = GraphicalMonitor2D(100,use_log=False)\n",
    "plots.plots\n",
    "x,err,errs = iterate_scheme( SinvT, Sinvb, np.array([3,-2]),  100, plots, tol=1e-8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "A     = np.array([[ 6., 2, -3],\n",
    "                  [-1., 3,  1],\n",
    "                  [ 1., 2,  4]])\n",
    "b = np.array([1., 1, 1])\n",
    "\n",
    "S     = np.diag( np.diag(A) )\n",
    "T     = S - A\n",
    "Sinv  = np.linalg.inv(S)    # Note:  In our iteration, solve  S x_n = T x_{n-1} + b   for x_n  instead!!!!\n",
    "\n",
    "SinvT = Sinv @ T\n",
    "Sinvb = Sinv @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eigA  = np.linalg.eigvals(SinvT)\n",
    "print(\"Spectral Radius: \")\n",
    "print(\".  eigenvalues: \", np.round( eigA, 3) )\n",
    "print(\".  max(abs(e)): \", np.round(max(np.abs(eigA)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plots = GraphicalMonitorND(3,10,use_log=False)\n",
    "Markdown('**<font size=\"5\">Jacobi Iteration, 3D Example**</font>')\n",
    "\n",
    "plots.plots\n",
    "x,err,errsJ = iterate_scheme( SinvT, Sinvb, np.array([3,-2,1]), 100, plots, tol=1e-8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Gauss Seidel Iteration (GS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:** Consider an iterative scheme<br>\n",
    "$\\qquad\\begin{align}\n",
    "x_{n} &=\\;\\; 4 \\ x_{n-1} &+ 4 \\ &y_{n-1} &+ 3 \\ &z_{n-1} &+ 2 \\\\\n",
    "y_{n} &=\\;\\; 2 \\ x_{n-1} &- 2 \\ &y_{n-1} &- 3 \\ &z_{n-1} &- 1 \\\\\n",
    "z_{n} &=\\;\\; 3 \\ x_{n-1} &- 3 \\ &y_{n-1} &+ 3 \\ &z_{n-1} &+ 1 \\\\\n",
    "\\end{align}$\n",
    "\n",
    "why not use the updated versions of $x_{n}, y_{n}$ as soon as they become available?<br>\n",
    "$\\qquad\\begin{align}\n",
    "x_{n} &= \\;\\; 4\\ x_{n-1}            &+ 4 \\ &y_{n-1}            &+ 3 \\ &z_{n-1} &+ 2 \\\\\n",
    "y_{n} &= \\;\\; 2\\ x_{\\color{red}{n}} &- 2 \\ &y_{n-1}            &- 3 \\ &z_{n-1} &- 1 \\\\\n",
    "z_{n} &= \\;\\; 3\\ x_{\\color{red}{n}} &- 3 \\ &y_{\\color{red}{n}} &+ 3 \\ &z_{n-1} &+ 1 \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This corresponds to choosing $\\mathbf{S}$ to be the <font color=\"red\">lower triangular part of $\\mathbf{A}$.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S     = np.tril(A)\n",
    "T     = S - A\n",
    "Sinv  = np.linalg.inv(S)    # Note:  In our iteration, solve  S x_n = T x_{n-1} + b   for x_n  instead!!!!\n",
    "\n",
    "SinvT = Sinv @ T\n",
    "Sinvb = Sinv @ b\n",
    "\n",
    "eigA  = np.linalg.eigvals(SinvT)\n",
    "print(\"Spectral Radius: \")\n",
    "print(\".  eigenvalues: \", np.round(eigA,3) )\n",
    "print(\".  max(abs(e)): \", np.round( max(np.abs(eigA)),3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = GraphicalMonitorND(3,10,use_log=False)\n",
    "Markdown('**<font size=\"5\">Gauss Seidel Iteration**</font>')\n",
    "\n",
    "plots.plots\n",
    "x,errGS,errsGS = iterate_scheme( SinvT, Sinvb, np.array([3,-2,1]), 100, plots, tol=1e-8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Successive Overrelaxation (SOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea:** A step of the Gauss-Seidel scheme moves the solution estimate form $x_{n-1}$ to $x_n$.<br>\n",
    "$\\quad$ How about moving by a different amount, i.e., for $x_{n-1}$ to $x_{n-1} + \\alpha \\left(x_{n}-x_{n-1}\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $S = L$, $T=A-L$ i.e., the matrix split used for Gauss-Seidel, we have<br><br>\n",
    "$\\quad\\begin{align}\n",
    "\\tilde{x}_{n} &= S^{-1}T x_{n-1} + S^{-1} b \\\\\n",
    "x_n &= x_{n-1} + \\alpha \\left( \\tilde{x}_n - x_{n-1} \\right)\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting $\\omega = 1-\\alpha$, the sheme can be shown to be equivalent to<br><br>\n",
    "$\\quad \\left( D+\\omega L \\right) x_{n} = \\left( \\left( 1-\\omega \\right) D - \\omega U \\right) x_{n-1} + \\omega b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Iterative Methods for $\\mathbf{A x = \\lambda x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Power Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method( A, x, n, plots, tol=1.e-10 ):\n",
    "    plots.reset_plots()\n",
    "    print( \" Iteration\\t\\t x \\t          y \\t\\t Change\")\n",
    "\n",
    "    for i in range(n):\n",
    "        x_old = x\n",
    "        x     = A @ x\n",
    "        x     = x / np.linalg.norm( x )\n",
    "\n",
    "        err   = np.linalg.norm( x-x_old )\n",
    "\n",
    "        print( f\"{i:10}\\t {x[0]: .10f}\\t   {x[1]: .10f}\\t{err: .10f}\" )\n",
    "        plots.monitor( [i, x[0], x[1], err] )\n",
    "\n",
    "        if err < tol:\n",
    "            return x, x.T @ A @ x\n",
    "\n",
    "        time.sleep(0.3)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = GraphicalMonitor2D(15,use_log=False)\n",
    "Markdown('**<font size=\"5\">Power Method**</font>')\n",
    "plots.plots\n",
    "power_method( np.array([[3,1],[5,2]]),\n",
    "              np.array([1,1]),\n",
    "              100,\n",
    "              plots\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Power Method with Streamz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "#  Streamz Pipeline Functions\n",
    "# ================================================================================================\n",
    "def make_Step(A):\n",
    "    def step(x):\n",
    "        x,diff2 = power_method_step( step.A, x )\n",
    "        e_val = estimate_eigenvalue( step.A, x)\n",
    "        return { 'e_val': e_val, 'e_vec': x, 'diff2': diff2}\n",
    "    step.A=A\n",
    "    return step\n",
    "# --------------------------------------------------\n",
    "def step_Counter( state, cur):\n",
    "    state += 1\n",
    "\n",
    "    cur['step_n'] = state\n",
    "    return state,cur\n",
    "# --------------------------------------------------\n",
    "def make_ConvergenceCheck(n_max, tol=1e-6):\n",
    "    def checkConvergence(cur):\n",
    "        cur['cnv_flag'] = check_convergence( cur['diff2'], cur['step_n'], checkConvergence.n_max, checkConvergence.tol2 )\n",
    "        return cur\n",
    "\n",
    "    checkConvergence.tol2  = tol*tol\n",
    "    checkConvergence.n_max = n_max\n",
    "    return checkConvergence\n",
    "# --------------------------------------------------\n",
    "def make_LoopBack(n, pipeline):\n",
    "    def loop( cur ):\n",
    "        if loop.n > cur['step_n'] and cur['cnv_flag'] == 0:\n",
    "            loop.p.emit(cur['e_vec'])\n",
    "        return cur\n",
    "    loop.n = n\n",
    "    loop.p = pipeline\n",
    "    return loop\n",
    "# --------------------------------------------------\n",
    "def make_Delay(t):     # the sz rate limit function appears to cause problems in jupyter\n",
    "    def delay(cur):\n",
    "        time.sleep(t)\n",
    "        return cur\n",
    "    return delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def power_method_step(A, x):\n",
    "    '''implements a single step of the power method\n",
    "\n",
    "    Args:\n",
    "        A  : the np.array matrix\n",
    "        x  : the current np.array unit vector\n",
    "    Returns\n",
    "        diff2, eigval_estimate, eigvec_estimate, where diff2 is the l2 norm of the change in the eigvec_estimate\n",
    "    '''\n",
    "    new_x        = A @ x\n",
    "\n",
    "    normalized_x = new_x / np.linalg.norm(new_x)\n",
    "    diff         = normalized_x - x\n",
    "\n",
    "    return normalized_x, diff @ diff\n",
    "\n",
    "def power_method_convergence_check(diff, n_step, n_max, tol):\n",
    "    '''power method convergence\n",
    "\n",
    "    Args:\n",
    "        n_step:  the current step number\n",
    "        diff:    convergence estimate\n",
    "        tol:     tolerance\n",
    "        n_max:   maximum number of steps\n",
    "    Returns:\n",
    "        1 if converged, -1 if too many steps, 0 otherwise\n",
    "    '''\n",
    "    return 1 if diff < tol else -1 if n_step >= n_max else 0\n",
    "\n",
    "def estimate_eigenvalue( A, x_hat ):\n",
    "    '''eigenvalue estimate using the Rayleigh coefficient\n",
    "\n",
    "    Args:\n",
    "        A:       matrix\n",
    "        dix_hat: unit length eigenvector estimate\n",
    "    Returns:\n",
    "        estimated eigenvalue\n",
    "    '''\n",
    "\n",
    "    return np.dot( x_hat, A @ x_hat )\n",
    "\n",
    "def check_convergence(diff2, n_step, n_max, tol2):\n",
    "    '''power method convergence\n",
    "\n",
    "    Args:\n",
    "        n_step:  the current step number\n",
    "        cur:     current output of step(A, x)\n",
    "        tol:     tolerance\n",
    "        n_max:   maximum number of steps\n",
    "    Returns:\n",
    "        1 if converged, -1 if too many steps, 0 otherwise\n",
    "    '''\n",
    "    #print( 'diff2', diff2, ', cnv?', diff2<tol2, '; n_step', n_step,' < ', n_max)\n",
    "    return 1 if diff2 < tol2 else -1 if n_step >= n_max else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================================================\n",
    "#  Streamz Pipeline Sinks\n",
    "# ================================================================================================\n",
    "def make_SaveResults():\n",
    "    def save(cur):\n",
    "        save.results.append(cur)\n",
    "\n",
    "    save.results = []\n",
    "    return save\n",
    "# --------------------------------------------------\n",
    "def show_ppresults(x):\n",
    "    with np.printoptions(formatter={'float': '{: 0.4f}'.format}):\n",
    "        print( f'''{x[\"step_n\"]:3}:   {x[\"e_vec\"]},  diff2: {x[\"diff2\"]:.8g}''' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Holoviews Output Streams\n",
    "# --------------------------------------------------\n",
    "def eigenvalue_plot( buffer_size = 100 ):\n",
    "    import math\n",
    "\n",
    "    eigenvalue_estimates = pd.DataFrame({'iteration': [], 'eigenvalue': [], 'diff_squared': []}, columns=['iteration', 'eigenvalue', 'diff_squared'])\n",
    "    eigenvalue_stream    = hv.streams.Buffer(eigenvalue_estimates, length=buffer_size, index=False)\n",
    "\n",
    "    #diff2_dim = hv.Dimension( \"diff_squared\", range=(1e-18, np.nan))\n",
    "    def plot(data):\n",
    "        h = hv.Curve(data, 'iteration', 'eigenvalue',   label = 'Evolution of the Eigenvalue')+\\\n",
    "            hv.Curve(data, 'iteration', 'diff_squared', label = 'Change in the Eigenvector')\n",
    "        return h\n",
    "\n",
    "    h =\\\n",
    "    hv.DynamicMap( plot, streams=[eigenvalue_stream] )\\\n",
    "      .relabel('Evolution of the Eigenvalue')\n",
    "\n",
    "    return eigenvalue_stream, h.opts({'Curve':{'width':500, 'show_grid':True, 'logy':True} }).relabel('')\n",
    "# ----------------------------------------------------------\n",
    "def make_StreamToHoloviews( eigenvalue_stream ):\n",
    "    def stream_ToHoloviews(x):\n",
    "        eigenvalue_stream.send( pd.DataFrame({'iteration':[x['step_n']], 'eigenvalue': [x['e_val']], 'diff_squared':[np.abs(x['diff2'])]},\n",
    "                                         columns=['iteration', 'eigenvalue','diff_squared']) )\n",
    "    return stream_ToHoloviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[12., 1., -1.],\n",
    "              [20., 2.,  1.],\n",
    "              [ 0., 0., 30.]])\n",
    "\n",
    "pipeline = sz.Stream(loop=None)\n",
    "pm_out   = pipeline.map(make_Step(A))\\\n",
    "              .accumulate( step_Counter, returns_state=True, start=0)\\\n",
    "              .map(make_ConvergenceCheck(50,tol=1e-6))\n",
    "              #.rate_limit(0.5) #appears not to work in jupyter notebook :-(\n",
    "pm_out.sink(make_LoopBack(50,pipeline));\n",
    "pm_out.sink(show_ppresults);\n",
    "save = make_SaveResults()\n",
    "pm_out.sink(save);\n",
    "\n",
    "eigenvalue_stream, evolution_plot = eigenvalue_plot()\n",
    "pm_out.sink( make_StreamToHoloviews(eigenvalue_stream) );\n",
    "\n",
    "display(pipeline.visualize(rankdir=\"LR\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.emit(np.array([1.,2.,1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save.results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(\n",
    "    {'step_n': [i['step_n'] for i in save.results ],\n",
    "     'e_val':  [i['e_val' ] for i in save.results ],\n",
    "     'e_vec':  [i['e_vec' ] for i in save.results ],\n",
    "     'diff2':  [i['diff2' ] for i in save.results ]})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Inverse Power Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_power_method( A, x, n, plots, tol=1.e-10 ):\n",
    "    plots.reset_plots()\n",
    "    print( \" Iteration\\t\\t x \\t          y \\t\\t Change\")\n",
    "\n",
    "    lu = sla.lu_factor(A)\n",
    "\n",
    "    for i in range(n):\n",
    "        x_old = x\n",
    "        x     = sla.lu_solve( lu, x)        # new_x = A^.inv x <=> solve A new_x = x\n",
    "        x     = x / np.linalg.norm( x )\n",
    "\n",
    "        err   = np.linalg.norm( x-x_old )\n",
    "\n",
    "        print( f\"{i:10}\\t {x[0]: .10f}\\t   {x[1]: .10f}\\t{err: .10f}\" )\n",
    "        plots.monitor( [i, x[0], x[1], err] )\n",
    "\n",
    "        if err < tol:\n",
    "            return x, x.T @ A @ x   # return an eigenpair, lambda estimated using Rayleigh Coefficient\n",
    "\n",
    "        time.sleep(0.3)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[12., 1., -1.],\n",
    "              [20., 2.,  1.],\n",
    "              [ 0., 0., 30.]])\n",
    "\n",
    "plots = GraphicalMonitor2D(15,use_log=False)\n",
    "Markdown('**<font size=\"5\">Power Method**</font>')\n",
    "plots.plots\n",
    "\n",
    "inverse_power_method( A,\n",
    "                np.array([1,1,1]),\n",
    "                100,\n",
    "                plots\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Implementation using streamz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_power_method_step( lu, x):\n",
    "    '''implements a single step of the power method\n",
    "\n",
    "    Args:\n",
    "        A  : the np.array matrix\n",
    "        x  : the current np.array unit vector\n",
    "\n",
    "    Returns\n",
    "        diff2, eigval_estimate, eigvec_estimate, where diff2 is the l2 norm of the change in the eigvec_estimate\n",
    "    '''\n",
    "    new_x        = sla.lu_solve( lu, x)   # new_x = A^.inv x <=> solve A new_x = x\n",
    "\n",
    "    normalized_x = new_x / np.linalg.norm(new_x)\n",
    "    diff         = normalized_x - x\n",
    "\n",
    "    return normalized_x, diff @ diff\n",
    "\n",
    "def make_InverseStep(A):\n",
    "    def inverse_step(x):\n",
    "        x,diff2 = inverse_power_method_step( inverse_step.lu, x )\n",
    "        e_val   = estimate_eigenvalue( inverse_step.A, x)\n",
    "        return { 'e_val': e_val, 'e_vec': x, 'diff2': diff2}\n",
    "    inverse_step.lu = sla.lu_factor(A)\n",
    "    inverse_step.A  = A\n",
    "    return inverse_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = sz.Stream(loop=None)\n",
    "pm_out   = pipeline.map(make_InverseStep(A))\\\n",
    "              .accumulate( step_Counter, returns_state=True, start=0)\\\n",
    "              .map(make_ConvergenceCheck(50,tol=1e-8))\n",
    "#              .rate_limit(0.5) appears not to work in jupyter notebook :-(\n",
    "pm_out.sink(make_LoopBack(50,pipeline));\n",
    "pm_out.sink(show_ppresults);\n",
    "save = make_SaveResults()\n",
    "pm_out.sink(save);\n",
    "\n",
    "eigenvalue_stream, evolution_plot = eigenvalue_plot()\n",
    "pm_out.sink( make_StreamToHoloviews(eigenvalue_stream) );\n",
    "\n",
    "display(pipeline.visualize(rankdir=\"LR\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evolution_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.emit(np.array([1.,2.,1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
