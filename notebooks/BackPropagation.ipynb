{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f80b083-af89-4dfe-b070-c63b6b42e6fe",
   "metadata": {},
   "source": [
    "<div style=\"float:center;width:100%;text-align: center;\"><strong style=\"height:60px;color:darkred;font-size:40px;\">The Backpropagation Algorithm</strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c7233-f73d-47a4-879e-6f6bb6413337",
   "metadata": {},
   "source": [
    "The backpropagation algorithm uses the chain rule to update a set of derivatives.<br>\n",
    "$\\qquad$ It is easy to explain using a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64643911-fb9c-4f64-afe5-3db963ac75b6",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4524a92-a35c-4573-a1d9-61de169a0f3c",
   "metadata": {},
   "source": [
    "<div style=\"float:left;width:50%;\">\n",
    "Consider $f(x,y) = 5 x^2 -3 y, g(x,y)= 2 x + y -5$ and $h(x,y) = x y$.<br><br>\n",
    "$\\qquad$ Let's compute $\\frac{\\partial}{\\partial x} \\left. { h(f(x,y), g(x,y) } \\right|_{x=1,y=3}$<br><br>\n",
    "\n",
    "Using the chain rule, we obtain:\n",
    "\n",
    "$\\qquad \\begin{align} \\frac{\\partial}{\\partial x} { g(x,y) f(x,y) } = \\;\n",
    "      & \\left.\\frac{\\partial h(f,g)}{\\partial f}\\right|_{f=f(x,y), g=g(x,y)} \\frac{\\partial f(x,y)}{\\partial x} \\\\\n",
    "    + & \\left.\\frac{\\partial h(f,g)}{\\partial g}\\right|_{f=f(x,y), g=g(x,y)} \\frac{\\partial g(x,y)}{\\partial x}\n",
    "    \\end{align}\n",
    "$\n",
    "<br>\n",
    "\n",
    "where we have introduced variables $f$ and $g$ that will be evaluated as<br> $\\qquad f = f(x,y)$ and $g = g(x,y)$.<br><br>\n",
    "\n",
    "The graph representing the operations is shown on the right:<br>\n",
    "\n",
    "Since we are interested in specific values $x=1$ and $y=3$,<br>\n",
    "$\\qquad$ each function and each derivative<br>$\\qquad$ need to be computed for these values.\n",
    "</div><div style=\"float:left;width:50%;\">\n",
    "    <img src=\"Figs/backpropagation.svg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cbafc7-be34-4efe-b136-9776e6f9e29c",
   "metadata": {},
   "source": [
    "The computation proceeds in two phases:\n",
    "* phase 1: start at the bottom, and substitute the values for $x$ and $y$ in each of the functions as we work up toward $h$.<br>\n",
    "$\\qquad \\left.\n",
    "\\left. \\begin{align}\n",
    "x=1, y=3 \\\\\n",
    "f=5x^2-3y, g=2x+y-5\n",
    "\\end{align}\\right\\} \\Rightarrow f = -4, g= 0 \\right\\} \\Rightarrow h = f g = 0$\n",
    "\n",
    "* phase 2: start at the top, and evaluate each of the derivatives as we move down to the variables $x$ and $y$<br>\n",
    "using the values calculated in phase 1.\n",
    "\n",
    "$ \\qquad \\begin{align}\n",
    "& \\frac{\\partial h(f,g)}{\\partial f} = g = 0,    & \\frac{\\partial h(f,g)}{\\partial g} =& f = -4 & \\\\\n",
    "& \\frac{\\partial f(x,y)}{\\partial x} = 10 x =10, & \\frac{\\partial g(x,y)}{\\partial x} =& \\ 2  &\\\\\n",
    "& \\frac{\\partial f(x,y)}{\\partial y} = -3,       & \\frac{\\partial g(x,y)}{\\partial y} =& \\ 1,  & \\frac{\\partial h}{\\partial x} = -8, \\;\\;\\frac{\\partial h}{\\partial y} = -4\\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "**Remark:** this computes each of the partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c03d4e-ab8f-41ed-8186-80bb3cee4a65",
   "metadata": {},
   "source": [
    "<div style=\"float:left;width:45%;\">\n",
    "<strong>Backpropagation Phase 1</strong>\n",
    "\n",
    "<img src=\"Figs/backpropagation_phase_1.svg\">\n",
    "</div>\n",
    "<div style=\"float:right;width:45%;\">\n",
    "<strong>Backpropagation Phase 2</strong>\n",
    "<br><br>\n",
    "<img src=\"Figs/backpropagation_phase_2.svg\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dff1d3-8089-4aa1-a389-9d68ae106db9",
   "metadata": {},
   "source": [
    "**Remark:** If we do not require $\\frac{\\partial h}{\\partial y}$, we do not need to compute $\\frac{\\partial f}{\\partial y}$ and $\\frac{\\partial g}{\\partial y}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6016fb3-531e-4baf-bf52-780f5397f4ba",
   "metadata": {},
   "source": [
    "The **backpropagation** terminology is due to neural networks:<br>\n",
    "$\\qquad$ the variables $x,y$ are values at the input layer.<br>\n",
    "$\\qquad$ As we move upward in the dependency graph, we move toward the output layer.\n",
    "\n",
    "$\\qquad$ Phase 2 computes the partial derivatives at the output layer,<br>\n",
    "$\\qquad$ then propagates these values back toward the input layer<br>\n",
    "$\\qquad$ by making use of the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25614e3d-ebb7-443e-82d1-14b20d1ee655",
   "metadata": {},
   "source": [
    "____\n",
    "<div style=\"float:left;width:60%;\">\n",
    "\n",
    "**Could we compute the derivatives starting from the input layer instead?**<br>\n",
    "We certainly can compute the derivatives displayed on the edges of the graphs as before.\n",
    "\n",
    "However, to then compute $\\frac{\\partial h}{\\partial x}$ for example, we need to compute the derivatives along each path<br>$\\qquad$\n",
    "leading from the output node to the input node $x$, i.e.,\n",
    "$\\;\\;\\frac{\\partial h}{\\partial x} = \\frac{\\partial h}{\\partial f}  \\frac{\\partial f}{\\partial x} +  \\frac{\\partial h}{\\partial g} \\frac{\\partial g}{\\partial x}$.\n",
    "\n",
    "The problem with \"summing over the paths\" is that for large graphs we get a combinatorial explosion<br>\n",
    "    $\\qquad$ in the number of possible paths.\n",
    "<br><br>\n",
    "    \n",
    "To see what happens more clearly, let us **add another layer** to our example:\n",
    "\n",
    "$\\qquad$ Starting from the input layer, we obtain\n",
    "\n",
    "$\\qquad\\qquad\n",
    "\\frac{\\partial h}{\\partial a} = \\alpha_1 \\beta_1 \\gamma_1 + \\alpha_1 \\beta_3 \\gamma_2 + \\alpha_2 \\beta_2 \\gamma_1 + \\alpha_2 \\beta_4 \\gamma_2$\n",
    "\n",
    "$\\qquad$ Using backpropagation, the computation is\n",
    "\n",
    "$\\qquad\\qquad\n",
    "\\frac{\\partial h}{\\partial a} = \\left( \\gamma_1 \\beta_1 + \\gamma_2 \\beta_3 \\right) \\alpha_1 +\n",
    "\\left( \\gamma_1 \\beta_2 + \\gamma_2 \\beta_4 \\right) \\alpha_2$\n",
    "\n",
    "$\\therefore$ The computation has been factored. For large graphs, this represents a large saving!\n",
    "</div>\n",
    "<div style=\"float:left;width:40%;\"><img src=\"Figs/backpropagation_factorization.svg\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0c664-018d-4b53-ac8c-2d404ab8b1a8",
   "metadata": {},
   "source": [
    "A good explanation is given by [Chris Olah](https://colah.github.io/posts/2015-08-Backprop/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
