{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f49a823-093d-4b96-8324-38a5db085d13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "U=IndexedBase(\"U\")\n",
    "V=IndexedBase(\"V\")\n",
    "X=IndexedBase(\"X\")\n",
    "Y=IndexedBase(\"Y\")\n",
    "\n",
    "x=IndexedBase(\"x\")\n",
    "u=IndexedBase(\"u\")\n",
    "\n",
    "i,j,k,l,m,n=symbols(\"i,j,k,l,m,n\", integer=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab0628f-29b5-4c6a-977b-8f0b95b191ca",
   "metadata": {},
   "source": [
    "<div style=\"float:center;width:100%;text-align: center;\"><strong style=\"height:60px;color:darkred;font-size:40px;\">Matrix Derivatives</strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc2b77a-7ed3-4c7a-be91-81d6ecfec3c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0e260-b961-4c7f-8236-5fdf79621ff4",
   "metadata": {},
   "source": [
    "**Convention:** all vectors are **column vectors.** Note different fields use different conventions. One must check before using specific results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b31811-6ace-418a-b71b-c8a647b28f6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1 Einstein Summation Convention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd8af1-6d78-4147-9611-910bad33e25b",
   "metadata": {},
   "source": [
    "One can easily write matrix expressions by writing sums over the entries of the matrices and vectors.\n",
    "\n",
    "E.g, the matrix vector product $A x$, where $A = \\left( a_{i j} \\right)$ and $x = \\left( x_i \\right)$<br>$\\qquad$ can be written as\n",
    "\n",
    "$$\\left( A x \\right)_i = \\sum_{k=1}^{K} a_{i k} x_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c620e-c144-409c-ab84-c45feca1f56e",
   "metadata": {},
   "source": [
    "This normally results in a proliferation of summation symbols which can readily be omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb972ece-4c8c-49dd-8753-e26056ba06fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Einstein Summation Convention:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afa06a-b9ac-408e-b3d1-aca6b7ce3c7d",
   "metadata": {},
   "source": [
    "* if an **index variable** appears twice in a single term and is not otherwise defined,<br>\n",
    "$\\qquad$ it implies summation of that term over all the values of the index\n",
    "* an index that is not summed over (a **free index**) should appear only once in each term.<br>\n",
    "$\\qquad$ It will generally appear as a free index in each term of an equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580f985-c071-4248-af8e-5b8f575f546a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Examples:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fa90f-e1cf-4870-9757-9d74c32719f8",
   "metadata": {},
   "source": [
    "* The term $a_{i k} x_k$ has a repeated index $k$ that is summed over.<br>\n",
    "The index $i$ is free. This term thus stands for $\\sum_k { a_i k} x_k$.<br>\n",
    "It equals the $i^{th}$ entry of the matrix vector product $A x$.\n",
    "* The equation $p_i = a_{i j} x_j - b_i$ in matrix vector notation is $p = A x$.\n",
    "* The $trace(A) = a_{i i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb08211-91af-48b9-ab17-cfb123502980",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Kronecker Delta:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bba451-0cef-4b99-9c64-509c3169f3f9",
   "metadata": {},
   "source": [
    "$\\qquad \\delta_{i j} = \\left\\{ \\begin{align} 1 \\quad & i = j \\\\ 0 \\quad &\\text{otherwise}.\\end{align}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863ba9c-a17e-4a4a-b27a-f9ff43d02c86",
   "metadata": {},
   "source": [
    "* In a term with a Kronecker delta, a sum over a repeated index of $\\delta_{i j}$ reduces to a single entry corresponding to the other index, e.g.<br>\n",
    "$\\qquad a_{i k} \\delta_{ j k} = a_{ i j}$\n",
    "\n",
    "* This is useful with partial derivatives, for example:<br><br>$\\qquad$  $\\frac{\\partial x_i}{\\partial x_j} = \\delta_{i j}\\quad$ and $\\;\\;$\n",
    " $\\frac{\\partial x_{i j}}{\\partial x_{k l}} = \\delta_{i k} \\delta_{ j l }$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13dc10-e296-416a-8ce2-df647e04a1d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### **Levi-Civitta Density:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd646a-6b78-4352-94f9-328107f1f7fc",
   "metadata": {},
   "source": [
    "$\\qquad$ See [section 1.1.2 in 14_Determinants.ipynb](14_Determinants.ipynb)\n",
    "\n",
    "* The Leibnitz Formula for the determinant $\\det(A) = \\epsilon_{j_1 j_2\\dots j_n}\\; a_{1 j_1} a_{2 j_2} \\dots  a_{n j_n} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13e149-d5d7-4de8-8a94-125e71630a55",
   "metadata": {},
   "source": [
    "## 1.2 Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43b4f5-a54c-4193-b5ef-c9d1fdd6b048",
   "metadata": {},
   "source": [
    "The Einstein Summation Convention is useful for computing derivatives. For example\n",
    "\n",
    "$\\qquad\n",
    "\\begin{align}\n",
    "\\left( \\frac{\\partial}{\\partial x_1} A x \\right)_i &=\\ \\frac{\\partial}{\\partial x_1} a_{i j} x_j \\\\\n",
    "                                &=\\ a_{i j} \\frac{\\partial x_j}{\\partial x_1} \\\\\n",
    "                                &=\\ a_{i j} \\delta_{j 1} \\\\\n",
    "                                &=\\ a_{i 1}.\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae4661-842c-471b-bf41-eabd70c39912",
   "metadata": {},
   "source": [
    "While convenient and easy to use since we are dealing with scalar equations,<br>\n",
    "$\\qquad$ reassembling the results into matrices provides a convenient higher level view.\n",
    "\n",
    "**Remark:** There are a number of **vector and matrix layout conventions** in use.<br>$\\qquad$ In the following, we will follow the [Numerator Layout Convention](https://en.wikipedia.org/wiki/Matrix_calculus):<br>\n",
    "* vectors in numerators are laid out in columns\n",
    "* vectors in denominators are laid out in rows\n",
    "\n",
    "**Caveat Emptor:** Perusing texts or using software packages, it is **important to check the conventions** used!\n",
    "\n",
    "Below are **definitions of the vector and matrix views** of common derivatives.<br>\n",
    "$\\qquad$ To emphasize the distinction between vectors and scalars, vectors will be written in bold face.<br>\n",
    "$\\qquad$ For a step by step introduction, see [Ritvik Kharkar](https://www.youtube.com/watch?v=e73033jZTCI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d111ab-1157-4157-9075-8b76b3c7f2f3",
   "metadata": {},
   "source": [
    "### 1.2.1 Vector by Scalar Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfdddd7-f8de-47f2-8c4d-0a2e88e5e523",
   "metadata": {},
   "source": [
    "Given a vector $\\mathbf{y} = \\begin{pmatrix} y_1\\\\y_2\\\\ \\dots \\\\ y_n \\end{pmatrix} \\in \\mathbb{R}^n,\\;\\;$\n",
    "define $\\frac{\\partial \\mathbf{y}}{\\partial x} = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x} \\\\\n",
    "                                                                 \\frac{\\partial y_2}{\\partial x} \\\\\n",
    "                                                                 \\dots \\\\\n",
    "                                                                 \\frac{\\partial y_n}{\\partial x}  \\end{pmatrix},\\;\\;$ a column vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee2e85-eb9f-4449-ad8e-95392ed84575",
   "metadata": {},
   "source": [
    "### 1.2.2  Scalar by Vector Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bf852-fa3a-4670-b69a-cc9c22f7a74e",
   "metadata": {},
   "source": [
    "Given a vector $\\mathbf{x} = \\begin{pmatrix} x_1\\\\x_2\\\\ \\dots \\\\ x_n \\end{pmatrix} \\in \\mathbb{R}^n,\\;\\;$ define $\\frac{\\partial y}{\\partial \\mathbf{x}} =\n",
    "\\begin{pmatrix} \\frac{\\partial y}{\\partial x_1} & \\frac{\\partial y}{\\partial x_2} & \\dots & \\frac{\\partial y}{\\partial x_n}  \\end{pmatrix}.$\n",
    "\n",
    "If we define the gradient of a function $f: \\mathbf{x} \\in \\mathbb{R}^n \\rightarrow \\mathbb{R}$ to be a column vector,<br>\n",
    "$\\qquad$ we have $\\nabla f(\\mathbf{x}) = \\left( \\frac{\\partial f(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072a050-d19a-41e6-80e4-0ac310001a7a",
   "metadata": {},
   "source": [
    "### 1.2.3  Vector by Vector Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46badc-b3f0-4a0f-b12d-4eee81fb8254",
   "metadata": {},
   "source": [
    "Given column vectors $\\mathbf{x} \\in \\mathbb{R}^n$ and $\\mathbf{y} \\in \\mathbb{R}^m$\n",
    "\n",
    "$\\qquad\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\n",
    "\\left(\\begin{matrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\dots & \\frac{\\partial y_1}{\\partial x_n}\\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\dots & \\frac{\\partial y_2}{\\partial x_n}\\\\\n",
    "\\dots                             & \\dots                             & \\dots & \\dots                       \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\dots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{matrix}\\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9f5e7-a6eb-432d-93e3-a36c658baefa",
   "metadata": {},
   "source": [
    "### 1.2.3  Matrix by Scalar Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c13a14-1d4c-40e7-b17a-787419ba1385",
   "metadata": {},
   "source": [
    "Given matrix $\\mathbf{\\mathbf{Y}} \\in \\mathbb{R}^{m \\times n}$ and a scalar $x$\n",
    "\n",
    "$\\qquad\n",
    "\\frac{\\partial \\mathbf{Y}}{\\partial x}=\n",
    "\\left(\\begin{matrix}\n",
    "\\frac{\\partial y_{1 1}}{\\partial x} & \\frac{\\partial y_{1 2}}{\\partial x} & \\dots & \\frac{\\partial y_{1 n}}{\\partial x}\\\\\n",
    "\\frac{\\partial y_{2,1}}{\\partial x} & \\frac{\\partial y_{2,2}}{\\partial x} & \\dots & \\frac{\\partial y_{2,n}}{\\partial x}\\\\\n",
    "\\dots                             & \\dots                             & \\dots & \\dots                       \\\\\n",
    "\\frac{\\partial y_{4,1}}{\\partial x} & \\frac{\\partial y_{4,2}}{\\partial x} & \\dots & \\frac{\\partial y_{m,n}}{\\partial x}\n",
    "\\end{matrix}\\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec0a1a-c70b-44c1-ac95-e7f69046ac78",
   "metadata": {},
   "source": [
    "### 1.2.4  Scalar by Matrix Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0d1de-62c2-48ec-94a7-60fd5425eb76",
   "metadata": {},
   "source": [
    "Given matrix $\\mathbf{\\mathbf{X}} \\in \\mathbb{R}^{m \\times n}$ and a scalar $y$\n",
    "\n",
    "$\\qquad\n",
    "\\frac{\\partial y}{\\partial \\mathbf{X}}=\n",
    "\\left(\\begin{matrix}\n",
    "\\frac{\\partial y}{\\partial x_{1 1}} & \\frac{\\partial y}{\\partial x_{2 1}} & \\dots & \\frac{\\partial y}{\\partial x_{m 1}}\\\\\n",
    "\\frac{\\partial y}{\\partial x_{1 2}} & \\frac{\\partial y}{\\partial x_{2 2}} & \\dots & \\frac{\\partial y}{\\partial x_{m 2}}\\\\\n",
    "\\dots                             & \\dots                             & \\dots & \\dots                       \\\\\n",
    "\\frac{\\partial y}{\\partial x_{1 n}} & \\frac{\\partial y}{\\partial x_{2 n}} & \\dots & \\frac{\\partial y}{\\partial x_{m n}}\n",
    "\\end{matrix}\\right)\n",
    "$\n",
    "\n",
    "Note the derivative has size $n \\times m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12559fe5-ea6c-4a59-ad42-82f5aab7475b",
   "metadata": {},
   "source": [
    "### 1.2.5 The Differential of a Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23e18a5-1907-4952-94b2-541487f5125b",
   "metadata": {},
   "source": [
    "Given matrix $\\mathbf{\\mathbf{X}} \\in \\mathbb{R}^{m \\times n}$ \n",
    "\n",
    "$\\qquad\n",
    "d\\mathbf{X} =\n",
    "\\left(\\begin{matrix}\n",
    "dx_{1 1} & dx_{1 2} & \\dots & dx_{1 n}\\\\\n",
    "dx_{2 1} & dx_{2 2} & \\dots & dx_{2 n}\\\\\n",
    "\\dots & \\dots  & \\dots & \\dots \\\\\n",
    "dx_{m 1} & dx_{m 2} & \\dots & dx_{m n}\n",
    "\\end{matrix}\\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779b8fa-c2ab-400e-9752-aff9573ccf22",
   "metadata": {},
   "source": [
    "### 1.2.6 Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af97fc4-7530-4b84-9e7d-8345e999dc3d",
   "metadata": {},
   "source": [
    "Let $f( x_{1 1}, x_{1 2}, x_{2 1}, x_{2 2}) = x_{1 1}+ 2 x_{1 2} - x_{2 1} x_{2 2}\\;\\;$ which we can think of as a function $f: \\mathbb{R}^{2 \\times 2} \\rightarrow \\mathbb{R},$<br>$\\qquad$i.e., a function $f(\\mathbf X)$ for $\\mathbf X = \\begin{pmatrix} x_{1 1} & x_{1 2}\\\\ x_{2 1} & x_{2 2} \\end{pmatrix}$\n",
    "\n",
    "All possible derivatives are<br>\n",
    "$\\qquad\n",
    "\\frac{\\partial f(\\mathbf X)}{\\partial \\mathbf X}\n",
    "= \\begin{pmatrix}\n",
    "\\frac{\\partial f(\\mathbf X)}{\\partial x_{1 1}} & \\frac{\\partial f(\\mathbf X)}{\\partial x_{1 2}} \\\\ \\frac{\\partial f(\\mathbf X)}{\\partial x_{2 1}} & \\frac{\\partial f(\\mathbf X)}{\\partial x_{2 2}} \\\\ \n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix} 1 & 2 \\\\ -x_{2 2} & -x_{2 1}\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db07d25c-66d9-49a7-b4ef-d6676557b2b5",
   "metadata": {},
   "source": [
    "### 1.2.7 Vector by Matrix, Matrix by Matrix, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd83f36-f08f-4139-9089-d9af9defcdb9",
   "metadata": {},
   "source": [
    "These derivatives require more than 2 indices (tensors!). While they can be vectorized, the various notations are unwieldy.<br>\n",
    "$\\quad$ When needed, we will confine ourselves to index notation and the Einstein Summation Convention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea7ed6-38c9-4f2e-ae75-56b79f6e273a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Basic Formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d493f7-51bc-472d-b65c-fb3ab08398d4",
   "metadata": {},
   "source": [
    "For more detail, consult the [Matrix Cookbook](http://www2.imm.dtu.dk/pubdb/edoc/imm3274.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196fcc4-aa50-439e-967d-2d4df6c02cfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Sums and Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232cc2f8-00ac-4e67-8e9c-714d1f8af6fe",
   "metadata": {},
   "source": [
    "Consider two matrices $\\mathbf U$ and $\\mathbf V$ with entries that are functions of a vector $\\mathbf x$.\n",
    "\n",
    "* Consider the derivatives of a sum: $\\frac{\\partial}{\\partial x}  ( \\mathbf{U + V})$<br><br>\n",
    "$\\qquad \\left(\n",
    "\\frac{\\partial}{\\partial \\mathbf x} ( \\mathbf U +  \\mathbf V )\n",
    "\\right)_{i j k}\n",
    "= \\frac{\\partial (u_{i j} + v_{i j})}{\\partial x_k}\n",
    "= \\frac{\\partial u_{i j}}{\\partial x_k}\n",
    "+ \\frac{\\partial v_{i j}}{\\partial x_k}\n",
    "\\;\\;\\Leftrightarrow\\;\\;\n",
    "\\frac{\\partial}{\\partial x}  ( \\mathbf U +  \\mathbf V)\n",
    "= \\frac{\\partial  \\mathbf U}\n",
    "{\\partial  \\mathbf x} + \\frac{\\partial  \\mathbf V}{\\partial  \\mathbf x}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb429e1-8630-479c-9d3d-09d11c9005c0",
   "metadata": {},
   "source": [
    "* Next, look at the derivatives of a product $\\frac{\\partial (U V)}{\\partial x}$<br><br>\n",
    "$\\qquad \\left( \\frac{\\partial (U V)}{\\partial x}\\right)_{i j k}\n",
    "%= \\frac{\\partial (u_{i l} v_{l j})}{\\partial x_k} = \\frac{\\partial u_{i l}}{\\partial x_k}{v_{l j} + u_{i l} %\\frac{\\partial v_{l j}}{\\partial x_k}\n",
    "\\;\\;\\Leftrightarrow\\;\\;\n",
    "\\frac{\\partial}{\\partial \\mathbf x}  (\\mathbf{U V}) = \\frac{\\partial \\mathbf U}{\\partial \\mathbf x} \\mathbf V + \\mathbf U \\frac{\\partial \\mathbf V}{\\partial \\mathbf x}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003d49ab-b571-4415-b638-5a81b641912f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 The Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac5d1f-668f-4651-9f1d-6d6a21489c48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.2.1 Functions ${g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c8a0e-f85f-4364-b89a-5b6dd7b4d1bf",
   "metadata": {},
   "source": [
    "$\\qquad\n",
    "\\left( \\frac{\\partial f( g( \\mathbf x))}{\\partial \\mathbf x}\\right)_{i j}\n",
    "=\\ \\frac{\\partial \\left( f( g( \\mathbf x)) \\right)_{i}}{\\partial x_{j}}\n",
    "=\\ \n",
    "\\frac{\\partial g(x)_{k}}{\\partial x_{j}} \\ \n",
    "\\left.\\frac{\\partial f(\\mathbf u)_{i}}{\\partial u_{k}}\\right|_{\\mathbf u= g(\\mathbf x)}\\ \n",
    "\\;\\;\\Leftrightarrow \\;\\;\n",
    "\\large{\n",
    "\\frac{ \\partial f(g(\\mathbf x))}{\\partial \\mathbf  x} = \\ \n",
    "\\frac{\\partial g( \\mathbf x )}{\\partial \\mathbf x}\\ \n",
    "\\left. \\frac{\\partial f( \\mathbf u )}{\\partial \\mathbf u}\\right|_{\\mathbf u = g( \\mathbf x)}\\;\n",
    "}\n",
    "$\n",
    "\n",
    "**Remark:** Note the order of the matrices!\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a782c-9e80-4db0-ad52-0a5ed52a1390",
   "metadata": {},
   "source": [
    "Consider a simple example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb13b8a-a19f-4651-9775-5486f357b9e1",
   "metadata": {},
   "source": [
    "Let $\\mathbf y = \\mathbf B \\mathbf A \\mathbf x,$ where $\\mathbf A$ and $\\mathbf B$ are constant matrices.<br>\n",
    "$\\qquad$ Setting $g(\\mathbf x) = \\mathbf A \\mathbf x$, and $f(\\mathbf u) = \\mathbf B \\mathbf u,$ we have $\\mathbf y = f(g(\\mathbf x)),$<br>\n",
    "$\\qquad$ where $f(x)$ and $g(x)$ are functions transforming **vectors into vectors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308436ae-2c97-4936-aa0f-f1e1d6a0078e",
   "metadata": {},
   "source": [
    "We can compute the derivatives directly:\n",
    "\n",
    "$\\qquad\n",
    "\\frac{ \\partial y_i}{\\partial x_j} = \\frac{ \\partial b_{i k} a_{k l} x_l}{\\partial x_j} = b_{i k} a_{k l} \\delta_{l j} = b_{i k} a_{k j}\\;\\; \\Leftrightarrow \\frac{ \\partial \\mathbf y}{\\partial  \\mathbf x} =  \\mathbf A  \\mathbf B\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e634da-7ec7-4f14-ac8c-5c885161565d",
   "metadata": {},
   "source": [
    "We want to compare this to $\\frac{\\partial f( g( \\mathbf x))}{\\partial \\mathbf x}$\n",
    "\n",
    "$\\qquad$ The chain rule calls for $\\frac{\\partial f( \\mathbf u )}{\\partial \\mathbf u}$ and $\\frac{\\partial g( \\mathbf x)}{\\partial \\mathbf x}$\n",
    "\n",
    "$\\qquad \\left. \\begin{align}\n",
    "\\left( \\frac{\\partial f( \\mathbf u )}{\\partial \\mathbf u} \\right)_{i j}\n",
    "= \\frac{\\partial b_{i k} u_k}{\\partial u_j} = b_{i k} \\delta_{k j} = b_{i j} \\Rightarrow \\frac{\\partial f( \\mathbf u )}{\\partial \\mathbf u} = \\mathbf B\n",
    "\\\\\n",
    "\\left( \\frac{\\partial g( \\mathbf x )}{\\partial \\mathbf x} \\right)_{i j}\n",
    "= \\frac{\\partial a_{i k} x_k}{\\partial x_j} = a_{i k} \\delta_{k j} = a_{i j} \\Rightarrow  \\frac{\\partial g( \\mathbf x )}{\\partial \\mathbf x} = \\mathbf A\n",
    "\\end{align}\\right\\}\n",
    "\\;\\;\\Rightarrow\\;\\;\n",
    "a_{i k} b_{k j} = \n",
    "\\left( \\frac{\\partial g( \\mathbf x )}{\\partial \\mathbf x}\\right)_{i k} \n",
    "\\left( \\frac{\\partial f( \\mathbf u )}{\\partial \\mathbf u} \\right)_{k j}\n",
    "\\;\\;\\Rightarrow\\;\\;\n",
    "A B =\\ \\frac{\\partial g( \\mathbf x )}{\\partial \\mathbf x}\\;\n",
    "\\left. \n",
    "\\frac{\\partial f( \\mathbf u )}{\\partial \\mathbf u}\n",
    "\\right|_{\\mathbf u = g( \\mathbf x)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242369e-4fda-4917-a496-42ad5cd98637",
   "metadata": {},
   "source": [
    "### 2.2.2 Functions $g: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{k \\times l}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef561d-52f5-4fbd-a084-ecac783929d3",
   "metadata": {},
   "source": [
    "Apply the chain rule to $\\frac{\\partial f( g( \\mathbf X))}{\\partial \\mathbf X}.$ We get<br>\n",
    "\n",
    "$\\qquad \\large{\n",
    "\\left( \\frac{\\partial f( g( \\mathbf X))}{\\partial \\mathbf X}\\right)_{i j m n}\n",
    "=\\ \\frac{\\partial \\left( f( g( \\mathbf X)) \\right)_{i j}}{\\partial x_{m n}}\n",
    "=\\ \n",
    "\\frac{\\partial g(X)_{k l}}{\\partial x_{m n}} \\ \n",
    "\\left.\\frac{\\partial f(\\mathbf U)_{i j}}{\\partial u_{k l}}\\right|_{\\;\\mathbf U = g(\\mathbf X)}\\ \n",
    "}$\n",
    "\n",
    "These are Tensors! To fit these into matrices, we would have to vectorize them, resulting in forbidding looking formulae...\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a8922-9f44-4368-8f46-4e3079a6b2bd",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c336dda-ec00-4327-a8b5-d5dedb178618",
   "metadata": {},
   "source": [
    "Let $g(\\mathbf X) = \\mathbf X + \\mathbf X^t$ and $f(\\mathbf{X}) = \\mathbf X^2$, and consider $F(\\mathbf X) = f( g( \\mathbf X ))$\n",
    "\n",
    "$\\qquad$ We want to compute $\\frac{\\partial F(\\mathbf X)}{\\partial \\mathbf X}$<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b634c5-42da-4215-b89f-66a073d724e0",
   "metadata": {},
   "source": [
    "##### Direct Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b624f8-aaae-4d68-97d1-a6a520c89293",
   "metadata": {},
   "source": [
    "Substituting, we find $F(\\mathbf X) = \\left( \\mathbf X + \\mathbf X^t \\right)^2 = \\mathbf X^2 + ( \\mathbf X^t)^2 +  \\mathbf X^t \\mathbf X + \\mathbf X \\mathbf X^t.\\;\\;$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e8af9-0f0f-4d53-9165-bae4bd7949c8",
   "metadata": {},
   "source": [
    "$\\qquad \\begin{align}\n",
    "\\left( \\frac{\\partial F(\\mathbf X)}{\\partial \\mathbf X}\\right)_{i j m n} \n",
    "=&\\ \\frac{\\partial}{\\partial x_{m n}}\n",
    "\\left( x_{i k} x_{k j} + x_{j k} x_{k i} + x_{i k} x_{j k} + x_{k i} x_{k j}\\right) \\\\\n",
    "=&\\ x_{k j} \\delta_{i m} \\delta_{k n} + x_{i k} \\delta_{k m}\\delta_{j n}\n",
    "  + x_{k i} \\delta_{j m} \\delta_{k n} + x_{j k} \\delta_{k m}\\delta_{i n}\n",
    "  + x_{j k} \\delta_{i m} \\delta_{k n} + x_{i k} \\delta_{j m}\\delta_{k n}\n",
    "  + x_{k j} \\delta_{k m} \\delta_{i n} + x_{k i} \\delta_{k m}\\delta_{j n} \\\\\n",
    "=&\\ x_{n j} \\delta_{i m} + x_{i m} \\delta_{j n}\n",
    "  + x_{n i} \\delta_{j m} + x_{j m} \\delta_{i n}\n",
    "  + x_{j n} \\delta_{i m} + x_{i n} \\delta_{j m}\n",
    "  + x_{l j} \\delta_{i n} + x_{m i} \\delta_{j n} \\\\\n",
    "=&\\ \n",
    "   (x_{n j}+x_{j n}) \\delta_{i m}\n",
    " + (x_{m j}+x_{j m}) \\delta_{i n}\n",
    " + (x_{n i}+x_{i n}) \\delta_{j m}\n",
    " + (x_{m i}+x_{i m}) \\delta_{j n}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e04163-db2d-4c7b-b6be-5fcec50742b5",
   "metadata": {},
   "source": [
    "##### Chain Rule Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf6c2e-5f12-49b8-ada7-00ef3e069ea2",
   "metadata": {},
   "source": [
    "The chain rule applied to $\\frac{\\partial f(g(\\mathbf X))}{\\partial \\mathbf X}$ calls for $\\frac{\\partial f( \\mathbf U )}{\\partial \\mathbf U}$ and $\\frac{\\partial g( \\mathbf X)}{\\partial \\mathbf X}$\n",
    "\n",
    "$\\qquad\\qquad \\left. \\begin{align}\n",
    "&\\left( \\frac{\\partial f( \\mathbf U )}{\\partial \\mathbf U} \\right)_{i j k l}\n",
    "= \\frac{\\partial}{\\partial u_{k l}}\\left( u_{i m} u_{m j} \\right) = u_{l j} \\delta_{i k} + u_{i k} \\delta_{j l}\n",
    "\\\\\n",
    "&\\left( \\frac{\\partial g( \\mathbf X )}{\\partial \\mathbf X} \\right)_{k l m n}\n",
    "= \\frac{\\partial \\left(x_{k l} + x_{l k}\\right)}{\\partial x_{m n}} = \\delta_{k m} \\delta_{l n} + \\delta_{l m} \\delta_{k n}\n",
    "\\end{align}\\right\\}\n",
    "\\;\\;\\Leftrightarrow\\;\\; \\left(\\frac{\\partial F(\\mathbf X)}{\\partial \\mathbf X}\\right)_{i j m n} =\n",
    "\\left. \\left(\\delta_{k m} \\delta_{l n} + \\delta_{l m} \\delta_{k n}\\right)\\left( u_{l j} \\delta_{i k} + u_{i k} \\delta_{j l} \\right)\\right|_{\\ \\mathbf U = X+X^t}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7e062-f928-4194-8a56-7968102b362f",
   "metadata": {},
   "source": [
    "$\\qquad$ Simplifying, we get\n",
    "\n",
    "$\\qquad\\qquad \\left(\\frac{\\partial F(\\mathbf X)}{\\partial \\mathbf X}\\right)_{i j m n}\\ \n",
    "\\begin{align}=&\\ \\left(\n",
    "u_{n j} \\delta_{i m} +\n",
    "u_{m j} \\delta_{i n} +\n",
    "u_{i m} \\delta_{j n} +\n",
    "u_{i n} \\delta_{j m} \\right)_{\\ \\mathbf U = X+X^t}\\\\\n",
    "=&\n",
    "   (x_{n j}+x_{j n}) \\delta_{i m}\n",
    " + (x_{m j}+x_{j m}) \\delta_{i n}\n",
    " + (x_{n i}+x_{i n}) \\delta_{j m}\n",
    " + (x_{m i}+x_{i m}) \\delta_{j n}\n",
    "\\end{align}$\n",
    "\n",
    "$\\qquad$ the same expression as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfed39e-9fd4-4225-acab-64e62863609c",
   "metadata": {},
   "source": [
    "##### Why so Many Indices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd7e1d-0a64-4bfb-a741-bf92c5bb9965",
   "metadata": {},
   "source": [
    "Let's look at a matrix $\\mathbf X$ of size $2\\times 2$ for this example\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "230cb4aa-d59b-4506-98b4-0fccf2f2718b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The matrix F(X) has size 2x2, with entries made up of 4 variables\n",
      "F(X) =\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}4 {x}_{1,1}^{2} + {x}_{1,2}^{2} + 2 {x}_{1,2} {x}_{2,1} + {x}_{2,1}^{2} & 2 {x}_{1,1} {x}_{1,2} + 2 {x}_{1,1} {x}_{2,1} + 2 {x}_{1,2} {x}_{2,2} + 2 {x}_{2,1} {x}_{2,2}\\\\2 {x}_{1,1} {x}_{1,2} + 2 {x}_{1,1} {x}_{2,1} + 2 {x}_{1,2} {x}_{2,2} + 2 {x}_{2,1} {x}_{2,2} & {x}_{1,2}^{2} + 2 {x}_{1,2} {x}_{2,1} + {x}_{2,1}^{2} + 4 {x}_{2,2}^{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[                   4*x[1, 1]**2 + x[1, 2]**2 + 2*x[1, 2]*x[2, 1] + x[2, 1]**2, 2*x[1, 1]*x[1, 2] + 2*x[1, 1]*x[2, 1] + 2*x[1, 2]*x[2, 2] + 2*x[2, 1]*x[2, 2]],\n",
       "[2*x[1, 1]*x[1, 2] + 2*x[1, 1]*x[2, 1] + 2*x[1, 2]*x[2, 2] + 2*x[2, 1]*x[2, 2],                    x[1, 2]**2 + 2*x[1, 2]*x[2, 1] + x[2, 1]**2 + 4*x[2, 2]**2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xmatrix  = Matrix( [[x[r,c] for r in range(1,3)] for c in range(1,3)])\n",
    "Fmatrix = expand( (Xmatrix + Xmatrix.T)**2 )\n",
    "print(\"The matrix F(X) has size 2x2, with entries made up of 4 variables\")\n",
    "print(\"F(X) =\")\n",
    "Fmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efbffe4f-c422-4f42-92ea-801f4f61fc83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a 2x2 matrix for EACH of the partial derivatives! That's four 2x2 matrices\n",
      ".   e.g., the derivative with respect to  x[1, 1] is given by\n",
      ".   ∂F/∂x_1,1 =\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}8 {x}_{1,1} & 2 {x}_{1,2} + 2 {x}_{2,1}\\\\2 {x}_{1,2} + 2 {x}_{2,1} & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[            8*x[1, 1], 2*x[1, 2] + 2*x[2, 1]],\n",
       "[2*x[1, 2] + 2*x[2, 1],                     0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"We have a 2x2 matrix for EACH of the partial derivatives! That's four 2x2 matrices\")\n",
    "print( \".   e.g., the derivative with respect to \", x[1,1], \"is given by\")\n",
    "print( \".   ∂F/∂x_1,1 =\")\n",
    "diff(Fmatrix, x[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ebee7-8563-4926-a829-291e8ba0f5b1",
   "metadata": {},
   "source": [
    "____\n",
    "When we make use of the $\\mathbf U = g(\\mathbf X)$, we introduce an additional set of matrices $U$ and its 4 partial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bafe0196-afd3-43b3-895f-0421e6d70b29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(U)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}{u}_{1,1}^{2} + {u}_{1,2} {u}_{2,1} & {u}_{1,1} {u}_{2,1} + {u}_{2,1} {u}_{2,2}\\\\{u}_{1,1} {u}_{1,2} + {u}_{1,2} {u}_{2,2} & {u}_{1,2} {u}_{2,1} + {u}_{2,2}^{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[     u[1, 1]**2 + u[1, 2]*u[2, 1], u[1, 1]*u[2, 1] + u[2, 1]*u[2, 2]],\n",
       "[u[1, 1]*u[1, 2] + u[1, 2]*u[2, 2],      u[1, 2]*u[2, 1] + u[2, 2]**2]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff F(U) with respect to U_1,1\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 {u}_{1,1} & {u}_{2,1}\\\\{u}_{1,2} & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[2*u[1, 1], u[2, 1]],\n",
       "[  u[1, 2],       0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_f_u():\n",
    "    um =  Matrix( [[u[r,c] for r in range(1,3)] for c in range(1,3)])\n",
    "    return expand(um*um)\n",
    "def make_df_u(f_u):\n",
    "    return [ [diff(f_u, u[r,c]) for r in range(1,3)] for c in range(1,3)]\n",
    "\n",
    "f_u  = make_f_u()\n",
    "df_u = make_df_u(f_u)\n",
    "\n",
    "print(\"f(U)\")\n",
    "display(f_u)\n",
    "print(\"Diff F(U) with respect to U_1,1\")\n",
    "display(df_u[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49bf4753-dd6a-4fb4-bdf5-2cc61708593c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(X)= \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 {x}_{1,1} & {x}_{1,2} + {x}_{2,1}\\\\{x}_{1,2} + {x}_{2,1} & 2 {x}_{2,2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[        2*x[1, 1], x[1, 2] + x[2, 1]],\n",
       "[x[1, 2] + x[2, 1],         2*x[2, 2]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff g(X) with respect to x_2,1\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1\\\\1 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1],\n",
       "[1, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_g_x():\n",
    "    gm =  Matrix( [[x[r,c] for r in range(1,3)] for c in range(1,3)])\n",
    "    return gm+gm.T\n",
    "def make_dg_x( g_x ):\n",
    "    return [ [diff(g_x, x[r,c]) for r in range(1,3)] for c in range(1,3)]\n",
    "\n",
    "g_x  = make_g_x()\n",
    "dg_x = make_dg_x( g_x)\n",
    "print(\"g(X)= \")\n",
    "display(g_x)\n",
    "print(\"Diff g(X) with respect to x_2,1\")\n",
    "display(dg_x[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d45c02-5adc-4a7f-a101-0e8b3e0f8c44",
   "metadata": {},
   "source": [
    "That makes for 16 matrices that need to be computed and assembled!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc7b58-9e87-4576-b472-deea66c2877e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.3 Some Common Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cc2d5-6a3a-44e3-8413-d11e37b42b0c",
   "metadata": {},
   "source": [
    "Common derivatives readily computed using the sum product and chain rules.<br>\n",
    "In the following, $\\alpha, a, A$ and $B$ are considered constant.\n",
    "* $\\frac{\\partial}{\\partial \\mathbf x} \\mathbf A \\mathbf x = \\mathbf A$\n",
    "* $\\frac{\\partial}{\\partial \\mathbf x} (\\mathbf x^t \\mathbf A) = \\mathbf A^t$\n",
    "* $\\frac{\\partial}{\\partial \\mathbf x} (\\mathbf x^t \\mathbf A \\mathbf x) = \\mathbf x^t \\left( \\mathbf A + \\mathbf A^t \\right)$\n",
    "* $\\frac{\\partial}{\\partial t} \\mathbf X^{-1} = - \\mathbf X^{-1} \\frac{d \\mathbf X}{dt} X^{-1}$, assuming $x_{i j} = x_{i j}(t)$.\n",
    "\n",
    "The last identity is readily obtained by taking the derivatives of $X^{-1} X = I$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae3ebd-ef96-458f-83a0-a9a947dc5e37",
   "metadata": {},
   "source": [
    "Derivatives of Common Functions\n",
    "* $\\frac{\\partial}{\\partial \\mathbf x} \\Vert x - a \\Vert^2 = 2 ( x - a ) \\Rightarrow \\frac{\\partial}{\\partial \\mathbf x} \\Vert x - a \\Vert =  \\frac{ x - a }{\\Vert x - a \\Vert }$ \n",
    "* $\\frac{\\partial}{\\partial \\mathbf X} \\Vert A x \\Vert^2 = 2 x^t A^t A$\n",
    "* $\\frac{\\partial}{\\partial \\mathbf X} trace\\mathbf (X) = I\\quad$ ($\\mathbf X$ is a square matrix)\n",
    "* $\\frac{\\partial}{\\partial \\mathbf X} \\Vert \\mathbf X \\Vert^2 = 2 \\mathbf X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be7182-9c3c-4706-8e65-142f52dc5510",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.4 Differentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bf2b1-dbd0-4ce4-9795-c53b081a90d7",
   "metadata": {},
   "source": [
    "As shown above, derivatives can be tediously computed via partials,<br>$\\qquad$\n",
    "but they instead can be computed directly with matrix manipulations.<br>$\\qquad$\n",
    "In the following, we use an article by [Thomas P. Minka](https://tminka.github.io/papers/matrix/minka-matrix.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d821e-3e5c-4efc-b66f-6cf1a43cdffc",
   "metadata": {},
   "source": [
    "The idea is to use differential notation: for a differentiable function $y = f(x)$,<br>$\\qquad$\n",
    "the function can be approximated by a tangent hyperplane.\n",
    "\n",
    "Let $dx$ be a given step size starting from some point $x$, and let $dy$ be the resulting change in $y$<br>$\\qquad$\n",
    "in the tangent hyperplane, then\n",
    "\n",
    "$\\qquad y(x + dx) = y(x) + f'(x) dx + \\text{(higher order terms)} \\Rightarrow dy = f'(x) dx$\n",
    "\n",
    "$\\qquad$ This equation applies even when $y$ and $x$ are not scalars. E.g., <br>\n",
    "$\\qquad\\qquad dy = \\frac{ \\partial f(x) }{ \\partial x} dx, \\;\\;\\;$\n",
    "where $dx = \\begin{pmatrix} dx_1\\\\ dx_2\\\\ \\dots \\\\ dx_n \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b38c66-b034-40f2-813b-2fc85dbde9bb",
   "metadata": {},
   "source": [
    "#### **Computations of Derivatives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f018698c-4d10-4207-bec3-7fac7e9a9cf6",
   "metadata": {},
   "source": [
    "To compute a given derivative,\n",
    "* compute the differential using the expressions given below\n",
    "* put the result in canonical form and read of the derivative as the coefficient of $dx$, $d\\mathbf{x}$ or $d\\mathbf{X}$.<br>\n",
    "This can prove difficult! One way to address this is to \"vectorize\" the result, yielding equations that are hard to read and interpret. It may be easier to switch to index notation at this point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794cd575-2078-4a4b-af28-283f93b23360",
   "metadata": {},
   "source": [
    "Let $x$ and $y$ be variables (scalars, vectors and/or matrices), and $\\alpha$ and $\\mathbf{A}$ be constants.\n",
    "\n",
    "The following differentials are easily established using the Einstein Summation Convention:\n",
    "* $d\\mathbf{A} = 0$\n",
    "* $d(\\alpha \\mathbf{X}) = \\alpha\\ d\\mathbf{X}$\n",
    "* $d(\\mathbf{X}+\\mathbf{Y}) = d\\mathbf{X} + d\\mathbf{Y}$\n",
    "* $d( \\mathbf{X}\\mathbf{Y} ) = d\\mathbf{X}\\ \\mathbf{Y} + \\mathbf{X}\\ d\\mathbf{Y}$\n",
    "* $d\\mathbf{X}^{-1} =  -\\mathbf{X}^{-1}\\ d\\mathbf{X}\\ \\mathbf{X}^{-1}$\n",
    "* $d( trace(\\mathbf{X}) ) = trace( d\\mathbf{X} )$\n",
    "* $d( det\\mathbf{X} ) = det\\mathbf{X}\\ trace\\left( \\mathbf{X}^{-1}\\ d\\mathbf{X} \\right)$\n",
    "* $d( log\\left(det\\left(\\mathbf{X}\\right)\\right)) = trace\\left(\\mathbf{X}^{-1}\\ d\\mathbf{X}\\right)$\n",
    "* $d\\mathbf{X}^{*}= (d\\mathbf{X})^{*},\\;\\;\\;$ where ⋆\n",
    "is any operator that rearranges elements, e.g. transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4204c606-fe4f-4aec-9407-b11bc4de2348",
   "metadata": {},
   "source": [
    "A useful relationship to know is $trace(A B) = trace( B A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75845eb-08f6-41b9-83b4-b755a46f4777",
   "metadata": {},
   "source": [
    "___\n",
    "Let's establish the formula for the derivative of $\\mathbf X^{-1}$.<br>\n",
    "$\\qquad$ An easy way is to consider $\\mathbf X \\mathbf X^{-1} = \\mathbf I$.\n",
    "\n",
    "$\\qquad$ By the product rule the differential is $d\\mathbf X\\ \\mathbf X^{-1}\\ \\mathbf X + \\mathbf X\\ d\\mathbf X^{-1} = 0$. Solving for $d\\mathbf X^{-1}$,<br>\n",
    "$\\qquad$ we obtain $d\\mathbf X^{-1} = - \\mathbf X^{-1}\\ d\\mathbf{X}\\ \\mathbf X^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2453e-bcd8-40d8-9c36-8ed39226cae2",
   "metadata": {},
   "source": [
    "$\\qquad$ If for example $\\mathbf X$ is a function of a variable $t$, we have $d \\mathbf X = \\frac{ d \\mathbf X}{dt} dt$,<br>\n",
    "$\\qquad\\qquad$ so that $\\frac{d \\mathbf X^{-1}}{ dt } = \\mathbf X^{-1} \\frac{d \\mathbf X}{ dt }  \\mathbf X^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a5bc8-ebc5-4e59-bdbe-890eb3b99a69",
   "metadata": {},
   "source": [
    "# 3. Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a14770-6091-4e6e-865c-3b70670889de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1. Neural Network Weight Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3b01b-2c79-4364-b2eb-685500386f2c",
   "metadata": {},
   "source": [
    "Neural network updates take the form $\\mathbf{w} = \\mathbf{X} \\mathbf{v},$ where $\\mathbf X$ contains weights that need to be updated.<br>\n",
    "$\\quad$ The weight update equations require $\\frac{\\partial \\mathbf{w}}{\\partial \\mathbf{X}} = \\frac{\\partial (\\mathbf{X v})}{\\partial \\mathbf{X}}$ \n",
    "\n",
    "In Einstein Summation Convention notation, this is\n",
    "\n",
    "$\\qquad\n",
    "\\left( \\frac{\\partial (\\mathbf{X v})}{\\partial \\mathbf{X}}\\right)_{i j k}\n",
    "=\\ \\frac{ \\partial}{\\partial x_{j k}}\n",
    "X_{i l} v_l\n",
    "=\\ \\delta_{i j} \\delta_{k l} v_l\n",
    "=\\ \\delta_{i j} v_k\n",
    "$\n",
    "\n",
    "i.e.,<br>\n",
    "$\\qquad\n",
    "\\left( \\frac{\\partial (\\mathbf{X v})}{\\partial \\mathbf{X}}\\right)_{i j k}\n",
    "= \\left\\{\\begin{align}\n",
    "v_k \\quad & \\text{ if } i \\ne j \\\\\n",
    "0 \\quad & \\text{otherwise}\n",
    "\\end{align}\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725031d4-eaf8-4d70-b74b-6965831d0a77",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2 Least Mean Squares Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c1c57-75e9-4ba4-b2f2-d2c153b84b24",
   "metadata": {},
   "source": [
    "The least mean squares solution of $A x - b$ is given by $argmin_x \\Vert A x - b \\Vert$. \n",
    "\n",
    "$\\qquad$ Let $F(x) = \\Vert A x - b \\Vert^2 = x^t A^t A x - x^t A^t b - b^t A x + b^t b$<br>\n",
    "$\\qquad$ and compute the gradient $\\nabla F(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c2226-39d5-4744-8e83-c8927d4fe931",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial F(x)}{\\partial x_l}\n",
    "=&\\ \\frac{\\partial}{\\partial x_l}  \\left( x_i A_{j i} A_{j k} x_k - x_i A_{j i} b_j - b_i A_{i j} x_j + b_i b_i \\right) \\\\\n",
    "=&\\ A_{j i} A_{j k} \\delta_{i l} x_k + A_{j i} A_{j k} \\delta_{k l} x_i - A_{j i} b_j \\delta_{i l} - A_{i j} b_i \\delta_{j l} \\\\\n",
    "=&\\ A_{j l} A_{j k} x_k + A_{j k} A_{j l} x_k  - A_{j l} b_j - A_{j l} b_j \\\\ \n",
    "=&\\ 2 A_{j l} \\left( A_{j k} x_k - b_j \\right)\n",
    "\\end{align}\n",
    "\n",
    "Hence\n",
    "\n",
    "$\\qquad \\frac{\\partial}{\\partial x} \\Vert A x - b \\Vert^2 = 2 A^t (A x - b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0b33d-8eb8-41c3-8a02-c56b76c191a1",
   "metadata": {},
   "source": [
    "## 3.3 The Rayleigh Quotient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469407e-1ac1-47ba-8d2e-1ced9910f04e",
   "metadata": {},
   "source": [
    "The derivative of the Rayleigh Coefficient\n",
    "\n",
    "$\\qquad \\mathbf x^t \\mathbf A \\mathbf x = a_{i j} x_i x_j$, so $R( \\mathbf x) = \\frac{a_{i j} x_i x_j}{ x_k x_k }$\n",
    "\n",
    "The numerator and denominator are scalars, so we can use the quotient rule.\n",
    "\n",
    "* Derivative or the numerator: $\\qquad\\;\\;\\frac{\\partial a_{i j} x_i x_j  }{\\partial x_l} = { a_{i j} (\\delta_{i l} x_j + \\delta_{j l} x_i )} = a_{l j} x_j + a_{i l} x_i$\n",
    "* Derivative of the denominator: $\\qquad\\frac{\\partial x_k x_k}{x_l} = 2 \\delta_{k l} x_k = 2 x_l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75510e40-09a3-409c-89f4-dff941c082ee",
   "metadata": {},
   "source": [
    "$\\quad \\therefore \\;\\; \\frac{\\partial R(\\mathbf x)}{\\partial x_l} = \\frac{ (a_{l j} x_j + a_{i l} x_i) x_s x_s - 2 a_{i j} x_i x_j x_l}{(x_k x_k)^2}\n",
    "\\;\\;\\Leftrightarrow\\;\\; \\frac{\\partial R(\\mathbf x)}{\\partial \\mathbf x} = \\frac{ ( \\mathbf A + \\mathbf A^t) \\mathbf x  - 2 R(\\mathbf x) \\mathbf x }{ \\mathbf x^t \\mathbf x }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c45864-654f-45da-880e-9a21e421b041",
   "metadata": {},
   "source": [
    "Since the matrix $\\mathbf A$ for the Rayleigh quotient $R(\\mathbf x)$ is symmetric, we finally have<br>\n",
    "$\\qquad \\frac{\\partial R(\\mathbf x)}{\\partial \\mathbf x} = 2 \\frac{A x}{x^t x} -2 R(x) \\frac{x }{x^t x}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
