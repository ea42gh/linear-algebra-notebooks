{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><p style=\"color:darkred;font-size:30pt;height:15px;text-align:left;\">Orthogonality</p></div>"
      ],
      "text/plain": [
       "HTML{String}(\"<div><p style=\\\"color:darkred;font-size:30pt;height:15px;text-align:left;\\\">Orthogonality</p></div>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra, RowEchelon, LaTeXStrings, Plots, SymPy\n",
    "include(\"LAcodes.jl\");\n",
    "LAcodes.title( \"Orthogonality\", sz=30, color=\"darkred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# 1. Adding Vector Length to Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 1.1 Basic Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The definitions below are carefully written to generalize to arbitrary vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "#### **Allow Both Real and Complex Numbers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Adding the notion of the **length of a vector** to a vector space proceeds in two steps.<br>\n",
    "In the following, we allow the scalars to be $\\mathbb{F} = \\mathbb{R}, \\mathbb{Q}$ or $\\mathbb{C}$.\n",
    "\n",
    "To allow complex numbers, we need to allow complex conjugate:<br>\n",
    "a bar over an expression signifies complex conjugation.\n",
    "\n",
    "**Example:**  $\\overline{3 + 2 i} = 3 - 2 i.$<br>\n",
    "$\\quad\\quad\\quad\\;$ Note that for real numbers $\\overline{x} = x.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "#### **Inner Products (Dot Product)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9\">\n",
    "    \n",
    "**Definition:** An **inner product space** is a vector space $V$ over the scalars $\\mathbb{F}$\n",
    " with a function $\\; \\cdot : V \\times V \\rightarrow \\mathbb{F}$<br> $\\quad\\quad$ with the following properties\n",
    "$\\forall x,y,z \\in V, \\; \\forall \\alpha\\in \\mathbb{F}:$\n",
    "$$\n",
    "\\begin{align}\n",
    " &x \\cdot y          &=& \\;\\overline{ y \\cdot x } & \\text{(conjugate symmetry)             } \\\\\n",
    " &x \\cdot (\\alpha y)  &=& \\;\\alpha x \\cdot y       & \\text{(linearity in the second argument)} \\\\\n",
    " &x \\cdot ( y+z )     &=&\\; x \\cdot y + x \\cdot z        & \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "x \\cdot x\\quad\\quad\\quad  = \\left\\{ \\begin{align}& c > 0 & \\quad x \\ne 0\\\\ & 0 \\quad & \\text{ otherwise} \\end{align} \\right. \\quad\\quad \\text{ (positive definite)}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Remark:** The complex conjugate is required for complex numbers, since the dot product is not positive definite.\n",
    "$$\n",
    "\\begin{align} (\\alpha + \\beta i ) \\cdot (\\alpha + \\beta i ) & = \\alpha^2 - \\beta^2 + 2 \\alpha \\beta\\; i  \\\\\n",
    " \\overline{(\\alpha + \\beta i )} \\cdot (\\alpha + \\beta i ) & = \\alpha^2 + \\beta^2  \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> **Note** that for modulo 2 arithmetic, i.e., $\\mathbb{F} = \\mathbb{Z}_2$ the dot product is not positive definite.\n",
    "> \n",
    "> <span style=\"color:red;\"><strong>ALL THEOREMS FROM HERE ON REQUIRE THE POSITIVE DEFINITE PROPERTY of the dot product</strong></span>\n",
    ">\n",
    "> This is why the Fundamental theorem is broken into two parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "#### **Distance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Inner products can be used to define a **distance** function, i.e.,\n",
    "<div style=\"background-color:#F2F5A9;float:left;width:18cm;\">\n",
    "\n",
    "**Definition:** A **metric** for a set $M$ is a function $d : M \\times M \\rightarrow \\mathbb{R}$<br>\n",
    "$\\quad\\quad$ with the following properties\n",
    "$\\forall x,y,z \\in V, \\; \\forall \\alpha\\in \\mathbb{F}:$\n",
    "$$\n",
    "\\begin{align}\n",
    "d(x,y) & = 0 \\; \\Leftrightarrow \\; x = y & \\\\\n",
    "d(x,y) & = d(y,x) & \\\\\n",
    "d(x,y) & \\le d(x,z) + d(z,y) & \\quad\\quad \\text{ ( triangle inequality ) }\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Remark**: The axioms for a metric guarantee $$d(x,y) \\ge 0$$\n",
    "</div><img src=\"NormAndDistance.svg\" style=\"float:center;\" width=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### **Length of a Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;width:18cm;\">\n",
    "\n",
    "**Definition:** The **norm** of a vector $v$ in an inner product space\n",
    "    $$\\lVert v \\rVert = \\sqrt{ \\overline{v} \\cdot v }$$\n",
    "**Definition:** The **distance** between two vectors $x$ and $y$ in an inner product space is\n",
    "    $$ d(x,y) = \\lVert x-y \\rVert $$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### **Remarks:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* For the dot product in $\\mathbb{R}^2$ and $\\mathbb{R}^3$, this definition yields the Euclidean length of a vector. E.g.,\n",
    "$$\\lVert \\begin{pmatrix}v_1\\\\v_2 \\end{pmatrix} \\rVert = \\sqrt{ v_1^2 + v_2^2 }$$\n",
    "* The definition of the norm from the inner product shows that\n",
    "$$\n",
    "\\lVert \\alpha v \\rVert = \\sqrt{ \\overline{\\alpha v} \\cdot \\alpha v  } = \\ \\lvert \\alpha \\rvert \\ \\lVert v \\rVert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let $$u = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}, v = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}.$$\n",
    "\n",
    "$$\n",
    "\\lVert u \\rVert = \\sqrt{ 2^2 + 5^2 } = \\sqrt{29}, \\quad \\lVert v \\rVert = \\sqrt{ 3^2 + (-1)^2 } = \\sqrt{10}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "$$\\lVert 2 u \\rVert = \\sqrt{ \\begin{pmatrix} 4\\\\10 \\end{pmatrix} \\cdot \\begin{pmatrix} 4\\\\10 \\end{pmatrix} } = \\sqrt{ 4 \\begin{pmatrix} 2\\\\5 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} } = 2 \\sqrt{ \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} \\cdot \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} }$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9\">\n",
    "\n",
    "**Definition:** A **unit vector** is a vector with norm equal to 1.\n",
    "    \n",
    "Such a vector may be constructed from any non-zero vector $u$ with\n",
    "$$\n",
    "\\hat{u} = \\frac{1}{\\lVert u \\rVert} u\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    u = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} & \\Rightarrow \\lVert u \\rVert = \\sqrt{29} \\\\\n",
    "                                             & \\Rightarrow \\hat{u} =\\frac{1}{\\sqrt{29}}\n",
    "                                                            \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 1.2 Inequalities, Angle, Orthogonal Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "#### **Orthogonal Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;\">\n",
    "\n",
    "**Constructing Orthogonal Vectors in <span style=\"color:red;\">2D and 3D</span>:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lVert u+v \\rVert = \\lVert u-v \\rVert\n",
    "& \\Leftrightarrow \\sqrt{ (u+v) \\cdot (u+v) } =  \\sqrt{ (u-v) \\cdot (u-v) } \\\\\n",
    "& \\Leftrightarrow      { (u+v) \\cdot (u+v) } =       { (u-v) \\cdot (u-v) } \\\\\n",
    "& \\Leftrightarrow      { \\lVert u \\rVert^2 + \\lVert v \\rVert^2 + 2 u \\cdot v } = \n",
    "                       { \\lVert u \\rVert^2 + \\lVert v \\rVert^2 - 2 u \\cdot v } \\\\\n",
    "& \\Leftrightarrow      { u \\cdot v = 0 } \\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Orthogonal Vectors:**\n",
    "$$u \\perp v \\Leftrightarrow u \\cdot v = 0$$\n",
    "\n",
    "</div><div style=\"float:right;\"><img src=\"OrthogonalDirection.svg\" width=300></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Remark:** The construction shown allows the vector $v = 0$.\n",
    "\n",
    "<div style=\"background-color:#F2F5A9;\">\n",
    "\n",
    "**Definition:** The **zero vector** in is orthogonal to any other vector in the metric space.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### **Generalization: Angles Between Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9\">\n",
    "\n",
    "**Theorem: (Cauchy-Schwartz Inequality)** The inner product between two vectors $u$ and $v$ satisfies $$\\lvert \\overline{u} \\cdot v \\rvert \\le \\lVert u \\rVert \\ \\lVert v \\rVert$$\n",
    "</div>\n",
    "\n",
    "**Remark:** The equality is trivially satisfied if either $u = 0$ or $v = 0$. When neither of the vectors is zero, we can rewrite this as\n",
    "$$\n",
    "-1 \\le \\frac{ \\overline{u} \\cdot v }{\\lVert u \\rVert \\ \\lVert v \\rVert} \\le 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "In $\\mathbb{R}^2$ this quotient is the cosine of the angle between the vectors $u$ and $v$.<br>\n",
    "we therefore generalize this to\n",
    "<div style=\"background-color:#F2F5A9\">\n",
    "\n",
    "$$\n",
    "\\cos ( \\angle (u,v) ) = \\frac{ \\overline{u} \\cdot v }{\\lVert u \\rVert \\ \\lVert v \\rVert}, \\quad \\text{ where } u \\ne 0, v \\ne 0\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Remarks:**\n",
    "* orthogonal non-zero vectors have $\\; cos\\ 90^\\circ = 0$,<br>we recover our previous result\n",
    "$$ u \\cdot v = 0 \\Leftrightarrow u \\perp v $$\n",
    "* the definition for the angle is frequently rewritten\n",
    "$$ \\overline{u} \\cdot v = \\lVert u \\rVert\\ \\lVert v \\rVert \\ \\cos ( \\angle (u,v) )$$\n",
    "which holds for all vectors $u, v$ (including zero vectors). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let $u = \\begin{pmatrix} 1 \\\\ 5 \\\\ 3 \\end{pmatrix}, \\;\\; v = \\begin{pmatrix} 4 \\\\ 1 \\\\ 1 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The **angle between $u$ and $v$**  is $\\theta = arccos \\frac{u \\cdot v}{ \\lVert u \\rVert\\ \\lVert v \\rVert } \n",
    " = arccos \\frac{12}{ \\sqrt{35}\\ \\sqrt{18} } \n",
    " \\approx $ 61.44 degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The **distance from $u$ to $v$** is $\\quad\\quad\\quad\\quad\\quad\\quad\\lVert v - u \\rVert =\\ \\lVert\\ \\begin{pmatrix} 3 \\\\ -4 \\\\ -2 \\end{pmatrix}\\ \\rVert\\; \\approx 5.39$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "A **detour** via $w = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ increases the distance to $\\lVert v-w \\rVert + \\lVert w-u \\rVert  \\approx 7.42$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# 2. Fundamental Theorem of Linear Algebra (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 2.1 Main Definitions and Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 2.1.1 Linear Independence of Orthogonal Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;background-color:#F2F5A9;width:15cm;\">\n",
    "\n",
    "**Theorem:** Non-zero orthogonal vectors are **linearly independent.**\n",
    "\n",
    "</div><div style=\"float:center;\">\n",
    "$\\quad\\quad$ Let $u \\perp v \\Leftrightarrow u \\cdot v = 0.$ \n",
    "    \n",
    "$\\quad\\quad\\begin{align}\n",
    "\\alpha u + \\beta v = 0 & \\Rightarrow \\alpha u \\cdot u + \\beta u \\cdot v = 0 \\\\\n",
    "                       & \\Rightarrow \\alpha \\lVert u \\rVert^2 = 0.\\\\\n",
    "\\end{align}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<div style=\"float:left;background-color:#F2F5A9;width:15cm;\">\n",
    "\n",
    "**Fundamental Theorem:** Given a matrix $A$ in $\\mathbb{R}^{M \\times N}$\n",
    "* Any two vectors $r \\in \\mathscr{R}(A), n \\in \\mathscr{N}(A)$ are orthogonal, i.e., $r \\perp n$\n",
    "* Any two vectors $c \\in \\mathscr{C}(A), \\tilde{n} \\in \\mathscr{N}(A^t)$ are orthogonal, i.e., $c \\perp \\tilde{n}$\n",
    "\n",
    "\n",
    "---\n",
    "**Remark:**\n",
    "* The sketch of the Fundamental Theorem presented previously<br>\n",
    "    depicts this situation accurately!<br><br>\n",
    "    **Vectors in the two fundamental spaces<br>in the domain and the codomain are orthogonal**.\n",
    "</div>\n",
    "<div style=\"float:center;\">$\\quad$<img src=\"FundamentalTheorem_0.svg\" width=250></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> Start with a vector in the nullspace $\\mathscr{N}(A),$ i.e., $A x = 0:$\n",
    "> \n",
    "> $\\quad\\quad A x = 0 \\Rightarrow \\text{rows of } A \\cdot x = 0, $ so each row of $A$ is orthogonal to $x$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> Since any vector in the row space can be written as a linear combination of the rows,<br>\n",
    "> $\\quad\\quad (\\alpha_1 R_1 + \\alpha_2 R_2 + \\dots \\alpha_M R_M ) \\cdot x = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> $ \\therefore \\quad$ **any vector in $\\mathscr{R}(A)$ is orthogonal to any vector in $\\mathscr{N}(A)$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> **Remark:**<br>\n",
    "> Given one or more vectors $a_i, i=1,2, N$, we now know<br>\n",
    "**how to find a vector that is orthogonal**\n",
    "to the hyperplane $span \\{ a_1, a_2, \\dots a_n \\}:$\n",
    ">\n",
    "> * write the $a_i$ into a matrix $A$ as rows, and find a vector in the nullspace (a homogeneous solution of $A x = 0$ )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 2.1.2 Mutually Orthogonal Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> Look at the following 3 Vectors<br><br>\n",
    "> $ a_1 = \\begin{pmatrix} -4\\\\ -8\\\\  1 \\end{pmatrix}, \\;\n",
    "  a_2 = \\begin{pmatrix}  7\\\\ -4\\\\ -4 \\end{pmatrix}, \\;\n",
    "  a_3 = \\begin{pmatrix}  4\\\\ -1\\\\  8 \\end{pmatrix} \\quad \\Rightarrow \\quad\n",
    " \\left\\{ \\begin{align}  a_1 \\cdot a_1 =  81, &\\quad a_2 \\cdot a_2 =  81, \\quad a_3 \\cdot a_3 = 81, \\\\\n",
    "                        a_1 \\cdot a_2 = \\;0, &\\quad a_1 \\cdot a_3 = \\;0, \\quad\\; a_2 \\cdot a_3 = \\;0. \\end{align} \\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> The dot products show\n",
    "> * each of the vectors $a_i$ has length $\\lVert a_i \\rVert = \\sqrt{ a_i \\cdot a_i } = 9.$\n",
    "> * each of the vectors $a_i$ is orthogonal to the other two: $a_i \\cdot a_j = 0\\;$ for $i \\ne j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;float:left;width:12cm;\">\n",
    "\n",
    "**Definition:** A set of vectors $a_1, a_2, \\dots a_n$ is **mutually orthogonal**$\\quad$<br>\n",
    "$\\quad\\quad$ iff for all $i, j$ in $1,2,\\dots N,$<br><br>\n",
    "$$\n",
    "a_i \\cdot a_j = \\left\\{ \\begin{align} \\; \\lVert a_i \\rVert^2 \\ne 0 \\quad  &\\ \\text{ when } i = j\\\\ 0 \\quad &\\ \\text{ otherwise} \\end{align} \\right.\n",
    "$$\n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;\">\n",
    "\n",
    "**Remarks:**\n",
    "* If the vectors $a_i$ are the columns of a matrix $A$, then<br>\n",
    "$\\quad\\quad$  **the matrix $A^t A$ is diagonal**<br>\n",
    "$\\quad\\quad$  with entry $i,i$ equal to $a_i \\cdot a_i = \\lVert a_i \\rVert^2 .$\n",
    "* the matrix $A^t A$ is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Example Revisited**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;width:12cm\">\n",
    "> Let $A = \\begin{pmatrix} -4 & 7 & 4 \\\\ -8 & -4 & -1 \\\\  1 & -4 & 8 \\end{pmatrix} \\Rightarrow\n",
    "A^t A = \\begin{pmatrix} 81 & 0 & 0 \\\\ 0 & 81 & 0 \\\\ 0 & 0 & 81 \\end{pmatrix}\n",
    "$\n",
    "</div><div style=\"float:right;\">\n",
    "\n",
    "**Remark:** the example created a square matrix $A$.<br>$\\quad\\quad$ We can have more entries in a vector<br>\n",
    "$\\quad\\quad$ than vectors, e.g.,<br><br>\n",
    "$\\quad\\quad A = ( a_1 \\; a_2 )$.\n",
    "    \n",
    "The resulting matrix $A^t A$ is always **square.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Important Remarks:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* A set of mutually orthogonal vectors is often called an **orthogonal set** of vectors\n",
    "* The vectors in an **orthogonal set** are **linearly independent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* The vectors in an **orthogonal set form a basis** for their span.\n",
    "* If the vectors in an **orthogonal set** are **unit vectors**, then $A^t A = I$.<br>\n",
    "  $\\quad\\quad$ Think $i,j$ coordinate vectors, possibly in $\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 2.1.3 Orthogonal Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;float:left;width:15cm\">\n",
    "\n",
    "**Definition:** A vector space $U$ is orthogonal to a vector space $V$<br>\n",
    "    $\\quad\\quad$ iff $\\forall u \\in U, \\forall v \\in V, \\; u \\perp v$\n",
    "\n",
    "**Definition:** Let $U$ be a subspace of $V$.<br>\n",
    "    $\\quad\\quad$ The **orthogonal complement** $U^\\perp = \\left\\{ v \\in V \\mid \\forall u \\in U, \\ v \\perp u \\right\\}$.\n",
    "\n",
    "**Theorem:** Given a vector space $U$, then $(U^\\perp)^\\perp = U$.<br>\n",
    "**Theorem:** Given two vector spaces $U$ and $V$ such that $U^\\perp = V$, then $V^\\perp = U$.\n",
    "\n",
    "</div>\n",
    "<div style=\"float:right;\"><img src=\"FundamentalTheorem_1.svg\" width=250></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;float:left;width:15cm\">\n",
    "\n",
    "**Fundamental Theorem:** Given a matrix $A \\in \\mathbb{R}^{M \\times N}$<br>\n",
    "    $\\quad\\quad$ then $\\mathscr{R}(A)^\\perp = \\mathscr{N}(A)$ in $\\mathbb{R}^N$.<br>\n",
    "**Fundamental Theorem:** Given a matrix $A \\in \\mathbb{R}^{M \\times N}$<br>\n",
    "    $\\quad\\quad$  then $\\mathscr{C}(A)^\\perp = \\mathscr{N}(A^t)$ in $\\mathbb{R}^M$.\n",
    "\n",
    "**Fundamental Theorem:** Let $A$ be a matrix of size $M \\times N.$<br>\n",
    "    $\\quad\\quad$  The union of the bases for $\\mathscr{C}(A)$ and $\\mathscr{N}(A^t)$ is a basis for $\\mathbb{R}^M$.<br>\n",
    "**Fundamental Theorem:** Let $A$ be a matrix of size $M \\times N.$<br>\n",
    "    $\\quad\\quad$  The union of the bases for $\\mathscr{R}(A)$ and $\\mathscr{N}(A)$   is a basis for $\\mathbb{R}^N$.\n",
    "</div>\n",
    "<div style=\"float:right;\"><img src=\"FundamentalTheorem_2.svg\" width=250></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 2.2 Use the Fundamental Theorem to Decompose a Vector (Naive Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let $A$ be a matrix of size $M \\times N$ with rank $r$.<br><br>\n",
    "Let $\\left\\{ c_1, c_2, \\dots c_r \\right\\}$ be a basis for $\\mathscr{C}(A)$,<br>\n",
    "and $\\left\\{ ñ_1, ñ_2, \\dots ñ_{M-r} \\right\\}$ be a basis for $\\mathscr{N}(A')$.<br>\n",
    "\n",
    "The combined basis $\\quad\\left\\{\\  c_1, c_2, \\dots c_r,\\;  ñ_1, ñ_2, \\dots ñ_{M-r} \\ \\right\\}$ $\\quad\\quad$\n",
    "is a basis for $\\mathbb{R}^M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> Let $A = \\begin{pmatrix}  1  & 2 \\\\ 2 & 2 \\\\ -1 & 1 \\end{pmatrix}$.\n",
    ">\n",
    "> A quick computation shows\n",
    "> $$\n",
    "\\text{basis } \\mathscr{C}(A) = \\left\\{\\;\n",
    "c_1= \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\; c_2 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} \\; \\right\\},\\quad\\quad\n",
    "\\text{basis } \\mathscr{N}(A^t) = \\left\\{\\ \\tilde{n}\\ = \\ \\begin{pmatrix} -4 \\\\ 3 \\\\ 2 \\end{pmatrix} \\; \\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> Combining the two bases yields a basis for $\\mathbb{R}^3$:\n",
    "> $$\n",
    "\\text{basis } \\mathbb{R}^3 = \\left\\{\\;\n",
    "\\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\;  \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} \\;\n",
    "\\begin{pmatrix} -4 \\\\ 3 \\\\ 2 \\end{pmatrix} \\; \\right\\}.\n",
    "$$\n",
    ">\n",
    "> $\\therefore\\quad$ **Any vector $b$ in $\\mathbb{R}^3$ can be written as a unique linear combination**\n",
    "$\\quad\n",
    "b = \\alpha_1 c_1 + \\alpha_2 c_2 + \\alpha_3 \\tilde{n}. \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Sketch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;float:left;\">\n",
    "\n",
    "A consequence of the Fundamental Theorem Part II for a matrix $A \\in \\mathbb{R}^{M \\times N}$:\n",
    "    \n",
    "Given  $\\left\\{ c_1, c_2, \\dots c_r \\right\\},$ a basis for $\\mathscr{C}(A)$,<br>\n",
    "and $\\left\\{ ñ_1, ñ_2, \\dots ñ_{M-r} \\right\\},$ a basis for $\\mathscr{N}(A')$.<br>\n",
    "\n",
    "> Any vector $b \\in \\mathbb{R}^M$ can be written as a linear combination of these vectors:\n",
    "$$\n",
    "\\begin{align}\n",
    "&b           \\; = \\color{blue}{b_{\\parallel}} + \\color{red}{b_{\\perp}},  &\\\\\n",
    " \\text{ where }\\quad &\\color{blue}{b_{\\parallel}  = \\alpha_1 c_1 + \\alpha_2 c_2 \\dots + \\alpha_r c_r} &\\\\\n",
    " \\text{ and }\\quad &\\color{red}{b_\\perp        = \\beta_1 ñ_1 + \\beta_2 ñ_2 \\dots \\beta_{M-r} ñ_{M-r}}.&\n",
    "\\end{align}\n",
    "$$\n",
    "</div>\n",
    "<div style=\"float:right;\">\n",
    "The result is depicted in the following Figure:<br>\n",
    "    (note this is the codomain of $y=A x$ turned on its side)<br><br>\n",
    "<img src=\"./NormalEquations.svg\"  width=\"350\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:center;>\n",
    "* the red vector $b_\\parallel$ is the part of $b$<br>\n",
    "    that lies in the $\\mathscr{C}(A)$ hyperplane (the linear combination formed with $\\alpha_i c_i$)<br>\n",
    "  it is the orthogonal projection $Proj_{\\mathscr{C}(A)}^\\perp b$ onto the column space $\\mathscr{C}(A)$\n",
    "* the blue vector $b_\\perp$ is the part of $b$<br>\n",
    "    that lies in the $\\mathscr{N}(A')$ hyperplane (the linear combination formed with $\\beta_j ñ_j$)$\\quad\\quad$\n",
    "* these two vector components are orthogonal.\n",
    "            </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<div style=\"float:left;\">$\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad$</div>\n",
    "<div style=\"float:left;\">\n",
    "\n",
    "* the red vector $b_\\parallel$ is the part of $b$<br>\n",
    "that lies in the $\\mathscr{C}(A)$ hyperplane (the linear combination formed with $\\alpha_i c_i$)<br>\n",
    "it is the orthogonal projection $Proj_{\\mathscr{C}(A)}^\\perp b$ onto the column space $\\mathscr{C}(A)$\n",
    "* the blue vector $b_\\perp$ is the part of $b$<br>\n",
    "that lies in the $\\mathscr{N}(A')$ hyperplane (the linear combination formed with $\\beta_j ñ_j$)\n",
    "* these two vector components are orthogonal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Example Continued**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> **Decompose a Vector into Two Orthogonal Components ( <span style=\"color:red;\"> Naive Method</span> )** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> $\\quad\\quad\\;$ Let $A = \\begin{pmatrix}  1  & 2 \\\\ 2 & 2 \\\\ -1 & 1 \\end{pmatrix}$, the matrix from the previous example.\n",
    ">\n",
    "> **Step 1:** We had found the bases\n",
    ">\n",
    ">\n",
    "> $$\n",
    "\\text{basis } \\mathscr{C}(A) = \\left\\{\\;\n",
    "c_1= \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\; c_2 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} \\; \\right\\}, \\quad\\quad\n",
    "\\text{basis } \\mathscr{N}(A^t) = \\left\\{\\ \\tilde{n}\\ = \\ \\begin{pmatrix} -4 \\\\ 3 \\\\ 2 \\end{pmatrix} \\; \\right\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> **Step 2:** Split the vector $b = \\begin{pmatrix} -3 \\\\ 7 \\\\ -2 \\end{pmatrix}\\;$\n",
    "into two orthogonal components: $b = \\color{red}{b_{//}} + \\color{blue}{b_\\perp}\\;$,\n",
    "where $\\color{red}{b_{//}}$ is in $\\mathscr{C}(A)$ and $\\color{blue}{b_\\perp}$ is in $\\mathscr{N}(A^t)$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> $\\quad\\quad$ We need to solve $\\quad b = \\color{red}{ \\alpha_1 c_1 + \\alpha_2 c_2 } + \\color{blue}{ \\alpha_3 \\tilde{n}} \\; \\Leftrightarrow \\;\n",
    "\\begin{pmatrix} \\color{red}1 & \\color{red}2 & \\color{blue}{-4} \\\\ \\color{red}2 & \\color{red}2 & \\color{blue}3 \\\\  \\color{red}{-1} &  \\color{red}1 &  \\color{blue}2 \\end{pmatrix}\n",
    "\\begin{pmatrix} \\color{red}{\\alpha_1} \\\\ \\color{red}{\\alpha_2} \\\\ \\color{blue}{\\alpha_3} \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 7 \\\\ -2 \\end{pmatrix}.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> $\\quad\\quad$ This yields\n",
    "> $$\n",
    "\\begin{align}\n",
    "\\color{red}{b_{//}}   &= \\color{red}{3 a_1 - a2} &=& \\; \\color{red}{\\begin{pmatrix} 1 \\\\ 4 \\\\ -4 \\end{pmatrix}} \\\\\n",
    "\\color{blue}{b_\\perp} &= \\color{blue}{\\tilde{n}} &=& \\; \\color{blue}{\\begin{pmatrix} -4 \\\\ 3 \\\\ 2 \\end{pmatrix}} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> **Check:** Let's check orthogonality: $\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\color{red}{b_{//}} \\cdot \\color{blue}{b_\\perp} = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "> ---\n",
    "> This computation was involved: we needed to\n",
    "> * **Step 1:** find the bases of both $\\mathscr{C}(A)$ and $\\mathscr{N}(A^t)$\n",
    "> * **Step 2:** Solve the resulting $A x = b$ type problem\n",
    "> * **Step 3:** Split the resulting column view decomposition of $b = \\color{red}{b_\\parallel} + \\color{blue}{b_\\perp}.$\n",
    ">\n",
    "> **It turns out we can do better!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 2.3 Use the Fundamental Theorem to Decompose a Vector (Refinement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 2.3.1 Key Observation 1: Decomposing a Vector into Orthogonal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Key observation:** decomposition of a vector $b$\n",
    "<div style=\"float:left;\">\n",
    "> Any vector $b \\in \\mathbb{R}^M$ can be written as a linear combination:\n",
    "$$\n",
    "\\begin{align}\n",
    "                         &b                         \\;&=\\;& \\color{red}{b_{\\parallel}} + \\color{blue}{b_{\\perp}},\\\\\n",
    "\\text{ where }\\quad\\quad &\\color{red}{b_{\\parallel}}  &=\\;& \\color{red}{\\alpha_1 c_1 + \\alpha_2 c_2 \\dots + \\alpha_r c_r} &\\\\\n",
    "\\text{ and }  \\quad\\quad &\\color{blue}{b_\\perp}       &=\\;& \\color{blue}{\\beta_1 ñ_1 + \\beta_2 ñ_2 \\dots \\beta_{M-r} ñ_{M-r}},&\n",
    "\\end{align}\n",
    "$$\n",
    "the $c_i$ vectors form a basis for a hyperplane containing $b_\\parallel$,<br>\n",
    "the $ñ_j$ vectors form a basis for the orthogonal complement of this hyperplane.\n",
    "</div><div style=\"float:right;\">\n",
    "<img src=\"./NormalEquations.svg\"  width=\"350\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "----\n",
    "> The basic idea is to replace the system of equations for the coefficients $\\alpha_i, \\beta_j$ by taking dot products with each of the $c_j$ in the column space $\\mathscr{C}(A)$:\n",
    "> $$\n",
    "\\begin{align}\n",
    "(\\xi) & \\Leftrightarrow b &\\;=\\;& \\color{red}{\\alpha_1 c_1 + \\alpha_2 c_2 \\dots + \\alpha_r c_r} + \\color{blue}{b_\\perp} \\\\\n",
    "      & \\Rightarrow \\color{red}{c_j} \\cdot b &\\;=\\;& \\color{red}{\\alpha_1 c_j \\cdot c_1 + \\alpha_2  c_j \\cdot c_2 \\dots + \\alpha_r  c_j \\cdot c_r} + \\color{red}{c_j} \\cdot \\color{blue}{b_\\perp}, \\quad\\quad j=1,2, \\dots r\n",
    "\\end{align}\n",
    "$$\n",
    ">\n",
    "> Since $\\color{red}{c_j} \\perp \\color{blue}{b_\\perp}$, the $\\color{red}{c_j} \\cdot \\color{blue}{b_\\perp} = 0$: **we are left with a set of equations that only involve the unknown coefficients $\\color{red}{\\alpha_i}$!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> Let's return to $A = \\begin{pmatrix}  1  & 2 \\\\ 2 & 2 \\\\ -1 & 1 \\end{pmatrix}$, which has\n",
    "$\n",
    "\\text{basis } \\mathscr{C}(A) = \\left\\{\\;\n",
    "c_1= \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\; c_2 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} \\; \\right\\} \\quad\n",
    "$ and the vector $b = \\begin{pmatrix} -3 \\\\ 7 \\\\ -2 \\end{pmatrix}\\;$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> Set $ \\alpha_1  \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix} + \\alpha_2 \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\end{pmatrix} + b_\\perp = \\begin{pmatrix} -3 \\\\ 7 \\\\ -2 \\end{pmatrix}.$\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> Taking the dot product with $c_1$ and $c_2$ yields\n",
    "> $$ \\left.\n",
    "\\begin{align}\n",
    "c_1 : \\quad & 6 \\alpha_1 + 5 \\alpha_2 =& 13 \\\\\n",
    "c_2 : \\quad & 5 \\alpha_1 + 9 \\alpha_2 =& 6 \\\\\n",
    "\\end{align}\n",
    "\\right\\} \\Leftrightarrow \\left\\{ \\begin{aligned}\n",
    "\\alpha_1 =&\\; 3 \\\\ \\alpha_2 =& -1, \\end{aligned} \\right.\n",
    "$$\n",
    "the same solution we found before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 2.3.2 Key Observation 2: No Need to Identify the Column Space of $A$ and the Null Space of $A^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Key observation:** we do **not need to identify the fundamental spaces!**\n",
    "\n",
    "> Keeping all the columns of $A$ instead of just the pivot columns $c_j, j=1,2,\\dots r$<br>\n",
    "potentially yields an infinite number of ways of expressing the same solution $\\color{red}{b_\\parallel}:$<br>\n",
    "We would get the same solution as before by setting the additional free variables equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Key observation:** remember that $b = \\color{red}{b_\\parallel} + \\color{blue}{b_\\perp}$.\n",
    "\n",
    "> We are given vectors $\\color{blue}{a_1, a_2, \\dots a_n}$ and $b$.\n",
    ">\n",
    "> Once we compute $\\color{red}{b_\\parallel}$, we obtain $\\color{blue}{b_\\perp} = b - \\color{red}{b_\\parallel}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 2.3.3 The Final Touch: Rewrite the Equations in Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    " Finally, observe that we can rewrite our equations in matrix form:\n",
    " \n",
    " $$\n",
    " b \\;=\\; \\color{red}{\\alpha_1 a_1 + \\alpha_2 a_2 \\dots + \\alpha_N a_N} + \\color{blue}{b_\\perp}\n",
    " \\Leftrightarrow b =  \\color{red}{A x} + \\color{blue}{b_\\perp}.\n",
    " $$\n",
    " \n",
    " Taking the dot products with the columns $ \\color{red}{a_j = 1, 2, \\dots N}$ of $A$ yields the **normal equation**\n",
    " $$\n",
    " \\color{red}{A^t A x = A^t b}\n",
    " $$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "----\n",
    "All we need is any one solution, however: we get $ \\color{red}{b_\\parallel = A x}$ for some vector $x$:\n",
    " thus $\\quad b =  \\color{red}{A x} +  \\color{blue}{b_\\perp}$.\n",
    "  <br><br>\n",
    "  * Multiplying $b =  \\color{red}{A x} +  \\color{blue}{b_\\perp}$ by $A^t$ from the left still zeroes out the $ \\color{blue}{b_\\perp}$ term: we are left with the equations\n",
    "> $$\n",
    "   \\begin{align}\n",
    "   &A^t A x     &= A^t b &\\quad \\text{ known as the }\\textbf{normal equation} \\\\\n",
    "   &b_\\parallel &= A x   &\n",
    "   \\end{align}\\label{eq1}\\tag{1}\n",
    "$$\n",
    "> To solve for $b_\\perp$, it is sufficient to realize that $b = b_\\parallel + b_\\perp$, so\n",
    "> $$\n",
    "b_\\perp  = b - b_\\parallel \\label{eq2}\\tag{2}\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# 3. The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 3.1 Basic Properties of the Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 3.1.1 The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;\">\n",
    "    \n",
    "The **normal equation:** Given a set of vectors $\\{ \\; a_1, a_2 \\dots a_k \\; \\}$ in $\\mathbb{R}^n$,<br>\n",
    "$\\quad\\quad$ and let $A = ( a_1 \\ a_2 \\ \\dots a_k ).$\n",
    "    \n",
    "$$A^t A x = A^t b$$\n",
    "allows us to decompose vectors $b$ into two orthogonal components $$b = b_\\parallel + b_\\perp$$\n",
    "such that $b_\\parallel \\in S = span\\{  a_1, a_2 \\dots a_k \\}$ and\n",
    "$b_\\perp \\in S^\\perp$:\n",
    "$$\n",
    "   \\begin{align}\n",
    "   &A^t A x     &=&\\ A^t b  \\\\\n",
    "   &b_\\parallel &=&\\ A x    \\\\\n",
    "   &b_\\perp     &=&\\ b - b_\\parallel\n",
    "   \\end{align}.\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The key observation was that the multiplication of $A x = b = b_\\parallel + b_\\perp$ by $A^t$ zeros out the $b_\\perp$ term.\n",
    "\n",
    "<div style=\"background-color:#F2F5A9\">\n",
    "    \n",
    "**Theorem:** $\\quad\\quad\\quad\\quad\\quad\\mathscr{N}(A) = \\mathscr{N}(A^t A)$<br><br>\n",
    "\n",
    "</div>\n",
    "\n",
    "which **guarantees that the set of solutions of the normal equation is identical to the set of solution of $A x = b_\\parallel$**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;\">\n",
    "The proof is of interest:\n",
    "\n",
    "* Let $x$ be a solution of $A x = 0.$ Multiplying by $A^t$ from the left yields $A^t A x = 0,$<br>\n",
    "    and therefore $\\mathscr{N}(A) \\subseteq \\mathscr{N}(A^t A).$\n",
    "* Let $x$ be a solution of $A^t A x = 0.$<br>\n",
    "    $$\\begin{align}\n",
    "    A x = 0 \\Rightarrow & x^t A^t A x = 0\\\\\n",
    "            \\Rightarrow & (A x )^t (A x ) = 0 \\\\\n",
    "            \\Rightarrow & \\lVert A x \\rVert = 0 \\\\\n",
    "            \\Rightarrow & A x = 0,\n",
    "    \\end{align}\n",
    "    $$\n",
    "    and therefore $\\mathscr{N}(A^t A) \\subseteq \\mathscr{N}(A).$\n",
    "</div><div  style=\"float:right;\"><img src=\"VennDiagram_AtA.svg\" width=300>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 3.1.2 The Equivalent Minimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;\">\n",
    "The triangle inequality shows that the solutions to the normal equations also solve\n",
    "the problem\n",
    "<div style=\"background-color:#F2F5A9\">\n",
    "    \n",
    "$$\n",
    "x^* = \\arg\\min_x { \\lVert b - A x \\rVert }\n",
    "$$<br>\n",
    "</div>\n",
    "since $b_\\parallel = b - A x^*$ is the shortest vector from a point in $\\mathscr{C}(A)$ to $b$.<br><br>\n",
    "Note that the solution need not be unique: as before, we are interested in\n",
    "$$\n",
    "\\begin{align}\n",
    "   &b_\\parallel &=&\\ A x^*    \\\\\n",
    "   &b_\\perp     &=&\\ b - b_\\parallel,\n",
    "\\end{align}\n",
    "$$\n",
    "so homogeneous solutions do not enter.\n",
    "\n",
    "<div style=\"background-color:#F2F5A9\">\n",
    "\n",
    "**Definition:** The distance $d( b, \\mathscr{C}(A) ) = \\lVert b_\\perp \\rVert.$<br><br>\n",
    "    \n",
    "</div></div><div style=\"float:right;\"><img src=\"NormalEqMinDist.svg\" width=350></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### 3.1.3 Example: Projection Onto a Hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let $A = \\begin{pmatrix}\n",
    " 1 & 3 &4 \\\\ \n",
    " 5 & 3 &8 \\\\\n",
    " 1 &-1 &0 \\\\\n",
    " 2 & 2 &4\n",
    "\\end{pmatrix}, \\quad$ and\n",
    "$\\; b = \\begin{pmatrix} 15\\\\ 23\\\\ -2\\\\ 9 \\end{pmatrix}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Normal Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$$A^t A x = A^t b \\;\\Leftrightarrow\\; \\left( \\begin{array}{rrr|r}\n",
    " 31 & 21 & 52 & 146\\\\\n",
    " 21 & 23 & 44 & 134\\\\\n",
    " 52 & 44 & 96 & 280 \\end{array} \\right) \\quad \\Leftrightarrow x =\\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Split the $b$ Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$b = b_\\parallel + b_\\perp,$ where\n",
    "\n",
    "$$\n",
    "b_\\parallel = A x = \\begin{pmatrix} 14 \\\\ 22 \\\\ -2 \\\\ 12 \\end{pmatrix},\\quad \\text{ and } \\;\n",
    "b_\\perp     = b - b_\\parallel = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ -3 \\end{pmatrix}; \\quad\\quad \\textbf{Check orthogonality: }\n",
    "b_\\parallel \\cdot b_\\perp = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Distance from $b$ to the hyperplane $\\mathscr{C}(A)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$$\n",
    "d(\\ b,\\ \\mathscr{C}(A)\\ )\\ =\\ \\lVert b_\\perp \\rVert\\ =\\ \\sqrt{11}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 3.2 Special Case: Projection onto a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;width:40%\">\n",
    "Decompose a vector $b = b_\\parallel + b_\\perp$,  where\n",
    "\n",
    "* $\\quad\\quad b_\\parallel$ is a vector in the span$\\{ a \\}$, and $\\quad\\quad\\quad\\quad\\quad$\n",
    "* $\\quad\\quad b_\\perp$ is a vector orthogonal to $a$.\n",
    "\n",
    "**Remark:** Here $A = ( a ),$ $x = ( \\alpha ) $\n",
    "</div>\n",
    "<img src=\"NormalProjOntoLine.svg\" width=250 style=\"float:left;\">\n",
    "<div style=\"float:right;width:6cm;height:3.5cm;border:1px solid black;\">\n",
    "$\\quad$ Consider<br>\n",
    "$$\\quad a = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix}, \\quad b =  \\begin{pmatrix} 0 \\\\ 5 \\\\ 2 \\end{pmatrix}.$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Step 1: Solve the Normal Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;width:40%\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha\\ a\\ + \\ b_\\perp =\\ b \\;& \\Rightarrow\\;&\n",
    "\\alpha\\ a \\cdot a      = a \\cdot b \\; \\\\\n",
    "& \\Leftrightarrow\\;& \\alpha = \\frac{ a \\cdot b }{a \\cdot a}.\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</div><div style=\"float:left;background-color:#F2F5A9;\">\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A x + \\ b_\\perp =\\ b &\\; \\Rightarrow    \\;& A^t A x  = A^t A b \\\\\n",
    "                     &\\; \\Leftrightarrow\\;& x = \\frac{ a \\cdot b }{a \\cdot a}.\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</div>\n",
    "<div style=\"float:right;width:6cm;height:2cm;border:1px solid black;\">\n",
    "<br>\n",
    "$$ \\alpha =  \\frac{4}{5}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Step 2: Decompose the Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;width:40%;\">\n",
    "$$\n",
    "\\begin{align}\n",
    "& b_\\parallel        = \\ \\alpha\\ a \\quad  =  \\frac{ a \\cdot b }{a \\cdot a}\\ a\\\\\n",
    "& b_\\perp            = \\ b - b_\\parallel  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</div><div style=\"float:left;background-color:#F2F5A9;height:2cm;\">\n",
    "\\begin{align}\n",
    "\\quad b_\\parallel        =&\\ A x  =&  \\frac{ a \\cdot b }{a \\cdot a}\\ a &\\quad\\quad\\quad\\quad\\quad \\\\\n",
    "\\quad b_\\perp            =&\\ b - b_\\parallel &\n",
    "\\end{align}   \n",
    "</div><div style=\"float:right;width:6cm;height:2cm;border:1px solid black;\">\n",
    "$$\n",
    "b_\\parallel = \\frac{1}{5}\\begin{pmatrix} 12\\\\16\\\\0 \\end{pmatrix},\\; b_\\perp = \\frac{1}{5}\\begin{pmatrix} -12 \\\\ 9\\\\ 2 \\end{pmatrix}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 3.3 Special Case: the Columns of $A$ are Mutually Orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The equations simplify considerably when the columns $\\{ a_1, a_2, \\dots a_N \\}$ of $A$<br> are **mutually orthogonal vectors**, i.e, when\n",
    "\n",
    "$$\n",
    "a_i \\cdot a_j = \\left\\{ \\begin{align} \\; \\lVert a_i \\rVert^2 \\ne 0 \\quad  &\\ \\text{ when } i = j\\\\ 0 \\quad &\\ \\text{ otherwise} \\end{align} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The normal equation takes the form\n",
    "$$\n",
    "A^t A x = A^t b \\Leftrightarrow D x = A^t b \\Leftrightarrow x = (A^t A)^{-1} A^t b,\n",
    "$$\n",
    "where  $D$ is a diagonal matrix\n",
    "$$\n",
    "D = A^t A = \\begin{pmatrix} \\lVert a_1 \\rVert^2 & 0                   & \\dots & 0 \\\\\n",
    "                    0                   & \\lVert a_1 \\rVert^2 & \\dots & 0 \\\\\n",
    "                    \\                   &     \\               &  \\    & 0 \\\\\n",
    "                    0                   & 0                   & \\dots & \\lVert a_N \\rVert^2 \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "Assuming $a_i \\ne 0, i =1,2, \\dots N$, we obtain the following solution:\n",
    "<div style=\"background-color:#F2F5A9\">\n",
    "\n",
    "For mutually orthogonal non-zero vectors $a_i, i=1,\\dots N$, the normal equations reduce to\n",
    "$$\n",
    "\\begin{align}\n",
    "x_i          =&\\ \\frac{ b \\cdot a_i }{ a_i \\cdot a_i } \\\\\n",
    "b_\\parallel  =&\\ \\sum_{i=1}^{N}{ \\frac{ b \\cdot a_i }{ a_i \\cdot a_i} a_i } \\\\\n",
    "b_\\perp      =&\\ b - \\sum_{i=1}^{N}{ \\frac{ b \\cdot a_i }{ a_i \\cdot a_i} a_i } \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Remark**:\n",
    "* the **equations simplify even further when the $a_i$ are mutually orthonormal**, i.e., when $a_i \\cdot a_i = 1.$\n",
    "* Orthonormal Coordinate vectors are NICE TO HAVE!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "#### **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let $A = \\begin{pmatrix}\n",
    "1 & 1 &  1 \\\\\n",
    "1 & 1 & -1 \\\\\n",
    "1 &-1 &  0 \\\\\n",
    "1 &-1 &  0 \\\\\n",
    "\\end{pmatrix}, \\quad b = \\begin{pmatrix} 4\\\\-4\\\\8\\\\12 \\end{pmatrix}\\;\\Rightarrow\\;\\;\n",
    "A^t A = \\begin{pmatrix} \\color{red}4&0&0\\\\ 0&\\color{red}4&0\\\\ 0&0&\\color{red}2\\end{pmatrix}\n",
    ",$ so the columns of $A$ are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Step 1: Solve the Normal Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$A^t A x = A^t b\\quad $ yields\n",
    "$\\quad\n",
    "\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} =\n",
    "\\begin{pmatrix} \\color{red}4&0&0\\\\ 0&\\color{red}4&0\\\\ 0&0&\\color{red}2\\end{pmatrix}^{-1} \\;\n",
    "\\begin{pmatrix} 20 \\\\ -20 \\\\ 8 \\end{pmatrix}\\;\n",
    "= \\begin{pmatrix} 5 \\\\ -5 \\\\ 4 \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Step 2: Decompose the Vector**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$$\n",
    "b_\\parallel = A x = \\begin{pmatrix} 4 \\\\ -4 \\\\ 10 \\\\ 10 \\end{pmatrix},\\quad\n",
    "b_\\perp  = b - b_\\parallel = \\begin{pmatrix} 0 \\\\ 0 \\\\ -2 \\\\ 2 \\end{pmatrix},\\quad\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# 4. Take Away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 4.1 The Fundamental Theorem of Linear Algebra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;float:left;height:5cm; width:12cm\">\n",
    "\n",
    "**Fundamental Theorem (Part II)**\n",
    "* $\\mathscr{R}(A)^\\perp = \\mathscr{N}(A)$<br>\n",
    "* $\\mathscr{C}(A)^\\perp = \\mathscr{N}(A^t)$\n",
    "    \n",
    "**Nota Bene:** Part II requires a **positive definite** inner product.\n",
    "</div>\n",
    "<img src=\"FundamentalTheorem.svg\" width=300 style=\"float:center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 4.2 The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div>\n",
    "<style type=\"text/css\">\n",
    ".tftable {font-size:12px;color:#333333;width:100%;border-width: 1px;border-color: #729ea5;border-collapse: collapse;}\n",
    ".tftable th {font-size:12px;background-color:#acc8cc;border-width: 1px;padding: 8px;border-style: solid;border-color: #729ea5;text-align:left;}\n",
    ".tftable tr {background-color:#ffffff;}\n",
    ".tftable td {font-size:12px;border-width: 1px;padding: 8px;border-style: solid;border-color: #729ea5;}\n",
    "</style>\n",
    "\n",
    "<table class=\"tftable\" border=\"1\">\n",
    "<tr><th style=\"width:6cm;\">Equation</th><th style=\"width:6cm;\">Orthogonal Projection</th><th>Comment</th></tr>\n",
    "<tr><td>$x = \\arg\\min_x { \\lVert b - A x \\rVert }$</td><td>$b_\\parallel=A x$</td><td>Minimize Distance to $\\mathscr{C}(A)$</td></tr>\n",
    "<tr><td>$A^t A x = A^t b$</td><td>$b_\\parallel=A x$</td><td>Remove $\\mathscr{N}(A^t)$ component from $b$</td></tr>\n",
    "<tr><td>$x = \\frac{a \\cdot b}{a \\cdot a}$</td><td>$b_\\parallel=\\frac{a \\cdot b}{a \\cdot a}\\;a$</td><td>Column Vector Case: $A = a$</td></tr>\n",
    "<tr><td>$x_i = \\frac{a_i \\cdot b}{a_i \\cdot a_i}$</td><td>$b_\\parallel=\\sum_i\\frac{a_i \\cdot b}{a_i \\cdot a_i}\\;a_i$</td><td>Orthogonal Vectors $a_i$ Case: $A = (a_1 \\ a_2 \\ \\dots)$</td></tr>\n",
    "<tr><td>$x_i = q_i \\cdot b$</td><td>$b_\\parallel=\\sum_i{q_i \\cdot b\\;q_i}$</td><td>Orthonormal Vectors $q_i$ Case: $A = (q_1\\ q_2 \\dots )$</td></tr>\n",
    "</table>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
