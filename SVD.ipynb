{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><p style=\"color:darkred;font-size:30pt;height:15px;text-align:left;\">The Singular Value Decomposition</p></div>"
      ],
      "text/plain": [
       "HTML{String}(\"<div><p style=\\\"color:darkred;font-size:30pt;height:15px;text-align:left;\\\">The Singular Value Decomposition</p></div>\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra, RowEchelon, LaTeXStrings, Plots, SymPy\n",
    "include(\"LAcodes.jl\");  ##] dev --local \".\"\n",
    "LAcodes.title( \"The Singular Value Decomposition\", sz=30, color=\"darkred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 1.1 Generalize the Idea of an Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Eigendecomposition of a matrix $A$ has shortcomings: \n",
    "* A square matrix $A$ may or may not have a **complete eigenvector basis**\n",
    "* There is no eigendecomposition for **matrices that are not square.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> The case $A$ of size $M \\times N$ with $M \\ne N$ shows a restiction<br> we had imposed to find the eigendecomposition:<br>\n",
    "$\\quad\\quad$ **we used the same basis vectors** $s_1, s_2, \\dots s_n$ in both the domain and the codomain of $y = A x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;width:15cm;border:1px solid black;\">\n",
    "\n",
    "$\\;$**Idea:** use different bases for $y = A x.$\n",
    "$$\\left. \\begin{align}\n",
    "x &= V \\tilde{x} \\\\\n",
    "    y &= U \\tilde{y}\n",
    "\\end{align}\\right\\}\\quad \\Rightarrow \\quad U \\tilde{y} = A V \\tilde{x}\n",
    "\\quad \\Rightarrow \\quad \\tilde{y} = U^{-1} A V \\tilde{x} = \\Sigma \\tilde{x},\n",
    "$$\n",
    "\n",
    "$\\quad\\quad$ where we have set $\\Sigma = U^{-1} A V \\Leftrightarrow A = U \\Sigma V^{-1}.$\n",
    "</div>\n",
    "<div style=\"float:right;border:1px solid black;width:9cm;height:3.1cm;\">\n",
    "$\\;$ Better yet, let us try for orthonormal bases:\n",
    "\n",
    "$$\n",
    "U^{-1} = U^t, \\; \\text{ and }\\; V^{-1} = V^t.\n",
    "$$\n",
    "\n",
    "$\\;$**Remark: the matrix sizes are**<br>\n",
    "$\\quad\\quad A_{M\\times N}, \\Sigma_{M\\times N}, V_{N \\times N}, U_{M \\times M}$. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> What should $\\Sigma$ look like? We would like a diagonal matrix, but $\\Sigma$ is not square in general.\n",
    "Let us try for\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix},\n",
    "$$\n",
    "where $\\Sigma_r$ is a square diagonal matrix of size $r \\times r$ with $r$ non-zero entries on the diagonal,<br>\n",
    "and zero entries to fill out the remaining entries in a matrix of size $M \\times N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> **Examples:**\n",
    ">\n",
    "> $\\quad\\quad\n",
    "\\left( \\begin{array}{cc|c} \\color{red}5 & \\color{red}0 & 0 \\\\ \\color{red}0 & \\color{red}1 & 0 \\\\ \\hline 0 & 0 & 0 \\end{array}\\right),\\quad\n",
    "\\left( \\begin{array}{cc|c} \\color{red}5 & \\color{red}0 & 0 \\\\ \\color{red}0 & \\color{red}1 & 0\\end{array}\\right), \\quad\n",
    "\\left( \\begin{array}{cc} \\color{red}5 & \\color{red}0 \\\\ \\color{red}0 & \\color{red}1 \\\\ \\hline 0 & 0 \\end{array}\\right),\\quad\n",
    "$\n",
    "where the $\\color{red}{\\Sigma_r}$ entries are shown in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 1.2 Is this Feasible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;with:15cm;\">\n",
    "\n",
    "Consider $A = U \\Sigma V^t.$<br>\n",
    "We can manipulate this equation in various ways:\n",
    "\n",
    "$$\\begin{align}\n",
    "A = U \\Sigma V^t & \\quad \\Leftrightarrow \\quad &  A V & = \\Sigma U \\label{eqn1}\\tag{1} \\\\\n",
    "A = U \\Sigma V^t & \\quad \\Rightarrow \\quad  & A^t A & = V \\Sigma^t \\Sigma V^t \\label{eqn2}\\tag{2} \\\\\n",
    "A = U \\Sigma V^t & \\quad \\Rightarrow \\quad  & A A^t & = U \\Sigma \\Sigma^t U^t \\label{eqn3}\\tag{3} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Remarks:**\n",
    "* $\\Sigma^t \\Sigma = \\begin{pmatrix} \\Sigma_r^2 & 0 \\\\ 0 & 0 \\end{pmatrix}$ is a diagonal matrix of size $N \\times N.$\n",
    "* $\\Sigma \\Sigma^t = \\begin{pmatrix} \\Sigma_r^2 & 0 \\\\ 0 & 0 \\end{pmatrix}$ is a diagonal matrix of size $M \\times M.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* Eqn 2:  $\\;A^tA = V ( \\Sigma^t \\Sigma) V^t \\quad$ is an orthogonal eigendecomposition  of the symmetric matrix $A^t A.$<br>\n",
    "$\\quad\\quad$  An orthogonal matrix $V$ and a diagonal matrix $\\Sigma^t \\Sigma$ do exist\n",
    "* Eqn 3:  $\\;AA^t = U ( \\Sigma \\Sigma^t) U^t \\quad$ is an orthogonal eigendecomposition  of the symmetric matrix $A A^t.$<br>\n",
    "$\\quad\\quad$  An orthogonal matrix $U$ and a diagonal matrix $\\Sigma \\Sigma^t$ do exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> What is not clear is whether the diagonal matrices $\\Sigma^t \\Sigma$ and $\\Sigma \\Sigma^t$  are related:\n",
    "> * **do they share the same non-zero entries $\\Sigma_r^2?$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "* Eqn 1: $\\; A U = V \\Sigma \\quad$ further posits a relationship between $U$ and $V.$\n",
    "> **Is this relationship satisfied?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# 2. The Gram Matrix $A^t A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Some reminders for matrices $A$ of size $M \\times N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Rank and the Dimension of the Null Spaces**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;width:12cm;height:4cm;border:1px solid black;\">\n",
    "\n",
    "$\\;\\;$ We have previously seen that $\\mathscr{N}(A^t A)\\ =\\ \\mathscr{N}(A).$<br>\n",
    "$\\;\\;$ Therefore\n",
    "* $dim\\ \\mathscr{N}(A^t A)\\ =\\ dim\\ \\mathscr{N}(A) = N -rank(A)$\n",
    "* $rank\\ (A) = rank\\ (A^t A)$<br>since both matrices have the same number of columns $N$.\n",
    "</div>\n",
    "<div style=\"float:right;width:12cm;height:4cm;border:1px solid black;\">\n",
    "\n",
    "$\\;\\;$ Similarly, $\\mathscr{N}(A A^t)\\ =\\ \\mathscr{N}(A^t).$<br>\n",
    "$\\;\\;$ Therefore\n",
    "* $dim\\ \\mathscr{N}(A A^t)\\ =\\ dim\\mathscr{N}(A^t)\\ =\\ M - rank\\ (A^t)$\n",
    "* $rank\\ (A^t)\\ =\\ rank\\ (A A^t)$<br>since both matrices have the same number of columns $M$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Dimension of the Eigenspaces for $\\lambda = 0$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;width:12cm;height:4.5cm;border:1px solid black;\">\n",
    "$\\;\\;$ We also know that $rank(A) = rank(A^t)\\;$<br>\n",
    "$\\;\\;$ and that non-zero nullspace vectors are eigenvectors for $\\lambda =0$.\n",
    "\n",
    "* $A, A^t, A^t A$ and $A A^t$ all have the same rank.\n",
    "* Eigenspace $E_0$ of $A^t A$ has $dim\\ \\mathscr{N}(A^t A) = N - rank(A)$\n",
    "* Eigenspace $E_0$ of $A A^t$ has $dim\\ \\mathscr{N}(A^t A) = M - rank(A)$\n",
    "* $A^t A$ and $A A^t$ have the same number<br>$rank(A)$ non-zero eigenvalues\n",
    "</div>\n",
    "<img src=\"SVD_ranks.svg\" width=400 style=\"float:right;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Relationship of Eigenpairs for $A^t A$ and $A A^t$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;width:13cm;height:4.5cm;border:1px solid black;\">\n",
    "\n",
    "$\\;\\;$ Let $(\\lambda, x)$ be an eigenpair of $A^t A$. Observe\n",
    "$$\n",
    "(A^t A)\\ x = \\lambda x \\Rightarrow (A A^t) (A x) = \\lambda (A x)\n",
    "$$\n",
    "\n",
    "$\\;\\;\\therefore$ If $A x \\ne 0,$ it is an eigenvector of $A A^t.$ Is it?<br><br>\n",
    "\n",
    "$\\;\\;$ **$(\\lambda \\ne 0, x)$ is an eigenpair of $A^t A \\Rightarrow (\\lambda, A x)$ is an eigenpair of $A A^t.$**<br>\n",
    "$\\;\\;$ **$(\\lambda \\ne 0, x)$ is an eigenpair of $A A^t \\Rightarrow (\\lambda, A^t x)$ is an eigenpair of $A^t A.$**\n",
    "</div>\n",
    "<div style=\"float:right;width:11cm;height:4.5cm;border:1px solid black;\">\n",
    "\n",
    "$\\begin{align}\n",
    "\\left( \\lVert A x \\rVert^2 \\right)&  = \\left( (A x) \\cdot (A x) \\right)\\\\\n",
    "& = (A x)^t (A x) \\\\\n",
    "& = x^t A^t A x \\\\\n",
    "& = \\lambda x^t x \\\\\n",
    "& = \\lambda \\left( \\lVert x \\rVert^2 \\right) \\ne 0.\n",
    "\\end{align}$\n",
    "\n",
    "$\\;\\;$ Since $x$ is an eigenvector $\\lVert x \\rVert \\ne 0 . \\quad$\n",
    "$\\therefore \\color{red}{A x \\ne 0 \\;\\text{ iff } \\lambda \\ne 0}.$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **So $A^t A$ and $A A^t$ Have the Same Non-zero Eigenvalues and Corresponding Eigenvectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "* For the eigenspaces for eigenvalue $\\lambda = 0$, we have\n",
    "    * for $A^t A$, we have $dim\\ E_0 = N - rank(A)$\n",
    "    * for $A A^t$, we have $dim\\ E_0 = M - rank(A)$\n",
    "* Both matrices have the **same dimension for the eigenspaces $dim\\ E_\\lambda$ for $\\lambda \\ne 0$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "* **Both $A^t A$ and $A A^t$ share the diagonal matrix $\\Sigma_r^2.$**\n",
    "* **The size of this matrix $r = rank(A).$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "* The eigenvectors of $A^t A$ for the non-zero eigenvalues are **the first $r$ vectors in $V:$**<br>\n",
    "* The eigenvectors of $A A^t$ for the non-zero eigenvalues are **the first $r$ vectors in $U$.**\n",
    "\n",
    "* Since the remaining vectors are bases for the null spaces of $A^t A$ and $A A^t$ respectively,<br>\n",
    "these first $r$ vectors are **bases for the respective row spaces.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **What About Orthogonality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let $(\\lambda_1, x_1)$ and $(\\lambda_2, x_2)$ be eigenpairs of $A^t A$.<br>\n",
    "$\\quad\\quad$ If $x_1 \\perp x_2$, we find\n",
    "$\\quad\n",
    "(A x_1) \\cdot (A x_2) = \\lambda_1 \\lambda_2\\; x_1 \\cdot x_2 = 0.\n",
    "$\n",
    "\n",
    "$\\quad\\quad$ Since $A x_1$,$A x_2$ are eigenvectors of $A A^t$ provided $\\lambda_1 \\lambda_2 \\ne 0,$ we find\n",
    "\n",
    "* **Given eigenpairs $(\\lambda_1 \\ne 0, x_1)$ and $(\\lambda_2 \\ne 0, x_2)$ of $A^t A$,\n",
    "  then $(\\lambda_1, A x_1, \\lambda_2 A x_2)$ are eigenpairs of $A A^t.$<br>\n",
    "  $\\quad\\quad$ Further, $x_1 \\perp x_2 \\Rightarrow A x_1 \\perp A x_2.$**\n",
    "* **Given eigenpairs $(\\lambda_1 \\ne 0, x_1)$ and $(\\lambda_2 \\ne 0, x_2)$ of $A A^t$,\n",
    "  then $(\\lambda_1, A^t x_1, \\lambda_2 A^t x_2)$ are eigenpairs of $A A^t.$<br>\n",
    "  $\\quad\\quad$ Further, $x_1 \\perp x_2 \\Rightarrow A^t x_1 \\perp A^t x_2.$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **The Reduced Decomposition (Compact Decomposition)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's look at the decomposition again, where we partition the $U$ and $V$ matrices to separate out the first $r$ vectors:\n",
    "$\\quad\\quad U = \\begin{pmatrix} U_r & \\tilde{U}_r \\end{pmatrix}, \\quad V = \\begin{pmatrix} V_r & \\tilde{V}_r \\end{pmatrix}.$<br>\n",
    "\n",
    "$\n",
    "\\quad\\quad A = \\begin{pmatrix} U_r & \\tilde{U}_r \\end{pmatrix}\n",
    "    \\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix}\n",
    "    \\begin{pmatrix} V_r & \\tilde{V}_r \\end{pmatrix}^t\n",
    "  = \\color{red}{U_r\\ \\Sigma_r\\ V_r^t}.\n",
    "$\n",
    "\n",
    "**The null space basis vectors have no effect on this decomposition!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> As long as we use a basis for the null space of $A^t A$ for $\\tilde{V}_r$<br>\n",
    "and a basis for the null space of $A A^t$ for $\\tilde{U}_r$,<br>\n",
    "we have established that **a decomposition $A V = U \\Sigma$ exists!**\n",
    "\n",
    "But what about orthogonality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Non-negative Eigenvalues, Orthogonal Eigenvectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We need $\\Sigma_r$, not $\\Sigma_r^2$. Let's look again at the eigenvalues of $A^t A.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Given an eigenpair $(\\lambda, x)$ of $A^t A:$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A^t A x = \\lambda x\n",
    "& \\Rightarrow x^t A^t A x = \\lambda x^t x \\\\\n",
    "& \\Rightarrow \\lVert A x \\rVert^2 = \\lambda \\lVert x \\rVert^2\n",
    "& \\Rightarrow \\lambda \\ge 0\\quad\\text{ since } x \\ne 0.\n",
    "\\end{align}$$\n",
    "\n",
    "$\\quad\\quad$ **The matrices $A^t A$ and $A A^t$ are positive semidefinite.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> $\\quad\\quad \\Sigma_r =\\begin{pmatrix} \\sigma_1  & 0         & \\dots & 0 \\\\\n",
    "                           0         & \\sigma_2  & \\dots & 0 \\\\\n",
    "                           \\dots     & \\ldots    & \\dots & \\dots \\\\\n",
    "    0         & 0         & \\dots & \\sigma_r \\end{pmatrix},$\n",
    ">\n",
    "> $\\quad\\quad$ where $\\sigma_i\\ =\\ \\sqrt{ \\lambda_i },\\ i=1,2, \\dots r\\quad$ <span>are the square roots<br>$\\quad\\quad$  of the nonzero (and hence positive) eigenvalues of $A^t A.$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "> $\\quad\\quad$ A small refinement: since the $\\sigma_i$ are positive reals,<br>\n",
    "$\\quad\\quad$ we will **order them by decreasing magnitude:**$\\quad\\quad\n",
    "\\sigma_1 \\ge \\sigma_2 \\dots \\ge \\sigma_r.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Obtain $U_r$ from $V_r$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The SVD decomposition requires unit column vectors in $U$ and $V$.\n",
    "\n",
    "Consider an eigenpair of $(\\lambda = \\sigma^2 \\ne 0, v)$ for $A^t A$ <br>\n",
    "with the eigenvector scaled to be a unit vector:\n",
    "$\\quad \\lVert x \\rVert = 1$.\n",
    "\n",
    "Observe $\\quad$\n",
    "$x^t A^t A x = \\sigma^2 x^t x \\Rightarrow \\lVert A x \\rVert^2 = \\sigma^2$.\n",
    "\n",
    "The corresponding unit eigenvector for $A A^t$ is therefore given by $$u = \\frac{1}{\\sigma} A x.$$\n",
    "Combining all the eigenvectors into a matrix as columns, we see that\n",
    "$$\n",
    "\\color{red}{U_r = A V_r \\Sigma_R^{-1} \\Leftrightarrow A V_r = U_r \\Sigma_r}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# 3. The Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 3.1 SVD Existence Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;\">\n",
    "\n",
    "**Definition:** Given a matrix $A \\in \\mathbb{R}^{M \\times N}$.<br>\n",
    "    $\\quad\\quad$ $A = U \\Sigma V^t$ is a **singular value decomposition** of $A$ iff<br>\n",
    "    $\\quad\\quad$ $U$ and $V$ are orthogonal matrices, and $\\Sigma = \\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix}$,<br>\n",
    "    $\\quad\\quad$ where $\\Sigma_r$ is a diagonal matrix of size $r\\times r$, with diagonal entries $\\sigma_1 \\ge \\sigma_2 \\dots \\ge \\sigma_r >0.$\n",
    "    \n",
    "$\\quad\\quad$ The $\\sigma_i$ are **singular values** of $A.$<br>\n",
    "$\\quad\\quad$ The columns of $V$ are **right singular vectors** of $A.$<br>\n",
    "$\\quad\\quad$ The columns of $U$ are **left singular vectors** of $A.$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"background-color:#F2F5A9;\">\n",
    "\n",
    "**Theorem:** Every matrix $A \\in \\mathbb{R}^{M \\times N}$ has a **singular value decomposition** such that<br>\n",
    "    $\\quad\\quad$ $A = U \\Sigma V^t  = U_r \\Sigma_r V_r^t$\n",
    "<div style=\"background-color:#F2F5A9;margin:30px;\">\n",
    "\n",
    "* $\\Sigma = \\begin{pmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{pmatrix}$,\n",
    "    where $\\Sigma_r$ is a diagonal matrix of size $r\\times r$, with non-negative diagonal entries $\\sigma_1 \\ge \\sigma_2 \\dots \\ge \\sigma_r >0.$\n",
    "* $r = rank(A)$\n",
    "* $V = \\begin{pmatrix} V_r & \\tilde{V}_r \\end{pmatrix}$\n",
    "    * the $r$ columns of $V_r$ form an orthonormal basis for $\\mathscr{C}(A)$\n",
    "    * the $N-r$ columns of $\\tilde{V}_r$ form an orthonormal basis for $\\mathscr{N}(A)$\n",
    "* $U = \\begin{pmatrix} U_r \\tilde{U}_r \\end{pmatrix}$\n",
    "    * the $r$ columns of $V_r$ form an orthonormal basis for $\\mathscr{R}(A)$\n",
    "    * the $M-r$ columns of $\\tilde{V}_r$ form an orthonormal basis for $\\mathscr{N}(A^t)$\n",
    "<br><br>\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## 3.2 SVD Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The derivation shows one way of computing the SVD:\n",
    "* Start with either $A^t A$ (size $N \\times N$, or $A A^t$ (size $M \\times M$).<br>\n",
    "  We typically choose the smaller matrix.<br>\n",
    "\n",
    "<div style=\"margin:30px;border:1px solid black;\">\n",
    "\n",
    "* **Compute the orthogonal eigendecomposition of $A^t A.$**<br>\n",
    "  $\\;\\;$ this results in $\\Sigma_r$, $V_r$<br>\n",
    "  $\\;\\;$ and (optionally) an orthogonal basis for $\\mathscr{N}(A)$.\n",
    "  * If the full SVD is required, <br>\n",
    "    we need to compute an orthogonal basis for $\\mathscr{N}(A),$<br>\n",
    "    and obtain $\\Sigma$ by augmenting $\\Sigma_r$ with zeros to the same size as $A.$\n",
    "* **Compute $U_r = A V_r \\Sigma_r^{-1} \\Leftrightarrow u_i = \\frac{1}{\\sigma_i} A v_i$**\n",
    "* If the full SVD is required, **compute an orthogonal basis** $\\tilde{v}_i, i=1,2, \\dots M-r$<br>\n",
    "    $\\;\\;$ for $\\mathscr{N}(A^t) = \\mathscr{N}(A A^t) = span\\{u_1, u_2, \\dots u_r \\}^\\perp.$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let $A = \\begin{pmatrix}  -3 & -1 & -1 \\\\\n",
    "-3 & -1 & -1 \\\\\n",
    "1 & 3 & -1 \\\\\n",
    "-1 & -3 & 1\n",
    " \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Step 1: Orthonormal Eigendecomposition of $A^t A$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Eigenvalues:**\n",
    "\n",
    "> $A^t A = \\begin{pmatrix} 20 & 12 & 4 \\\\\n",
    "12 & 20 & -4 \\\\\n",
    "4 & -4 & 4\n",
    "\\end{pmatrix}$ has characteristic polynomial\n",
    "$p(\\lambda) = - \\lambda ( \\lambda^2 + 44 \\lambda -384 )$\n",
    ">\n",
    "> $\\therefore \\lambda  = 32, 12,0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Eigenvector Basis**\n",
    "\n",
    "> Bases for the null spaces $\\mathscr{N}(A- \\lambda I)$ are shown in the table below.<br>\n",
    "**Caveat:** the eigenvalues must be entered in decreasing order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<div style=\"float:left;margin:30px;width:40%;\">\n",
    "<table border=\"1\" cellpadding=\"0\" cellspacing=\"0\" style=\"border-collapse: collapse\" width=\"300px\">\n",
    "<tr>\n",
    "    <td height=\"19\" width=\"100px\">$\\color{blue}{\\sigma=\\sqrt{\\lambda}}$</td>\n",
    "    <td height=\"19\" width=\"100px\">$4 \\sqrt{2}$</td>\n",
    "    <td height=\"19\" width=\"100px\">$2 \\sqrt{3}$</td>\n",
    "    <td height=\"19\" width=\"100px\">$\\quad\\quad 0$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td height=\"19\" width=\"100px\">$\\color{blue}\\lambda$</td>\n",
    "    <td height=\"19\" width=\"100px\">$32$</td>\n",
    "    <td height=\"19\" width=\"100px\">$12$</td>\n",
    "    <td height=\"19\" width=\"100px\">$\\quad\\quad 0$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td height=\"16\" width=\"100px\" ><span  style=\"color:blue;\">(m)</span></td>\n",
    "    <td height=\"16\" width=\"100px\"><span  style=\"justify:right;\">$\\quad$ (1)</span></td>\n",
    "    <td height=\"16\" width=\"100px\">$\\quad$ (1)</td>\n",
    "    <td height=\"16\" width=\"100px\">$\\quad\\;$ (1)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td height=\"19\" width=\"100px\"><span  style=\"color:blue;\">Basis for $E_\\lambda$</span></td>\n",
    "    <td height=\"19\" width=\"100px\">$\\;\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\\;$</td>\n",
    "    <td height=\"19\" width=\"100px\">$\\;\\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\\;$</td>\n",
    "    <td height=\"19\" width=\"100px\">$\\quad\\;\\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix}\\;$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td height=\"19\" width=\"100px\"><span  style=\"color:blue;\">Orthonormal Basis for $E_\\lambda$</span></td>\n",
    "    <td height=\"19\" width=\"100px\">$\\;\\begin{pmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\\\ 0 \\end{pmatrix}\\;$</td>\n",
    "    <td height=\"19\" width=\"100px\">$\\;\\begin{pmatrix} -\\frac{\\sqrt{3}}{3} \\\\ \\frac{\\sqrt{3}}{3} \\\\ -\\frac{\\sqrt{3}}{3} \\end{pmatrix}\\;$</td>\n",
    "    <td height=\"19\" width=\"100px\">$\\quad\\;\\begin{pmatrix} -\\frac{\\sqrt{6}}{6} \\\\ \\frac{\\sqrt{6}}{3} \\\\ 0 \\end{pmatrix}\\;$</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div><div style=\"float:right;margin:30px;width:40%;\">\n",
    "Therefore $\\quad \\color{red}{rank(A) = 2}$\n",
    "\n",
    "$V = \\frac{1}{6} \\left( \\begin{array}{cc|c} 3 \\sqrt{2} &  2 \\sqrt{3} &\\sqrt{6}\\\\\n",
    "                                            3 \\sqrt{2} & -2 \\sqrt{3} & -\\sqrt{6} \\\\\n",
    "                                            0          & 2 \\sqrt{3} & -2 \\sqrt{6} \\end{array} \\right)$\n",
    "\n",
    "$\\Sigma = \\left(  \\begin{array}{cc|c} \\color{red}{4 \\sqrt{2}} & 0 & 0\\\\\n",
    "                                      0 & \\color{red}{2 \\sqrt{3}} & 0\\\\ \\hline\n",
    "                                      0 & 0 & 0 \\\\\n",
    "                                      0 & 0 & 0\n",
    "\\end{array}\\right)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "##### **Step 2: $U_r$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$U_r = A V_r \\Sigma_r^{-1} = \\frac{1}{2}\\begin{pmatrix} -1 & -1 \\\\\n",
    "-1 & -1 \\\\\n",
    "1 & -1 \\\\\n",
    "-1 & 1\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### **Verify the Compact SVD**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "$$A = U_r \\Sigma_r V_r^t = \\frac{1}{2}\\begin{pmatrix} -1 & -1 \\\\\n",
    "-1 & -1 \\\\\n",
    "1 & -1 \\\\\n",
    "-1 & 1\n",
    "    \\end{pmatrix}\\; \\begin{pmatrix}\n",
    "4 \\, \\sqrt{2} & 0 \\\\\n",
    "0 & 2 \\, \\sqrt{3}\n",
    "    \\end{pmatrix}\\; \\frac{1}{6} \\begin{pmatrix}\n",
    "3 \\, \\sqrt{2} & 2 \\, \\sqrt{3} \\\\\n",
    "3 \\, \\sqrt{2} & -2 \\, \\sqrt{3} \\\\\n",
    "0 & 2 \\, \\sqrt{3}\n",
    "\\end{pmatrix} \\quad = \\begin{pmatrix}\n",
    "-3 & -1 & -1 \\\\\n",
    "-3 & -1 & -1 \\\\\n",
    "1 & 3 & -1 \\\\\n",
    "-1 & -3 & 1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "##### **Step 3: Obtain $\\tilde{U}_r$ and Complete $U$ for the Full SVD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A basis for $\\mathscr{N}(A^t)$ is given by $\\left\\{\\; \n",
    " \\begin{pmatrix}-1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\\\n",
    " \\begin{pmatrix}-1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \n",
    "\\;\\right\\}$; using QR, an orthonormal basis is $\\left\\{\\; \n",
    " \\begin{pmatrix}-\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\\\ 0 \\\\ 0 \\end{pmatrix},\\\n",
    " \\begin{pmatrix}0 \\\\ 0 \\\\ \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{pmatrix} \n",
    "\\;\\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Finally $U = \\left( U_r \\; \\tilde{U}_r \\right)$:\n",
    "\n",
    "$$U = \\frac{1}{2}\\begin{pmatrix}\n",
    "-1 & -1 & -\\sqrt{2} & 0 \\\\\n",
    "-1 & -1 &  \\sqrt{2} & 0 \\\\\n",
    " 1 & -1 & 0 &  \\sqrt{2}\\\\\n",
    "-1 &  1 & 0 &  \\sqrt{2}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "BTW, I like\n",
    "https://towardsdatascience.com/svd-8c2f72e264f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
